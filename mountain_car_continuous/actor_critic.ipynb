{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "action_space=Box(1,)\n",
      "obs_space=Box(2,)\n",
      "threshold=90.0 \n",
      "\n",
      "Episode 0\tLast length:   221\t Reward:   84.99\t Avg Reward:   84.99\n",
      "Episode 1\tLast length:   998\t Reward: -114.73\t Avg Reward:  -14.87\n",
      "Episode 2\tLast length:   998\t Reward:  -17.70\t Avg Reward:  -15.82\n",
      "Episode 3\tLast length:   679\t Reward:   84.79\t Avg Reward:    9.34\n",
      "Episode 4\tLast length:   998\t Reward:  -22.99\t Avg Reward:    2.87\n",
      "Episode 5\tLast length:   998\t Reward:  -21.87\t Avg Reward:   -1.25\n",
      "Episode 6\tLast length:   998\t Reward:  -20.78\t Avg Reward:   -4.04\n",
      "Episode 7\tLast length:   998\t Reward:  -17.83\t Avg Reward:   -5.76\n",
      "Episode 8\tLast length:   476\t Reward:   91.29\t Avg Reward:    5.02\n",
      "Episode 9\tLast length:   998\t Reward:  -22.90\t Avg Reward:    2.23\n",
      "Episode 10\tLast length:   684\t Reward:   81.42\t Avg Reward:    9.43\n",
      "Episode 11\tLast length:   998\t Reward: -107.09\t Avg Reward:   -0.28\n",
      "Episode 12\tLast length:   998\t Reward:  -53.75\t Avg Reward:   -4.40\n",
      "Episode 13\tLast length:   775\t Reward:   29.72\t Avg Reward:   -1.96\n",
      "Episode 14\tLast length:   998\t Reward:  -64.79\t Avg Reward:   -6.15\n",
      "Episode 15\tLast length:   998\t Reward:  -31.52\t Avg Reward:   -7.73\n",
      "Episode 16\tLast length:   998\t Reward:  -21.60\t Avg Reward:   -8.55\n",
      "Episode 17\tLast length:   998\t Reward:  -31.71\t Avg Reward:   -9.84\n",
      "Episode 18\tLast length:   998\t Reward:  -24.28\t Avg Reward:  -10.60\n",
      "Episode 19\tLast length:   998\t Reward:  -20.45\t Avg Reward:  -11.09\n",
      "Episode 20\tLast length:   998\t Reward:  -18.42\t Avg Reward:  -11.44\n",
      "Episode 21\tLast length:   490\t Reward:   90.02\t Avg Reward:   -6.83\n",
      "Episode 22\tLast length:   998\t Reward:  -18.41\t Avg Reward:   -7.33\n",
      "Episode 23\tLast length:   998\t Reward:  -20.73\t Avg Reward:   -7.89\n",
      "Episode 24\tLast length:   998\t Reward:  -20.59\t Avg Reward:   -8.40\n",
      "Episode 25\tLast length:   758\t Reward:   76.24\t Avg Reward:   -5.14\n",
      "Episode 26\tLast length:   998\t Reward: -111.38\t Avg Reward:   -9.08\n",
      "Episode 27\tLast length:   998\t Reward:  -97.13\t Avg Reward:  -12.22\n",
      "Episode 28\tLast length:   998\t Reward:  -61.99\t Avg Reward:  -13.94\n",
      "Episode 29\tLast length:   998\t Reward:  -32.10\t Avg Reward:  -14.54\n",
      "Episode 30\tLast length:   998\t Reward:  -17.10\t Avg Reward:  -14.62\n",
      "Episode 31\tLast length:   998\t Reward:  -22.82\t Avg Reward:  -14.88\n",
      "Episode 32\tLast length:   998\t Reward:  -19.10\t Avg Reward:  -15.01\n",
      "Episode 33\tLast length:   998\t Reward:  -20.04\t Avg Reward:  -15.16\n",
      "Episode 34\tLast length:   712\t Reward:   83.24\t Avg Reward:  -12.35\n",
      "Episode 35\tLast length:   997\t Reward:   83.68\t Avg Reward:   -9.68\n",
      "Episode 36\tLast length:   998\t Reward:  -17.60\t Avg Reward:   -9.89\n",
      "Episode 37\tLast length:   998\t Reward:  -22.65\t Avg Reward:  -10.23\n",
      "Episode 38\tLast length:   998\t Reward:  -47.09\t Avg Reward:  -11.17\n",
      "Episode 39\tLast length:   647\t Reward:   25.90\t Avg Reward:  -10.25\n",
      "Episode 40\tLast length:   998\t Reward: -114.87\t Avg Reward:  -12.80\n",
      "Episode 41\tLast length:   998\t Reward: -119.33\t Avg Reward:  -15.33\n",
      "Episode 42\tLast length:   691\t Reward:   28.44\t Avg Reward:  -14.32\n",
      "Episode 43\tLast length:   998\t Reward: -124.74\t Avg Reward:  -16.83\n",
      "Episode 44\tLast length:   998\t Reward: -119.72\t Avg Reward:  -19.11\n",
      "Episode 45\tLast length:   998\t Reward: -102.70\t Avg Reward:  -20.93\n",
      "Episode 46\tLast length:   998\t Reward:  -26.37\t Avg Reward:  -21.05\n",
      "Episode 47\tLast length:   998\t Reward:  -21.81\t Avg Reward:  -21.06\n",
      "Episode 48\tLast length:   998\t Reward:  -17.18\t Avg Reward:  -20.98\n",
      "Episode 49\tLast length:   712\t Reward:   86.10\t Avg Reward:  -18.84\n",
      "Episode 50\tLast length:   998\t Reward:  -19.88\t Avg Reward:  -18.86\n",
      "Episode 51\tLast length:   998\t Reward:  -23.80\t Avg Reward:  -18.96\n",
      "Episode 52\tLast length:   998\t Reward:  -45.22\t Avg Reward:  -19.45\n",
      "Episode 53\tLast length:   998\t Reward:  -49.19\t Avg Reward:  -20.00\n",
      "Episode 54\tLast length:   998\t Reward:  -76.55\t Avg Reward:  -21.03\n",
      "Episode 55\tLast length:   820\t Reward:   15.20\t Avg Reward:  -20.38\n",
      "Episode 56\tLast length:   998\t Reward: -124.71\t Avg Reward:  -22.21\n",
      "Episode 57\tLast length:   973\t Reward:   -1.06\t Avg Reward:  -21.85\n",
      "Episode 58\tLast length:   430\t Reward:   42.39\t Avg Reward:  -20.76\n",
      "Episode 59\tLast length:   998\t Reward: -112.90\t Avg Reward:  -22.30\n",
      "Episode 60\tLast length:   379\t Reward:   46.50\t Avg Reward:  -21.17\n",
      "Episode 61\tLast length:   465\t Reward:   45.40\t Avg Reward:  -20.09\n",
      "Episode 62\tLast length:   998\t Reward:  -83.38\t Avg Reward:  -21.10\n",
      "Episode 63\tLast length:   998\t Reward:  -29.90\t Avg Reward:  -21.24\n",
      "Episode 64\tLast length:   998\t Reward:  -23.30\t Avg Reward:  -21.27\n",
      "Episode 65\tLast length:   998\t Reward:  -17.59\t Avg Reward:  -21.21\n",
      "Episode 66\tLast length:   998\t Reward:  -18.46\t Avg Reward:  -21.17\n",
      "Episode 67\tLast length:   998\t Reward:  -17.42\t Avg Reward:  -21.12\n",
      "Episode 68\tLast length:   998\t Reward:  -19.20\t Avg Reward:  -21.09\n",
      "Episode 69\tLast length:   636\t Reward:   89.01\t Avg Reward:  -19.52\n",
      "Episode 70\tLast length:   998\t Reward:  -39.53\t Avg Reward:  -19.80\n",
      "Episode 71\tLast length:   998\t Reward: -109.63\t Avg Reward:  -21.05\n",
      "Episode 72\tLast length:   998\t Reward: -108.36\t Avg Reward:  -22.24\n",
      "Episode 73\tLast length:   562\t Reward:   34.42\t Avg Reward:  -21.48\n",
      "Episode 74\tLast length:   247\t Reward:   74.75\t Avg Reward:  -20.19\n",
      "Episode 75\tLast length:   998\t Reward: -117.13\t Avg Reward:  -21.47\n",
      "Episode 76\tLast length:   778\t Reward:   18.42\t Avg Reward:  -20.95\n",
      "Episode 77\tLast length:   998\t Reward:  -26.62\t Avg Reward:  -21.02\n",
      "Episode 78\tLast length:   998\t Reward:  -21.83\t Avg Reward:  -21.03\n",
      "Episode 79\tLast length:   998\t Reward:  -16.85\t Avg Reward:  -20.98\n",
      "Episode 80\tLast length:   713\t Reward:   87.78\t Avg Reward:  -19.64\n",
      "Episode 81\tLast length:   998\t Reward:  -20.93\t Avg Reward:  -19.65\n",
      "Episode 82\tLast length:   998\t Reward:  -19.02\t Avg Reward:  -19.65\n",
      "Episode 83\tLast length:   998\t Reward:  -24.79\t Avg Reward:  -19.71\n",
      "Episode 84\tLast length:   998\t Reward:  -31.64\t Avg Reward:  -19.85\n",
      "Episode 85\tLast length:   998\t Reward:  -37.29\t Avg Reward:  -20.05\n",
      "Episode 86\tLast length:   998\t Reward:  -31.18\t Avg Reward:  -20.18\n",
      "Episode 87\tLast length:   998\t Reward: -102.90\t Avg Reward:  -21.12\n",
      "Episode 88\tLast length:   998\t Reward: -124.34\t Avg Reward:  -22.28\n",
      "Episode 89\tLast length:   998\t Reward: -105.37\t Avg Reward:  -23.20\n",
      "Episode 90\tLast length:   704\t Reward:   21.67\t Avg Reward:  -22.71\n",
      "Episode 91\tLast length:   998\t Reward:  -89.40\t Avg Reward:  -23.43\n",
      "Episode 92\tLast length:   998\t Reward:  -24.51\t Avg Reward:  -23.45\n",
      "Episode 93\tLast length:   998\t Reward:  -15.82\t Avg Reward:  -23.36\n",
      "Episode 94\tLast length:   998\t Reward:  -13.28\t Avg Reward:  -23.26\n",
      "Episode 95\tLast length:   998\t Reward:  -16.90\t Avg Reward:  -23.19\n",
      "Episode 96\tLast length:   998\t Reward:  -17.43\t Avg Reward:  -23.13\n",
      "Episode 97\tLast length:   998\t Reward:  -17.45\t Avg Reward:  -23.07\n",
      "Episode 98\tLast length:   831\t Reward:   50.78\t Avg Reward:  -22.33\n",
      "Episode 99\tLast length:   998\t Reward:  -91.49\t Avg Reward:  -23.02\n",
      "Episode 100\tLast length:   998\t Reward: -102.16\t Avg Reward:  -24.89\n",
      "Episode 101\tLast length:   998\t Reward:  -64.85\t Avg Reward:  -24.39\n",
      "Episode 102\tLast length:   998\t Reward:  -48.89\t Avg Reward:  -24.70\n",
      "Episode 103\tLast length:   998\t Reward:  -51.00\t Avg Reward:  -26.06\n",
      "Episode 104\tLast length:   998\t Reward:  -26.45\t Avg Reward:  -26.10\n",
      "Episode 105\tLast length:   998\t Reward:  -65.49\t Avg Reward:  -26.53\n",
      "Episode 106\tLast length:   998\t Reward:  -20.28\t Avg Reward:  -26.53\n",
      "Episode 107\tLast length:   998\t Reward:  -17.45\t Avg Reward:  -26.52\n",
      "Episode 108\tLast length:   998\t Reward:  -20.00\t Avg Reward:  -27.64\n",
      "Episode 109\tLast length:   998\t Reward:  -19.28\t Avg Reward:  -27.60\n",
      "Episode 110\tLast length:   998\t Reward:  -17.52\t Avg Reward:  -28.59\n",
      "Episode 111\tLast length:   998\t Reward:  -15.21\t Avg Reward:  -27.67\n",
      "Episode 112\tLast length:   998\t Reward:  -21.89\t Avg Reward:  -27.35\n",
      "Episode 113\tLast length:   998\t Reward: -108.48\t Avg Reward:  -28.74\n",
      "Episode 114\tLast length:   998\t Reward: -115.91\t Avg Reward:  -29.25\n",
      "Episode 115\tLast length:   998\t Reward:  -65.48\t Avg Reward:  -29.59\n",
      "Episode 116\tLast length:   998\t Reward:  -67.75\t Avg Reward:  -30.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 117\tLast length:   998\t Reward:  -31.37\t Avg Reward:  -30.04\n",
      "Episode 118\tLast length:   998\t Reward:  -28.53\t Avg Reward:  -30.09\n",
      "Episode 119\tLast length:   998\t Reward:  -23.53\t Avg Reward:  -30.12\n",
      "Episode 120\tLast length:   998\t Reward:  -17.37\t Avg Reward:  -30.11\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3339d420703b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"obs_space={}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"threshold={} \\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-3339d420703b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_episode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'render'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/gym/mountain_car_continuous/DDPG/ddpg.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m# Roll over last state and action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/gym/mountain_car_continuous/DDPG/ddpg.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, experiences)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;31m# Train actor model (local)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0maction_gradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_gradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# custom training function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m# Soft-update target models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gym/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gym/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gym/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "\n",
    "from DDPG.ddpg import DDPG\n",
    "\n",
    "args = {\n",
    "    'render': True,\n",
    "    'log_interval': 1\n",
    "}\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "episodes = 10000\n",
    "reward_history = []\n",
    "\n",
    "\n",
    "def main():   \n",
    "    task = {\n",
    "        'state_size': 2,\n",
    "        'action_size': 1,\n",
    "        'action_high': 1,\n",
    "        'action_low': 0\n",
    "    }\n",
    "    agent = DDPG(task)    \n",
    "    for i_episode in range(episodes):\n",
    "        running_reward = 0     \n",
    "        state = env.reset()\n",
    "        for t in range(10000):  # Don't infinite loop while learning\n",
    "            action = agent.act(state, i_episode)            \n",
    "            state, reward, done, _ = env.step(action)      \n",
    "            agent.step(action, reward, state, done)\n",
    "            if args['render']:\n",
    "                env.render()                   \n",
    "            running_reward += reward            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        reward_history.append(running_reward)\n",
    "        \n",
    "        if i_episode % args['log_interval'] == 0:\n",
    "            avg_reward = np.mean(reward_history[-100:])  \n",
    "            print('Episode {}\\tLast length: {:5d}\\t Reward: {:7.2f}\\t Avg Reward: {:7.2f}'.format(\n",
    "                i_episode, t, running_reward, avg_reward))\n",
    "        if avg_reward > env.spec.reward_threshold and i_episode > 100:\n",
    "            print(\"Solved! Average 100-episode reward is now {}!\".format(avg_reward))\n",
    "            break\n",
    "\n",
    "print(\"action_space={}\".format(env.action_space))\n",
    "print(\"obs_space={}\".format(env.observation_space))\n",
    "print(\"threshold={} \\n\".format(env.spec.reward_threshold))\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
