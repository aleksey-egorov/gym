{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "action_space=Box(1,)\n",
      "obs_space=Box(2,)\n",
      "threshold=90.0 \n",
      "\n",
      "Episode 0\tLast length:   998\t Reward:  -33.06\t Avg Reward:   -0.66\n",
      "Episode 50\tLast length:   998\t Reward:  -35.38\t Avg Reward:   -8.82\n",
      "Episode 100\tLast length:   998\t Reward:  -35.31\t Avg Reward:   -0.52\n",
      "Episode 150\tLast length:   982\t Reward:   65.74\t Avg Reward:  -14.54\n",
      "Episode 200\tLast length:   496\t Reward:   81.43\t Avg Reward:   -5.87\n",
      "Episode 250\tLast length:   998\t Reward:  -34.03\t Avg Reward:  -16.52\n",
      "Episode 300\tLast length:   998\t Reward:  -34.88\t Avg Reward:   -7.52\n",
      "Episode 350\tLast length:   998\t Reward:  -36.69\t Avg Reward:   -6.66\n",
      "Episode 400\tLast length:   998\t Reward:  -39.60\t Avg Reward:  -20.95\n",
      "Episode 450\tLast length:   998\t Reward:  -34.48\t Avg Reward:  -25.19\n",
      "Episode 500\tLast length:   998\t Reward:  -34.23\t Avg Reward:    3.36\n",
      "Episode 550\tLast length:   998\t Reward:  -35.32\t Avg Reward:  -27.43\n",
      "Episode 600\tLast length:   497\t Reward:   81.07\t Avg Reward:  -13.64\n",
      "Episode 650\tLast length:   998\t Reward:  -37.11\t Avg Reward:  -16.05\n",
      "Episode 700\tLast length:   571\t Reward:   76.64\t Avg Reward:  -11.75\n",
      "Episode 750\tLast length:   969\t Reward:   65.55\t Avg Reward:  -10.24\n",
      "Episode 800\tLast length:   998\t Reward:  -37.89\t Avg Reward:  -22.89\n",
      "Episode 850\tLast length:   998\t Reward:  -36.12\t Avg Reward:  -11.61\n",
      "Episode 900\tLast length:   638\t Reward:   75.46\t Avg Reward:  -15.99\n",
      "Episode 950\tLast length:   998\t Reward:  -35.61\t Avg Reward:  -20.10\n",
      "Episode 1000\tLast length:   998\t Reward:  -34.82\t Avg Reward:  -18.02\n",
      "Episode 1050\tLast length:   998\t Reward:  -37.15\t Avg Reward:  -18.53\n",
      "Episode 1100\tLast length:   998\t Reward:  -34.96\t Avg Reward:  -14.76\n",
      "Episode 1150\tLast length:   998\t Reward:  -39.86\t Avg Reward:  -23.11\n",
      "Episode 1200\tLast length:   998\t Reward:  -35.51\t Avg Reward:  -20.52\n",
      "Episode 1250\tLast length:   998\t Reward:  -36.34\t Avg Reward:  -21.06\n",
      "Solved! Running reward is now 90.84632879642787 and the last episode runs to 270 time steps!\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "\n",
    "from DDPG.ddpg import DDPG\n",
    "\n",
    "args = {\n",
    "    'gamma': 0.99,\n",
    "    'render': True,\n",
    "    'log_interval': 50\n",
    "}\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "episodes = 10000\n",
    "print(\"action_space={}\".format(env.action_space))\n",
    "print(\"obs_space={}\".format(env.observation_space))\n",
    "\n",
    "\n",
    "def main():   \n",
    "    task = {\n",
    "        'state_size': 2,\n",
    "        'action_size': 1,\n",
    "        'action_high': 1,\n",
    "        'action_low': 0\n",
    "    }\n",
    "    agent = DDPG(task)\n",
    "    sum_reward = 0\n",
    "    for i_episode in range(episodes):\n",
    "        running_reward = 0\n",
    "        #heights = []\n",
    "        #velocities = []\n",
    "        state = env.reset()\n",
    "        for t in range(10000):  # Don't infinite loop while learning\n",
    "            action = agent.act(state, i_episode)            \n",
    "            state, reward, done, _ = env.step(action)            \n",
    "            if args['render']:\n",
    "                env.render()                   \n",
    "            running_reward += reward\n",
    "            #heights.append(state[0])\n",
    "            #velocities.append(state[1])\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        sum_reward += running_reward\n",
    "        \n",
    "        if i_episode % args['log_interval'] == 0:\n",
    "            avg_reward = sum_reward / args['log_interval']\n",
    "            sum_reward = 0\n",
    "            print('Episode {}\\tLast length: {:5d}\\t Reward: {:7.2f}\\t Avg Reward: {:7.2f}'.format(\n",
    "                i_episode, t, running_reward, avg_reward))\n",
    "        if running_reward > env.spec.reward_threshold:\n",
    "            print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "            break\n",
    "\n",
    "print(\"threshold={} \\n\".format(env.spec.reward_threshold))\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
