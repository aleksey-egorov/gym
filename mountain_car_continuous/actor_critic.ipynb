{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "action_space=Box(1,)\n",
      "obs_space=Box(2,)\n",
      "threshold=90.0 \n",
      "\n",
      "Episode 0\tLast length:   723\t Reward:   49.49\t Avg Reward:   49.49\n",
      "Episode 1\tLast length:   706\t Reward:   21.19\t Avg Reward:   35.34\n",
      "Episode 2\tLast length:   998\t Reward: -124.01\t Avg Reward:  -17.78\n",
      "Episode 3\tLast length:   998\t Reward: -112.47\t Avg Reward:  -41.45\n",
      "Episode 4\tLast length:   998\t Reward: -111.58\t Avg Reward:  -55.48\n",
      "Episode 5\tLast length:   998\t Reward: -131.81\t Avg Reward:  -68.20\n",
      "Episode 6\tLast length:   998\t Reward: -129.96\t Avg Reward:  -77.02\n",
      "Episode 7\tLast length:   998\t Reward: -126.79\t Avg Reward:  -83.24\n",
      "Episode 8\tLast length:   998\t Reward: -119.74\t Avg Reward:  -87.30\n",
      "Episode 9\tLast length:   998\t Reward: -128.95\t Avg Reward:  -91.46\n",
      "Episode 10\tLast length:   998\t Reward: -126.44\t Avg Reward:  -94.64\n",
      "Episode 11\tLast length:   998\t Reward: -128.23\t Avg Reward:  -97.44\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "\n",
    "from DDPG.ddpg import DDPG\n",
    "\n",
    "args = {\n",
    "    'render': True,\n",
    "    'log_interval': 1\n",
    "}\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "episodes = 10000\n",
    "reward_history = []\n",
    "\n",
    "\n",
    "def main():   \n",
    "    task = {\n",
    "        'state_size': 2,\n",
    "        'action_size': 1,\n",
    "        'action_high': 1,\n",
    "        'action_low': 0\n",
    "    }\n",
    "    agent = DDPG(task)    \n",
    "    for i_episode in range(episodes):\n",
    "        running_reward = 0     \n",
    "        state = env.reset()\n",
    "        for t in range(10000):  # Don't infinite loop while learning\n",
    "            action = agent.act(state, i_episode)            \n",
    "            state, reward, done, _ = env.step(action)      \n",
    "            agent.step(action, reward, state, done)\n",
    "            if args['render']:\n",
    "                env.render()                   \n",
    "            running_reward += reward            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        reward_history.append(running_reward)\n",
    "        \n",
    "        if i_episode % args['log_interval'] == 0:\n",
    "            avg_reward = np.mean(reward_history[-100:])  \n",
    "            print('Episode {}\\tLast length: {:5d}\\t Reward: {:7.2f}\\t Avg Reward: {:7.2f}'.format(\n",
    "                i_episode, t, running_reward, avg_reward))\n",
    "        if avg_reward > env.spec.reward_threshold and i_episode > 100:\n",
    "            print(\"Solved! Average 100-episode reward is now {}!\".format(avg_reward))\n",
    "            break\n",
    "\n",
    "print(\"action_space={}\".format(env.action_space))\n",
    "print(\"obs_space={}\".format(env.observation_space))\n",
    "print(\"threshold={} \\n\".format(env.spec.reward_threshold))\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
