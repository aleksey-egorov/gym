{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "action_space=Box(1,)\n",
      "obs_space=Box(3,)\n",
      "threshold=None \n",
      "\n",
      "Episode 0\tLast length:   199\t Reward: -966.59\t Avg Reward: -966.59\t Noise: 1.00\n",
      "Episode 10\tLast length:   199\t Reward: -1490.84\t Avg Reward: -1280.16\t Noise: 1.00\n",
      "Episode 20\tLast length:   199\t Reward: -1509.60\t Avg Reward: -1396.75\t Noise: 0.99\n",
      "Episode 30\tLast length:   199\t Reward: -921.96\t Avg Reward: -1353.60\t Noise: 0.99\n",
      "Episode 40\tLast length:   199\t Reward: -1118.39\t Avg Reward: -1286.57\t Noise: 0.99\n",
      "Episode 50\tLast length:   199\t Reward: -1168.41\t Avg Reward: -1250.74\t Noise: 0.98\n",
      "Episode 60\tLast length:   199\t Reward: -1068.34\t Avg Reward: -1223.09\t Noise: 0.98\n",
      "Episode 70\tLast length:   199\t Reward: -1359.58\t Avg Reward: -1197.11\t Noise: 0.98\n",
      "Episode 80\tLast length:   199\t Reward: -1326.44\t Avg Reward: -1179.17\t Noise: 0.97\n",
      "Episode 90\tLast length:   199\t Reward: -1068.89\t Avg Reward: -1162.43\t Noise: 0.97\n",
      "Episode 100\tLast length:   199\t Reward: -138.91\t Avg Reward: -1125.60\t Noise: 0.97\n",
      "Episode 110\tLast length:   199\t Reward: -129.29\t Avg Reward: -1025.31\t Noise: 0.96\n",
      "Episode 120\tLast length:   199\t Reward: -244.75\t Avg Reward: -897.01\t Noise: 0.96\n",
      "Episode 130\tLast length:   199\t Reward: -491.94\t Avg Reward: -805.84\t Noise: 0.96\n",
      "Episode 140\tLast length:   199\t Reward: -505.74\t Avg Reward: -750.91\t Noise: 0.95\n",
      "Episode 150\tLast length:   199\t Reward: -530.76\t Avg Reward: -688.31\t Noise: 0.95\n",
      "Episode 160\tLast length:   199\t Reward: -373.54\t Avg Reward: -606.13\t Noise: 0.95\n",
      "Episode 170\tLast length:   199\t Reward: -989.48\t Avg Reward: -576.64\t Noise: 0.94\n",
      "Episode 180\tLast length:   199\t Reward: -775.02\t Avg Reward: -562.35\t Noise: 0.94\n",
      "Episode 190\tLast length:   199\t Reward: -879.57\t Avg Reward: -506.32\t Noise: 0.94\n",
      "Episode 200\tLast length:   199\t Reward: -954.30\t Avg Reward: -488.25\t Noise: 0.93\n",
      "Episode 210\tLast length:   199\t Reward: -123.41\t Avg Reward: -496.64\t Noise: 0.93\n",
      "Episode 220\tLast length:   199\t Reward: -236.95\t Avg Reward: -489.93\t Noise: 0.93\n",
      "Episode 230\tLast length:   199\t Reward: -470.15\t Avg Reward: -474.79\t Noise: 0.92\n",
      "Episode 240\tLast length:   199\t Reward: -447.77\t Avg Reward: -473.71\t Noise: 0.92\n",
      "Episode 250\tLast length:   199\t Reward: -512.76\t Avg Reward: -457.89\t Noise: 0.92\n",
      "Episode 260\tLast length:   199\t Reward: -382.90\t Avg Reward: -474.26\t Noise: 0.91\n",
      "Episode 270\tLast length:   199\t Reward: -506.60\t Avg Reward: -433.98\t Noise: 0.91\n",
      "Episode 280\tLast length:   199\t Reward: -1044.80\t Avg Reward: -416.55\t Noise: 0.91\n",
      "Episode 290\tLast length:   199\t Reward: -142.10\t Avg Reward: -422.11\t Noise: 0.90\n",
      "Episode 300\tLast length:   199\t Reward: -394.43\t Avg Reward: -429.06\t Noise: 0.90\n",
      "Episode 310\tLast length:   199\t Reward: -634.05\t Avg Reward: -457.88\t Noise: 0.90\n",
      "Episode 320\tLast length:   199\t Reward: -641.17\t Avg Reward: -512.16\t Noise: 0.89\n",
      "Episode 330\tLast length:   199\t Reward: -128.07\t Avg Reward: -539.28\t Noise: 0.89\n",
      "Episode 340\tLast length:   199\t Reward: -610.14\t Avg Reward: -541.08\t Noise: 0.89\n",
      "Episode 350\tLast length:   199\t Reward: -504.20\t Avg Reward: -571.08\t Noise: 0.88\n",
      "Episode 360\tLast length:   199\t Reward:   -3.05\t Avg Reward: -551.37\t Noise: 0.88\n",
      "Episode 370\tLast length:   199\t Reward: -449.40\t Avg Reward: -551.98\t Noise: 0.88\n",
      "Episode 380\tLast length:   199\t Reward: -129.08\t Avg Reward: -500.12\t Noise: 0.87\n",
      "Episode 390\tLast length:   199\t Reward: -404.91\t Avg Reward: -481.26\t Noise: 0.87\n",
      "Episode 400\tLast length:   199\t Reward: -248.50\t Avg Reward: -444.95\t Noise: 0.87\n",
      "Episode 410\tLast length:   199\t Reward: -270.21\t Avg Reward: -414.84\t Noise: 0.86\n",
      "Episode 420\tLast length:   199\t Reward: -623.86\t Avg Reward: -402.00\t Noise: 0.86\n",
      "Episode 430\tLast length:   199\t Reward: -395.57\t Avg Reward: -407.65\t Noise: 0.86\n",
      "Episode 440\tLast length:   199\t Reward: -123.33\t Avg Reward: -374.63\t Noise: 0.85\n",
      "Episode 450\tLast length:   199\t Reward: -329.68\t Avg Reward: -376.87\t Noise: 0.85\n",
      "Episode 460\tLast length:   199\t Reward: -1580.31\t Avg Reward: -450.90\t Noise: 0.85\n",
      "Episode 470\tLast length:   199\t Reward: -1514.17\t Avg Reward: -502.66\t Noise: 0.84\n",
      "Episode 480\tLast length:   199\t Reward: -1338.58\t Avg Reward: -535.99\t Noise: 0.84\n",
      "Episode 490\tLast length:   199\t Reward: -358.02\t Avg Reward: -546.87\t Noise: 0.84\n",
      "Episode 500\tLast length:   199\t Reward:   -1.40\t Avg Reward: -536.15\t Noise: 0.83\n",
      "Episode 510\tLast length:   199\t Reward: -372.80\t Avg Reward: -524.52\t Noise: 0.83\n",
      "Episode 520\tLast length:   199\t Reward: -463.21\t Avg Reward: -494.19\t Noise: 0.83\n",
      "Episode 530\tLast length:   199\t Reward: -236.67\t Avg Reward: -469.15\t Noise: 0.82\n",
      "Episode 540\tLast length:   199\t Reward: -229.86\t Avg Reward: -473.14\t Noise: 0.82\n",
      "Episode 550\tLast length:   199\t Reward:   -9.76\t Avg Reward: -424.27\t Noise: 0.82\n",
      "Episode 560\tLast length:   199\t Reward: -251.61\t Avg Reward: -352.12\t Noise: 0.81\n",
      "Episode 570\tLast length:   199\t Reward: -916.76\t Avg Reward: -302.12\t Noise: 0.81\n",
      "Episode 580\tLast length:   199\t Reward: -244.19\t Avg Reward: -263.83\t Noise: 0.81\n",
      "Episode 590\tLast length:   199\t Reward: -473.18\t Avg Reward: -239.35\t Noise: 0.80\n",
      "Episode 600\tLast length:   199\t Reward:  -10.28\t Avg Reward: -241.43\t Noise: 0.80\n",
      "Episode 610\tLast length:   199\t Reward: -251.52\t Avg Reward: -237.76\t Noise: 0.80\n",
      "Episode 620\tLast length:   199\t Reward: -138.26\t Avg Reward: -226.08\t Noise: 0.79\n",
      "Solved! Average 100-episode reward is now -226.08319127791623!\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "\n",
    "from DDPG.ddpg import DDPG\n",
    "\n",
    "args = {\n",
    "    'render': True,\n",
    "    'log_interval': 10\n",
    "}\n",
    "\n",
    "env = gym.make('Pendulum-v0')\n",
    "episodes = 100000\n",
    "reward_history = []\n",
    "threshold = -230\n",
    "\n",
    "\n",
    "def main():   \n",
    "    task = {\n",
    "        'state_size': 3,\n",
    "        'action_size': 1,\n",
    "        'action_high': 2,\n",
    "        'action_low': -2\n",
    "    }\n",
    "    agent = DDPG(task)    \n",
    "    for i_episode in range(episodes):\n",
    "        running_reward = 0        \n",
    "        state = env.reset()\n",
    "        for t in range(10000):  # Don't infinite loop while learning\n",
    "            action, noise_coeff = agent.act(state, i_episode)                \n",
    "            state, reward, done, _ = env.step(action)  \n",
    "            agent.step(action, reward, state, done)\n",
    "            if args['render']:\n",
    "                env.render()                   \n",
    "            running_reward += reward            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        reward_history.append(running_reward)\n",
    "        \n",
    "        if i_episode % args['log_interval'] == 0:\n",
    "            avg_reward = np.mean(reward_history[-100:])            \n",
    "            print('Episode {}\\tLast length: {:5d}\\t Reward: {:7.2f}\\t Avg Reward: {:7.2f}\\t Noise: {:.2f}'.format(\n",
    "                i_episode, t, running_reward, avg_reward, noise_coeff))\n",
    "        if avg_reward > threshold and i_episode > 100:\n",
    "            print(\"Solved! Average 100-episode reward is now {}!\".format(avg_reward))\n",
    "            break\n",
    "            \n",
    "print(\"action_space={}\".format(env.action_space))\n",
    "print(\"obs_space={}\".format(env.observation_space))\n",
    "print(\"threshold={} \\n\".format(env.spec.reward_threshold))\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
