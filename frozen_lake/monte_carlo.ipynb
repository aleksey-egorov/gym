{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "episodes = 5000000\n",
    "reward_history = []\n",
    "EPSILON = 0.3\n",
    "GAMMA = 0.95\n",
    "render = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_dict(d):\n",
    "    # returns the argmax (key) and max (value) from a dictionary   \n",
    "    max_key = None\n",
    "    max_val = float('-inf')\n",
    "    for k, v in d.items():\n",
    "        if v > max_val:\n",
    "            max_val = v\n",
    "            max_key = k\n",
    "    return max_key, max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(values, n=100) :\n",
    "    ret = np.cumsum(values, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_action(a, eps=0.1):\n",
    "    p = np.random.random()\n",
    "    if p < (1 - eps):\n",
    "        return a\n",
    "    else:\n",
    "        return env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_returns(states_actions_rewards):\n",
    "    # calculate the returns by working backwards from the terminal state\n",
    "    G = 0\n",
    "    states_actions_returns = []\n",
    "    first = True\n",
    "    for s, a, r in reversed(states_actions_rewards):\n",
    "        # a terminal state has a value of 0 by definition\n",
    "        # this is the first state we encounter in the reversed list\n",
    "        # we'll ignore its return (G) since it doesn't correspond to any move\n",
    "        if first:\n",
    "            first = False\n",
    "        else:\n",
    "            states_actions_returns.append((s, a, G))\n",
    "        G = r + GAMMA*G\n",
    "    states_actions_returns.reverse() # back to the original order of states visited\n",
    "\n",
    "    return states_actions_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_space=Discrete(4)\n",
      "obs_space=Discrete(16)\n",
      "threshold=0.78 \n",
      "\n",
      "Initial policy values: {0: 0, 1: 3, 2: 1, 3: 0, 4: 3, 5: 3, 6: 3, 7: 3, 8: 1, 9: 3, 10: 1, 11: 2, 12: 0, 13: 3, 14: 2, 15: 0} \n",
      "\n",
      "\n",
      "Episode: 0   Avg reward: 0.000  Avg Delta: 0.00000000  Epsilon: 0.300\n",
      "Episode: 5000   Avg reward: 0.098  Avg Delta: 0.00090168  Epsilon: 0.295\n",
      "Episode: 10000   Avg reward: 0.082  Avg Delta: 0.00033942  Epsilon: 0.290\n",
      "Episode: 15000   Avg reward: 0.100  Avg Delta: 0.00026108  Epsilon: 0.285\n",
      "Episode: 20000   Avg reward: 0.088  Avg Delta: 0.00016553  Epsilon: 0.280\n",
      "Episode: 25000   Avg reward: 0.096  Avg Delta: 0.00015711  Epsilon: 0.275\n",
      "Episode: 30000   Avg reward: 0.102  Avg Delta: 0.00012298  Epsilon: 0.270\n",
      "Episode: 35000   Avg reward: 0.102  Avg Delta: 0.00010595  Epsilon: 0.265\n",
      "Episode: 40000   Avg reward: 0.176  Avg Delta: 0.00012306  Epsilon: 0.260\n",
      "Episode: 45000   Avg reward: 0.176  Avg Delta: 0.00011145  Epsilon: 0.255\n",
      "Episode: 50000   Avg reward: 0.172  Avg Delta: 0.00009277  Epsilon: 0.250\n",
      "Episode: 55000   Avg reward: 0.138  Avg Delta: 0.00007627  Epsilon: 0.245\n",
      "Episode: 60000   Avg reward: 0.164  Avg Delta: 0.00007305  Epsilon: 0.240\n",
      "Episode: 65000   Avg reward: 0.160  Avg Delta: 0.00006845  Epsilon: 0.235\n",
      "Episode: 70000   Avg reward: 0.130  Avg Delta: 0.00005675  Epsilon: 0.230\n",
      "Episode: 75000   Avg reward: 0.190  Avg Delta: 0.00006115  Epsilon: 0.225\n",
      "Episode: 80000   Avg reward: 0.174  Avg Delta: 0.00005677  Epsilon: 0.220\n",
      "Episode: 85000   Avg reward: 0.156  Avg Delta: 0.00004958  Epsilon: 0.215\n",
      "Episode: 90000   Avg reward: 0.188  Avg Delta: 0.00005001  Epsilon: 0.210\n",
      "Episode: 95000   Avg reward: 0.180  Avg Delta: 0.00004069  Epsilon: 0.205\n",
      "Episode: 100000   Avg reward: 0.166  Avg Delta: 0.00004526  Epsilon: 0.200\n",
      "Episode: 105000   Avg reward: 0.190  Avg Delta: 0.00004242  Epsilon: 0.195\n",
      "Episode: 110000   Avg reward: 0.198  Avg Delta: 0.00004222  Epsilon: 0.190\n",
      "Episode: 115000   Avg reward: 0.234  Avg Delta: 0.00003995  Epsilon: 0.185\n",
      "Episode: 120000   Avg reward: 0.200  Avg Delta: 0.00003465  Epsilon: 0.180\n",
      "Episode: 125000   Avg reward: 0.192  Avg Delta: 0.00003405  Epsilon: 0.175\n",
      "Episode: 130000   Avg reward: 0.250  Avg Delta: 0.00003453  Epsilon: 0.170\n",
      "Episode: 135000   Avg reward: 0.210  Avg Delta: 0.00003205  Epsilon: 0.165\n",
      "Episode: 140000   Avg reward: 0.196  Avg Delta: 0.00002976  Epsilon: 0.160\n",
      "Episode: 145000   Avg reward: 0.182  Avg Delta: 0.00002526  Epsilon: 0.155\n",
      "Episode: 150000   Avg reward: 0.228  Avg Delta: 0.00002799  Epsilon: 0.150\n",
      "Episode: 155000   Avg reward: 0.214  Avg Delta: 0.00002676  Epsilon: 0.145\n",
      "Episode: 160000   Avg reward: 0.226  Avg Delta: 0.00002361  Epsilon: 0.140\n",
      "Episode: 165000   Avg reward: 0.284  Avg Delta: 0.00002448  Epsilon: 0.135\n",
      "Episode: 170000   Avg reward: 0.238  Avg Delta: 0.00002410  Epsilon: 0.130\n",
      "Episode: 175000   Avg reward: 0.206  Avg Delta: 0.00002010  Epsilon: 0.125\n",
      "Episode: 180000   Avg reward: 0.254  Avg Delta: 0.00002325  Epsilon: 0.120\n",
      "Episode: 185000   Avg reward: 0.272  Avg Delta: 0.00002163  Epsilon: 0.115\n",
      "Episode: 190000   Avg reward: 0.266  Avg Delta: 0.00001818  Epsilon: 0.110\n",
      "Episode: 195000   Avg reward: 0.246  Avg Delta: 0.00001940  Epsilon: 0.105\n",
      "Episode: 200000   Avg reward: 0.290  Avg Delta: 0.00001901  Epsilon: 0.100\n",
      "Episode: 205000   Avg reward: 0.256  Avg Delta: 0.00001792  Epsilon: 0.095\n",
      "Episode: 210000   Avg reward: 0.292  Avg Delta: 0.00001805  Epsilon: 0.090\n",
      "Episode: 215000   Avg reward: 0.314  Avg Delta: 0.00001678  Epsilon: 0.085\n",
      "Episode: 220000   Avg reward: 0.306  Avg Delta: 0.00001754  Epsilon: 0.080\n",
      "Episode: 225000   Avg reward: 0.320  Avg Delta: 0.00001568  Epsilon: 0.075\n",
      "Episode: 230000   Avg reward: 0.352  Avg Delta: 0.00001594  Epsilon: 0.070\n",
      "Episode: 235000   Avg reward: 0.362  Avg Delta: 0.00001582  Epsilon: 0.065\n",
      "Episode: 240000   Avg reward: 0.360  Avg Delta: 0.00001505  Epsilon: 0.060\n",
      "Episode: 245000   Avg reward: 0.352  Avg Delta: 0.00001266  Epsilon: 0.055\n",
      "Episode: 250000   Avg reward: 0.330  Avg Delta: 0.00001226  Epsilon: 0.050\n",
      "Episode: 255000   Avg reward: 0.346  Avg Delta: 0.00001210  Epsilon: 0.045\n",
      "Episode: 260000   Avg reward: 0.358  Avg Delta: 0.00001202  Epsilon: 0.040\n",
      "Episode: 265000   Avg reward: 0.324  Avg Delta: 0.00001032  Epsilon: 0.035\n",
      "Episode: 270000   Avg reward: 0.376  Avg Delta: 0.00000927  Epsilon: 0.030\n",
      "Episode: 275000   Avg reward: 0.406  Avg Delta: 0.00000827  Epsilon: 0.025\n",
      "Episode: 280000   Avg reward: 0.422  Avg Delta: 0.00000832  Epsilon: 0.020\n",
      "Episode: 285000   Avg reward: 0.392  Avg Delta: 0.00000655  Epsilon: 0.015\n",
      "Episode: 290000   Avg reward: 0.450  Avg Delta: 0.00000579  Epsilon: 0.010\n",
      "Episode: 295000   Avg reward: 0.406  Avg Delta: 0.00000438  Epsilon: 0.005\n",
      "Episode: 300000   Avg reward: 0.438  Avg Delta: 0.00000344  Epsilon: 0.000\n",
      "Episode: 305000   Avg reward: 0.428  Avg Delta: 0.00000356  Epsilon: 0.000\n",
      "Episode: 310000   Avg reward: 0.454  Avg Delta: 0.00000335  Epsilon: 0.000\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    eps = EPSILON\n",
    "    R = {}\n",
    "    Q = {}\n",
    "    P = {}\n",
    "    deltas = []\n",
    "    rewards_list = []\n",
    "    \n",
    "    for s in range(env.observation_space.n):\n",
    "        P[s] = env.action_space.sample()\n",
    "        Q[s] = {}\n",
    "        for a in range(env.action_space.n):\n",
    "            Q[s][a] = 0\n",
    "            R[(s,a)] = []\n",
    "            \n",
    "    print(\"Initial policy values: {} \\n\\n\".format(P))         \n",
    "        \n",
    "    for i_episode in range(episodes):            \n",
    "        \n",
    "        # Play episode\n",
    "        state = env.reset()        \n",
    "        reward_per_episode = 0  \n",
    "        states_actions_returns = []\n",
    "        action = epsilon_action(P[state], eps)\n",
    "        states_actions_rewards = [(state, action, 0)]\n",
    "        for t in range(10000):  # Don't infinite loop while learning                           \n",
    "            state, reward, done, _ = env.step(action) \n",
    "            reward_per_episode += reward\n",
    "            if render:\n",
    "                env.render()                           \n",
    "            if done:\n",
    "                states_actions_rewards.append((state, None, reward))\n",
    "                break\n",
    "            else:\n",
    "                action = epsilon_action(P[state], eps)\n",
    "                states_actions_rewards.append((state, action, reward))    \n",
    "        rewards_list.append(reward_per_episode)        \n",
    "        states_actions_returns = calculate_returns(states_actions_rewards)        \n",
    "        \n",
    "        # calculate Q(s,a)\n",
    "        biggest_change = 0\n",
    "        seen_state_action_pairs = set()\n",
    "        for s, a, r in states_actions_returns:\n",
    "            # check if we have already seen s\n",
    "            # first-visit Monte Carlo optimization\n",
    "            sa = (s, a)\n",
    "            if sa not in seen_state_action_pairs:\n",
    "                R[sa].append(r)\n",
    "                old_q = Q[s][a]\n",
    "                # the new Q[s][a] is the sample mean of all our returns for that (state, action)\n",
    "                Q[s][a] = np.mean(R[sa])\n",
    "                biggest_change = max(biggest_change, np.abs(old_q - Q[s][a]))\n",
    "                seen_state_action_pairs.add(sa)\n",
    "        deltas.append(biggest_change)\n",
    "        \n",
    "        # calculate new policy pi(s) = argmax[a]{ Q(s,a) }\n",
    "        for s in P.keys():\n",
    "            a, _ = max_dict(Q[s])\n",
    "            P[s] = a\n",
    "            \n",
    "        avg_reward = np.mean(rewards_list[-500:])     \n",
    "        if i_episode % 5000 == 0:            \n",
    "            avg_delta = np.mean(deltas[-500:])  \n",
    "            print (\"Episode: {}   Avg reward: {:3.3f}  Avg Delta: {:8.8f}  Epsilon: {:3.3f}\".format(\n",
    "                i_episode, avg_reward, avg_delta, eps))\n",
    "            eps = max(eps - 0.005, 0.0)\n",
    "            \n",
    "        if len(rewards_list) > 10000:\n",
    "            rewards_list.pop(0)\n",
    "            \n",
    "        if len(deltas) > 10000:\n",
    "            deltas.pop(0)\n",
    "          \n",
    "        if avg_reward >= env.spec.reward_threshold: \n",
    "            print(\"########## Solved! ###########\")\n",
    "            break\n",
    "            \n",
    "  \n",
    "    # calculate values for each state (just to print and compare)\n",
    "    # V(s) = max[a]{ Q(s,a) }\n",
    "    V = {}\n",
    "    for s in P.keys():\n",
    "        V[s] = max_dict(Q[s])[1]\n",
    "        \n",
    "    print(\"\\nV values: {}\".format(V))    \n",
    "   \n",
    "    return P\n",
    "            \n",
    "print(\"action_space={}\".format(env.action_space))\n",
    "print(\"obs_space={}\".format(env.observation_space))\n",
    "print(\"threshold={} \\n\".format(env.spec.reward_threshold))\n",
    "policy = main()\n",
    "\n",
    "print(\"Policy values: {}\".format(policy)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(policy):\n",
    "    print (policy)\n",
    "    state = env.reset()        \n",
    "    action = epsilon_action(policy[state], 0)\n",
    "    for t in range(10000):                        \n",
    "        state, reward, done, _ = env.step(action) \n",
    "        env.render()                           \n",
    "        if done:\n",
    "            print(\"Reward: {}\".format(reward))\n",
    "            break\n",
    "\n",
    "test(policy)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
