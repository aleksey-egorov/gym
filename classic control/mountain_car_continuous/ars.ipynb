{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "from PIL import Image\n",
    "\n",
    "from ARS.ars import Normalizer, Policy\n",
    "from ARS.utils import mkdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'MountainCarContinuous-v0'\n",
    "max_episodes = 30000\n",
    "max_timesteps = 2000\n",
    "learning_rate = 0.5\n",
    "num_deltas = 32\n",
    "num_best_deltas = 32\n",
    "noise = 0.9\n",
    "random_seed = 42\n",
    "log_interval = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARSTrainer():\n",
    "    def __init__(self,\n",
    "                 env_name='BipedalWalker-v2',\n",
    "                 max_episodes=30000,\n",
    "                 max_timesteps=2000,\n",
    "                 learning_rate=0.02,\n",
    "                 num_deltas=16,\n",
    "                 num_best_deltas=16,\n",
    "                 noise=0.03,\n",
    "                 random_seed=1,                \n",
    "                 input_size=None,\n",
    "                 output_size=None,\n",
    "                 normalizer=None,\n",
    "                 record_videos=True, record_interval=100, log_interval=5, threshold=None): \n",
    "           \n",
    "        self.algorithm_name = 'ars'\n",
    "        self.env_name = env_name\n",
    "        self.env = gym.make(env_name)\n",
    "        self.record_videos = record_videos\n",
    "        self.record_interval = record_interval        \n",
    "        if self.record_videos == True:\n",
    "            videos_dir = mkdir('.', 'videos')\n",
    "            monitor_dir = mkdir(videos_dir, self.algorithm_name)\n",
    "            should_record = lambda i: self.should_record\n",
    "            self.env = wrappers.Monitor(self.env, monitor_dir, video_callable=should_record, force=True)            \n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.shape[0]\n",
    "        self.action_low = self.env.action_space.low\n",
    "        self.action_high = self.env.action_space.high   \n",
    "        self.should_record = False\n",
    "        if not threshold == None:\n",
    "            self.threshold = threshold\n",
    "        else:    \n",
    "            self.threshold = self.env.spec.reward_threshold         \n",
    "        \n",
    "        self.max_episodes = max_episodes\n",
    "        self.max_timesteps = max_timesteps        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_deltas = num_deltas\n",
    "        self.num_best_deltas = num_best_deltas\n",
    "        assert self.num_best_deltas <= self.num_deltas\n",
    "        self.noise = noise\n",
    "        self.random_seed = random_seed\n",
    "                 \n",
    "        self.input_size = input_size or self.state_dim\n",
    "        self.output_size = output_size or self.action_dim       \n",
    "        self.log_interval = log_interval\n",
    "                 \n",
    "        self.rewards_list = []\n",
    "        self.optimal_policy = None\n",
    "                        \n",
    "        self.normalizer = normalizer or Normalizer(self.input_size)\n",
    "        self.policy = Policy(self.input_size, self.output_size, self.noise, self.learning_rate, \n",
    "                             self.num_deltas, self.num_best_deltas)         \n",
    "                 \n",
    "        if not random_seed == None:\n",
    "            print(\"Random Seed: {} \".format(self.random_seed))\n",
    "            self.env.seed(self.random_seed)           \n",
    "            np.random.seed(self.random_seed)                    \n",
    "                 \n",
    "\n",
    "    # Explore the policy on one specific direction and over one episode\n",
    "    def explore(self, direction=None, delta=None):\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        num_plays = 0.0\n",
    "        sum_rewards = 0.0\n",
    "        while not done and num_plays < self.max_timesteps:\n",
    "            self.normalizer.observe(state)\n",
    "            state = self.normalizer.normalize(state)\n",
    "            action = self.policy.evaluate(state, delta, direction)\n",
    "            state, reward, done, _ = self.env.step(action)\n",
    "            reward = max(min(reward, 1), -1)\n",
    "            sum_rewards += reward\n",
    "            num_plays += 1\n",
    "        return sum_rewards\n",
    "\n",
    "    def train(self):\n",
    "        start_time = time.time()\n",
    "        print(\"Training started ... \\n\")\n",
    "        print(\"action_space={}\".format(self.env.action_space))\n",
    "        print(\"obs_space={}\".format(self.env.observation_space))\n",
    "        print(\"threshold={}\".format(self.threshold)) \n",
    "        print(\"action_low={} action_high={} \\n\".format(self.action_low, self.action_high))    \n",
    "\n",
    "        \n",
    "        # logging variables:        \n",
    "        log_f = open(\"train_{}.txt\".format(self.algorithm_name), \"w+\")\n",
    "            \n",
    "        for episode in range(self.max_episodes):\n",
    "            # initialize the random noise deltas and the positive/negative rewards\n",
    "            deltas = self.policy.sample_deltas()\n",
    "            positive_rewards = [0] * self.num_deltas\n",
    "            negative_rewards = [0] * self.num_deltas\n",
    "\n",
    "            # play an episode each with positive deltas and negative deltas, collect rewards\n",
    "            for k in range(self.num_deltas):\n",
    "                positive_rewards[k] = self.explore(direction=\"+\", delta=deltas[k])\n",
    "                negative_rewards[k] = self.explore(direction=\"-\", delta=deltas[k])\n",
    "                \n",
    "            # Compute the standard deviation of all rewards\n",
    "            sigma_rewards = np.array(positive_rewards + negative_rewards).std()\n",
    "\n",
    "            # Sort the rollouts by the max(r_pos, r_neg) and select the deltas with best rewards\n",
    "            scores = {k:max(r_pos, r_neg) for k,(r_pos,r_neg) in enumerate(zip(positive_rewards, negative_rewards))}\n",
    "            order = sorted(scores.keys(), key = lambda x:scores[x], reverse = True)[:self.num_best_deltas]\n",
    "            rollouts = [(positive_rewards[k], negative_rewards[k], deltas[k]) for k in order]\n",
    "\n",
    "            # Update the policy\n",
    "            self.policy.update(rollouts, sigma_rewards)\n",
    "                        \n",
    "            # Only record video during evaluation, every n steps\n",
    "            if episode % self.record_interval == 0:\n",
    "                self.should_record = True\n",
    "                \n",
    "            # Play an episode with the new weights and print the score\n",
    "            reward_evaluation = self.explore()\n",
    "            \n",
    "            self.rewards_list.append(reward_evaluation)\n",
    "            avg_reward = np.mean(self.rewards_list[-100:])\n",
    "            \n",
    "            # logging updates:        \n",
    "            log_f.write('{},{}\\n'.format(episode, reward_evaluation))\n",
    "            log_f.flush()\n",
    "            \n",
    "            if episode % self.log_interval == 0:\n",
    "                print('Episode: {}  reward: {:6.4f}  avg_reward: {:6.4f}   sigma: {:6.4f}'.format(\n",
    "                    episode, reward_evaluation, avg_reward, sigma_rewards))\n",
    "                \n",
    "            self.should_record = False\n",
    "            \n",
    "            # if avg reward > threshold then save and stop traning:\n",
    "            if avg_reward >= self.threshold:                 \n",
    "                print(\"########## Solved! ###########\")                                 \n",
    "                self.optimal_policy = self.policy\n",
    "                log_f.close()\n",
    "                training_time = time.time() - start_time\n",
    "                print(\"Training time: {:6.2f} sec\".format(training_time))\n",
    "                break\n",
    "                \n",
    "    def test(self, direction=None, delta=None, episodes=3, render=True, save_gif=True):              \n",
    "\n",
    "        gifdir = mkdir('.','gif')\n",
    "        algdir = mkdir(gifdir, self.algorithm_name)\n",
    "        \n",
    "        for episode in range(1, episodes+1):            \n",
    "            state = self.env.reset()    \n",
    "            epdir = mkdir(algdir, str(episode))            \n",
    "            ep_reward = 0.0\n",
    "                       \n",
    "            for t in range(self.max_timesteps):\n",
    "                self.normalizer.observe(state)\n",
    "                state = self.normalizer.normalize(state)\n",
    "                action = self.optimal_policy.evaluate(state, delta, direction)\n",
    "                state, reward, done, _ = self.env.step(action)\n",
    "                reward = max(min(reward, 1), -1)\n",
    "                ep_reward += reward                                            \n",
    "                \n",
    "                if save_gif:                                       \n",
    "                    img = self.env.render(mode = 'rgb_array')\n",
    "                    img = Image.fromarray(img)\n",
    "                    img.save('{}/{}.jpg'.format(epdir, t))\n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "            print('Test episode: {}\\tReward: {:4.2f}'.format(episode, ep_reward))\n",
    "            ep_reward = 0\n",
    "            self.env.close()                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Random Seed: 42 \n",
      "Training started ... \n",
      "\n",
      "action_space=Box(1,)\n",
      "obs_space=Box(2,)\n",
      "threshold=90.0\n",
      "action_low=[-1.] action_high=[1.] \n",
      "\n",
      "Episode: 0  reward: -0.2045  avg_reward: -0.2045   sigma: 125.6280\n",
      "Episode: 10  reward: -1.1656  avg_reward: -0.6261   sigma: 75.4637\n",
      "Episode: 20  reward: -1.1421  avg_reward: -0.6319   sigma: 2.1912\n",
      "Episode: 30  reward: -1.1492  avg_reward: -0.6316   sigma: 107.1849\n",
      "Episode: 40  reward: -3.2531  avg_reward: -1.0077   sigma: 6.2239\n",
      "Episode: 50  reward: -0.3810  avg_reward: -0.9132   sigma: 2.3816\n",
      "Episode: 60  reward: -0.0049  avg_reward: -0.9020   sigma: 19.0239\n",
      "Episode: 70  reward: -1.4769  avg_reward: -0.8622   sigma: 79.7218\n",
      "Episode: 80  reward: -0.7453  avg_reward: -0.8174   sigma: 18.8226\n",
      "Episode: 90  reward: -1.9211  avg_reward: -0.7717   sigma: 88.9286\n",
      "Episode: 100  reward: -0.8353  avg_reward: -0.8070   sigma: 2.5753\n",
      "Episode: 110  reward: -1.8529  avg_reward: -0.8025   sigma: 58.0704\n",
      "Episode: 120  reward: -0.7843  avg_reward: -0.8041   sigma: 14.5293\n",
      "Episode: 130  reward: -0.2505  avg_reward: -0.8360   sigma: 3.0153\n",
      "Episode: 140  reward: -0.0925  avg_reward: -0.6273   sigma: 70.2039\n",
      "Episode: 150  reward: -0.5558  avg_reward: -0.6024   sigma: 88.4040\n",
      "Episode: 160  reward: -0.5092  avg_reward: -0.5671   sigma: 23.8938\n",
      "Episode: 170  reward: -0.0016  avg_reward: -0.5366   sigma: 30.6016\n",
      "Episode: 180  reward: -1.3700  avg_reward: -0.5773   sigma: 2.1655\n",
      "Episode: 190  reward: -1.8663  avg_reward: -0.5758   sigma: 18.2881\n",
      "Episode: 200  reward: -0.7649  avg_reward: -0.5353   sigma: 23.2978\n",
      "Episode: 210  reward: -0.0290  avg_reward: -0.5153   sigma: 35.9841\n",
      "Episode: 220  reward: -0.8872  avg_reward: -0.4877   sigma: 74.0398\n",
      "Episode: 230  reward: -0.1539  avg_reward: -0.4347   sigma: 3.6416\n",
      "Episode: 240  reward: -0.3195  avg_reward: -0.4946   sigma: 0.9372\n",
      "Episode: 250  reward: -0.2085  avg_reward: -0.4897   sigma: 0.8277\n",
      "Episode: 260  reward: -0.3620  avg_reward: -0.4983   sigma: 21.4836\n",
      "Episode: 270  reward: -0.1750  avg_reward: -0.4921   sigma: 74.9072\n",
      "Episode: 280  reward: -0.0487  avg_reward: -0.4412   sigma: 1.3536\n",
      "Episode: 290  reward: -0.0419  avg_reward: -0.4367   sigma: 19.1553\n",
      "Episode: 300  reward: -0.0060  avg_reward: -0.4477   sigma: 87.4552\n",
      "Episode: 310  reward: -0.3565  avg_reward: -0.4364   sigma: 78.3792\n",
      "Episode: 320  reward: -1.8157  avg_reward: -0.4411   sigma: 22.8868\n",
      "Episode: 330  reward: -0.0005  avg_reward: -0.4251   sigma: 18.4215\n",
      "Episode: 340  reward: -0.2305  avg_reward: -0.3788   sigma: 0.7478\n",
      "Episode: 350  reward: -0.5967  avg_reward: -0.3962   sigma: 1.4395\n",
      "Episode: 360  reward: -1.1169  avg_reward: -0.3931   sigma: 104.7433\n",
      "Episode: 370  reward: -0.2491  avg_reward: -0.3857   sigma: 41.6451\n",
      "Episode: 380  reward: -0.8507  avg_reward: -0.3718   sigma: 34.0752\n",
      "Episode: 390  reward: -0.2270  avg_reward: -0.3636   sigma: 30.3909\n",
      "Episode: 400  reward: -0.1441  avg_reward: -0.3396   sigma: 1.3493\n",
      "Episode: 410  reward: -0.1206  avg_reward: -0.3353   sigma: 1.1240\n",
      "Episode: 420  reward: -0.2415  avg_reward: -0.3284   sigma: 107.8994\n",
      "Episode: 430  reward: -1.0655  avg_reward: -0.3246   sigma: 1.0796\n",
      "Episode: 440  reward: -0.1806  avg_reward: -0.3605   sigma: 0.6667\n",
      "Episode: 450  reward: -0.1358  avg_reward: -0.3740   sigma: 2.2012\n",
      "Episode: 460  reward: -0.0525  avg_reward: -0.3536   sigma: 0.6657\n",
      "Episode: 470  reward: -0.1960  avg_reward: -0.3738   sigma: 0.7163\n",
      "Episode: 480  reward: -0.2115  avg_reward: -0.4110   sigma: 0.9618\n",
      "Episode: 490  reward: -0.0022  avg_reward: -0.4129   sigma: 20.7320\n",
      "Episode: 500  reward: -0.4735  avg_reward: -0.3932   sigma: 1.1927\n",
      "Episode: 510  reward: -0.4349  avg_reward: -0.4079   sigma: 1.2858\n",
      "Episode: 520  reward: -0.7152  avg_reward: -0.4286   sigma: 91.3028\n",
      "Episode: 530  reward: -0.0522  avg_reward: -0.4357   sigma: 26.1229\n",
      "Episode: 540  reward: -0.4217  avg_reward: -0.4223   sigma: 0.9144\n",
      "Episode: 550  reward: -0.5308  avg_reward: -0.4013   sigma: 27.6437\n",
      "Episode: 560  reward: -0.4443  avg_reward: -0.3892   sigma: 103.1060\n",
      "Episode: 570  reward: -0.8048  avg_reward: -0.3839   sigma: 0.9684\n",
      "Episode: 580  reward: -0.0272  avg_reward: -0.3530   sigma: 1.0348\n",
      "Episode: 590  reward: -0.0088  avg_reward: -0.3456   sigma: 4.9467\n",
      "Episode: 600  reward: -0.0986  avg_reward: -0.3361   sigma: 0.6898\n",
      "Episode: 610  reward: -0.0057  avg_reward: -0.3163   sigma: 76.1012\n",
      "Episode: 620  reward: -0.0626  avg_reward: -0.3115   sigma: 0.8093\n",
      "Episode: 630  reward: -0.2332  avg_reward: -0.2965   sigma: 0.6698\n",
      "Episode: 640  reward: -0.1149  avg_reward: -0.2856   sigma: 0.7267\n",
      "Episode: 650  reward: -0.5053  avg_reward: -0.2802   sigma: 0.7536\n",
      "Episode: 660  reward: -0.4229  avg_reward: -0.2968   sigma: 0.6704\n",
      "Episode: 670  reward: -0.0465  avg_reward: -0.3005   sigma: 0.6937\n",
      "Episode: 680  reward: -0.2983  avg_reward: -0.2976   sigma: 12.2576\n",
      "Episode: 690  reward: -0.6654  avg_reward: -0.3047   sigma: 0.4532\n",
      "Episode: 700  reward: -0.1302  avg_reward: -0.3128   sigma: 56.6229\n",
      "Episode: 710  reward: -0.0881  avg_reward: -0.3081   sigma: 86.3856\n",
      "Episode: 720  reward: -0.0109  avg_reward: -0.2767   sigma: 0.4840\n",
      "Episode: 730  reward: -0.6555  avg_reward: -0.2894   sigma: 67.0243\n",
      "Episode: 740  reward: -0.3061  avg_reward: -0.2759   sigma: 20.8522\n",
      "Episode: 750  reward: -0.0734  avg_reward: -0.2626   sigma: 39.1644\n",
      "Episode: 760  reward: -0.8717  avg_reward: -0.2774   sigma: 0.9650\n",
      "Episode: 770  reward: -0.1824  avg_reward: -0.2890   sigma: 0.7150\n",
      "Episode: 780  reward: -0.3532  avg_reward: -0.2993   sigma: 0.5275\n",
      "Episode: 790  reward: -0.6368  avg_reward: -0.2896   sigma: 22.1009\n",
      "Episode: 800  reward: -0.1652  avg_reward: -0.2981   sigma: 51.0933\n",
      "Episode: 810  reward: -0.0052  avg_reward: -0.3182   sigma: 0.2835\n",
      "Episode: 820  reward: -0.2963  avg_reward: -0.3099   sigma: 0.5329\n",
      "Episode: 830  reward: -0.0649  avg_reward: -0.3049   sigma: 0.6380\n",
      "Episode: 840  reward: -0.4337  avg_reward: -0.3038   sigma: 0.8457\n",
      "Episode: 850  reward: -0.0745  avg_reward: -0.3308   sigma: 0.7385\n",
      "Episode: 860  reward: -0.0220  avg_reward: -0.2928   sigma: 0.6656\n",
      "Episode: 870  reward: -0.0308  avg_reward: -0.2543   sigma: 0.8491\n",
      "Episode: 880  reward: -0.1070  avg_reward: -0.2351   sigma: 0.8503\n",
      "Episode: 890  reward: -0.4643  avg_reward: -0.2398   sigma: 84.4029\n",
      "Episode: 900  reward: -0.7400  avg_reward: -0.2363   sigma: 1.1455\n",
      "Episode: 910  reward: -0.1747  avg_reward: -0.2459   sigma: 0.6540\n",
      "Episode: 920  reward: -0.3676  avg_reward: -0.2789   sigma: 35.8718\n",
      "Episode: 930  reward: -0.0020  avg_reward: -0.2839   sigma: 0.8356\n",
      "Episode: 940  reward: -0.0472  avg_reward: -0.2900   sigma: 16.3794\n",
      "Episode: 950  reward: -0.5903  avg_reward: -0.2722   sigma: 0.5940\n",
      "Episode: 960  reward: -0.0155  avg_reward: -0.2780   sigma: 0.3362\n",
      "Episode: 970  reward: -0.6028  avg_reward: -0.2966   sigma: 61.5050\n",
      "Episode: 980  reward: -0.0740  avg_reward: -0.2993   sigma: 34.0329\n",
      "Episode: 990  reward: -1.2498  avg_reward: -0.3125   sigma: 26.1574\n",
      "Episode: 1000  reward: -0.7656  avg_reward: -0.2960   sigma: 0.3999\n",
      "Episode: 1010  reward: -0.0026  avg_reward: -0.2774   sigma: 35.5168\n",
      "Episode: 1020  reward: -0.5030  avg_reward: -0.2687   sigma: 0.5374\n",
      "Episode: 1030  reward: -0.0039  avg_reward: -0.2609   sigma: 80.4588\n",
      "Episode: 1040  reward: -0.0440  avg_reward: -0.2500   sigma: 0.6630\n",
      "Episode: 1050  reward: -0.6073  avg_reward: -0.2549   sigma: 76.8587\n",
      "Episode: 1060  reward: -0.0012  avg_reward: -0.2509   sigma: 15.2990\n",
      "Episode: 1070  reward: -0.1652  avg_reward: -0.2734   sigma: 0.5657\n",
      "Episode: 1080  reward: -0.0551  avg_reward: -0.2698   sigma: 0.3977\n",
      "Episode: 1090  reward: -0.1096  avg_reward: -0.2661   sigma: 0.7177\n",
      "Episode: 1100  reward: -0.0258  avg_reward: -0.2841   sigma: 0.4158\n",
      "Episode: 1110  reward: -0.1996  avg_reward: -0.2926   sigma: 1.3012\n",
      "Episode: 1120  reward: -0.2301  avg_reward: -0.2721   sigma: 136.6240\n",
      "Episode: 1130  reward: -0.0116  avg_reward: -0.2848   sigma: 22.1719\n",
      "Episode: 1140  reward: -0.9303  avg_reward: -0.3157   sigma: 19.6469\n",
      "Episode: 1150  reward: -0.0975  avg_reward: -0.3273   sigma: 77.2441\n",
      "Episode: 1160  reward: -0.0172  avg_reward: -0.3330   sigma: 60.6591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1170  reward: -0.0683  avg_reward: -0.3175   sigma: 1.0326\n",
      "Episode: 1180  reward: -0.0492  avg_reward: -0.3212   sigma: 0.6623\n",
      "Episode: 1190  reward: -0.0028  avg_reward: -0.3152   sigma: 0.6742\n",
      "Episode: 1200  reward: -0.4808  avg_reward: -0.2985   sigma: 104.9186\n",
      "Episode: 1210  reward: -0.4909  avg_reward: -0.3042   sigma: 0.4679\n",
      "Episode: 1220  reward: -0.1557  avg_reward: -0.3252   sigma: 1.1021\n",
      "Episode: 1230  reward: -0.3314  avg_reward: -0.3262   sigma: 31.7650\n",
      "Episode: 1240  reward: -0.0008  avg_reward: -0.3119   sigma: 0.6518\n",
      "Episode: 1250  reward: -0.0668  avg_reward: -0.3065   sigma: 11.0517\n",
      "Episode: 1260  reward: -0.1218  avg_reward: -0.3096   sigma: 0.4136\n",
      "Episode: 1270  reward: -0.0641  avg_reward: -0.2936   sigma: 0.7323\n",
      "Episode: 1280  reward: -0.3188  avg_reward: -0.2869   sigma: 7.0541\n",
      "Episode: 1290  reward: -0.2728  avg_reward: -0.2871   sigma: 0.8259\n",
      "Episode: 1300  reward: -0.1788  avg_reward: -0.2798   sigma: 0.5671\n",
      "Episode: 1310  reward: -0.0136  avg_reward: -0.2762   sigma: 0.8960\n",
      "Episode: 1320  reward: -0.7389  avg_reward: -0.2858   sigma: 0.5710\n",
      "Episode: 1330  reward: -1.4219  avg_reward: -0.2939   sigma: 71.4257\n",
      "Episode: 1340  reward: -0.0220  avg_reward: -0.2837   sigma: 0.6276\n",
      "Episode: 1350  reward: -0.6774  avg_reward: -0.2745   sigma: 77.8410\n",
      "Episode: 1360  reward: -0.0691  avg_reward: -0.2628   sigma: 0.3664\n",
      "Episode: 1370  reward: -0.1348  avg_reward: -0.2583   sigma: 63.5139\n",
      "Episode: 1380  reward: -0.0169  avg_reward: -0.2536   sigma: 0.3674\n",
      "Episode: 1390  reward: -0.0841  avg_reward: -0.2425   sigma: 0.3586\n",
      "Episode: 1400  reward: -1.8585  avg_reward: -0.2848   sigma: 60.4129\n",
      "Episode: 1410  reward: -0.6782  avg_reward: -0.2834   sigma: 0.6225\n",
      "Episode: 1420  reward: -0.0776  avg_reward: -0.2672   sigma: 0.3236\n",
      "Episode: 1430  reward: -0.2220  avg_reward: -0.2430   sigma: 11.3290\n",
      "Episode: 1440  reward: -0.1649  avg_reward: -0.2457   sigma: 90.4657\n",
      "Episode: 1450  reward: -0.6718  avg_reward: -0.2539   sigma: 0.6960\n",
      "Episode: 1460  reward: -0.0900  avg_reward: -0.2759   sigma: 0.5338\n",
      "Episode: 1470  reward: -0.2228  avg_reward: -0.2923   sigma: 13.5449\n",
      "Episode: 1480  reward: -0.2615  avg_reward: -0.3178   sigma: 0.4579\n",
      "Episode: 1490  reward: -0.7539  avg_reward: -0.3321   sigma: 98.7666\n",
      "Episode: 1500  reward: -0.0680  avg_reward: -0.3215   sigma: 0.4878\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b7ff8e4d62c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                  \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_deltas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_deltas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_best_deltas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_best_deltas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                  log_interval=log_interval)\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-de2e095a3877>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_deltas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0mpositive_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"+\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeltas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0mnegative_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeltas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;31m# Compute the standard deviation of all rewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-de2e095a3877>\u001b[0m in \u001b[0;36mexplore\u001b[0;34m(self, direction, delta)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0msum_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnum_plays\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_timesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/gym/mountain_car_continuous/ARS/ars.py\u001b[0m in \u001b[0;36mobserve\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mlast_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_diff\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlast_mean\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_diff\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = ARSTrainer(env_name, random_seed=random_seed, max_episodes=max_episodes, max_timesteps=max_timesteps,\n",
    "                 learning_rate=learning_rate, num_deltas=num_deltas, num_best_deltas=num_best_deltas, noise=noise,\n",
    "                 log_interval=log_interval)\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
