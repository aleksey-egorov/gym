{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from gym import wrappers\n",
    "from PIL import Image\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "\n",
    "from DuelingDDQN_PER.ddqn import Dueling_DDQN_PER\n",
    "from DuelingDDQN_PER.utils import mkdir\n",
    "from DuelingDDQN_PER.buffer import PrioritizedReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v1'\n",
    "lr_base = 0.001\n",
    "lr_decay = 0.0001\n",
    "epsilon_base = 0.5 \n",
    "epsilon_decay = 0.002\n",
    "\n",
    "random_seed = 43\n",
    "gamma = 0.99                # discount for future rewards\n",
    "batch_size = 256         # num of transitions sampled from replay buffer\n",
    "alpha=0.9\n",
    "beta_base=0.3\n",
    "beta_multiplier=0.0005\n",
    "max_episodes = 100000         # max num of episodes\n",
    "max_timesteps = 3000        # max timesteps in one episode\n",
    "max_buffer_length = 5000000\n",
    "log_interval = 10           # print avg reward after interval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_config = [\n",
    "        {'dim': [None, 64], 'dropout': False, 'activation': 'relu'},           \n",
    "    ] \n",
    "adv_stream_config = [\n",
    "       # {'dim': [64, 64], 'dropout': False, 'activation': 'relu'},      \n",
    "        {'dim': [64, None], 'dropout': False, 'activation': False}\n",
    "    ] \n",
    "value_stream_config = [\n",
    "       # {'dim': [64, 64], 'dropout': False, 'activation': 'relu'},      \n",
    "        {'dim': [64, 1], 'dropout': False, 'activation': False}\n",
    "    ] \n",
    "\n",
    "config = [fc_config, adv_stream_config, value_stream_config]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dueling_DDQN_PER_Trainer():\n",
    "    \n",
    "    def __init__(self, env_name, config, random_seed=42, lr_base=0.001, lr_decay=0.00005, \n",
    "                 epsilon_base=0.3, epsilon_decay=0.0001, gamma=0.99, batch_size=1024, \n",
    "                 max_episodes=100000, max_timesteps=3000, max_buffer_length=5000000, \n",
    "                 log_interval=5, threshold=None, lr_minimum=1e-10, epsilon_minimum=1e-10,\n",
    "                 record_videos=True, record_interval=100, alpha=0.9, beta_base=0.3, beta_multiplier=0.0001):\n",
    "                \n",
    "        self.algorithm_name = 'duel_ddqn_pre'\n",
    "        self.env_name = env_name\n",
    "        self.env = gym.make(env_name)\n",
    "        self.record_videos = record_videos\n",
    "        self.record_interval = record_interval        \n",
    "        if self.record_videos == True:\n",
    "            videos_dir = mkdir('.', 'videos')\n",
    "            monitor_dir = mkdir(videos_dir, self.algorithm_name)\n",
    "            should_record = lambda i: self.should_record\n",
    "            self.env = wrappers.Monitor(self.env, monitor_dir, video_callable=should_record, force=True)            \n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.n    \n",
    "        self.should_record = False\n",
    "        if not threshold == None:\n",
    "            self.threshold = threshold\n",
    "        else:    \n",
    "            self.threshold = self.env.spec.reward_threshold\n",
    "              \n",
    "        self.config = config\n",
    "        self.config[0][0]['dim'][0] = self.state_dim\n",
    "        self.config[1][-1]['dim'][1] = self.action_dim      \n",
    "        \n",
    "        \n",
    "        self.random_seed = random_seed\n",
    "        self.lr_base = lr_base\n",
    "        self.lr_decay = lr_decay\n",
    "        self.lr_minimum = lr_minimum  \n",
    "        self.epsilon_base = epsilon_base\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_minimum = epsilon_minimum\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size   \n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.beta_base = beta_base\n",
    "        self.beta_multiplier = beta_multiplier\n",
    "        \n",
    "        self.max_episodes = max_episodes\n",
    "        self.max_timesteps = max_timesteps\n",
    "        self.max_buffer_length = max_buffer_length\n",
    "        self.log_interval = log_interval\n",
    "        \n",
    "        prdir = mkdir('.', 'preTrained')\n",
    "        self.directory = mkdir(prdir, self.algorithm_name)\n",
    "        self.filename = \"{}_{}_{}\".format(self.algorithm_name, self.env_name, self.random_seed)\n",
    "        \n",
    "        self.policy = Dueling_DDQN_PER(self.env, config)    \n",
    "        self.replay_buffer = PrioritizedReplayBuffer(size=self.max_buffer_length, alpha=self.alpha)\n",
    "        \n",
    "        self.reward_history = []\n",
    "        self.make_plots = False       \n",
    "        \n",
    "        if self.random_seed:\n",
    "            print(\"Random Seed: {}\".format(self.random_seed))\n",
    "            self.env.seed(self.random_seed)\n",
    "            torch.manual_seed(self.random_seed)\n",
    "            np.random.seed(self.random_seed)\n",
    "            \n",
    "    def train(self):\n",
    "        \n",
    "        start_time = time.time()        \n",
    "        print(\"action_space={}\".format(self.env.action_space))\n",
    "        print(\"obs_space={}\".format(self.env.observation_space))\n",
    "        print(\"threshold={} \\n\".format(self.threshold))        \n",
    "\n",
    "        # loading models\n",
    "        self.policy.load(self.directory, self.filename)\n",
    "                \n",
    "        # logging variables:        \n",
    "        log_f = open(\"train_{}.txt\".format(self.algorithm_name), \"w+\")\n",
    "        \n",
    "        print(\"\\nTraining started ... \")\n",
    "        avg_Q_loss = 0\n",
    "                \n",
    "        # training procedure:        \n",
    "        for episode in range(self.max_episodes):\n",
    "            \n",
    "            # Only record video during evaluation, every n steps\n",
    "            if episode % self.record_interval == 0:\n",
    "                self.should_record = True\n",
    "            \n",
    "            ep_reward = 0.0        \n",
    "            state = self.env.reset()\n",
    "                       \n",
    "            # calculate params\n",
    "            epsilon = max(self.epsilon_base / (1.0 + episode * self.epsilon_decay), self.epsilon_minimum)\n",
    "            learning_rate = max(self.lr_base / (1.0 + episode * self.lr_decay), self.lr_minimum)    \n",
    "            beta = min(self.beta_base + episode * self.beta_multiplier, 1)\n",
    "            self.policy.set_optimizers(lr=learning_rate)\n",
    "           \n",
    "            for t in range(self.max_timesteps):\n",
    "                \n",
    "                action = self.policy.select_action(state, epsilon)                \n",
    "                next_state, reward, done, _ = self.env.step(action)          \n",
    "                self.replay_buffer.add(state, action, reward, next_state, float(done))\n",
    "                \n",
    "                # Updating policy\n",
    "                self.policy.update(self.replay_buffer, t, self.batch_size, self.gamma, beta)\n",
    "                \n",
    "                state = next_state               \n",
    "                ep_reward += reward            \n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "           \n",
    "            self.reward_history.append(ep_reward)\n",
    "            avg_reward = np.mean(self.reward_history[-100:]) \n",
    "                       \n",
    "            # logging updates:        \n",
    "            log_f.write('{},{}\\n'.format(episode, ep_reward))\n",
    "            log_f.flush()\n",
    "           \n",
    "            if len(self.policy.Q_loss_list) > 0:               \n",
    "                avg_Q_loss = np.mean(self.policy.Q_loss_list[-100:])     \n",
    "            \n",
    "            # Truncate training history if we don't plan to plot it later\n",
    "            if not self.make_plots:\n",
    "                self.policy.truncate_loss_lists() \n",
    "                if len(self.reward_history) > 100:\n",
    "                    self.reward_history.pop(0)    \n",
    "         \n",
    "            # Print avg reward every log interval:\n",
    "            if episode % self.log_interval == 0:            \n",
    "                self.policy.save(self.directory, self.filename)\n",
    "                print(\"Ep:{:5d}  Rew:{:8.2f}  Avg Rew:{:8.2f}  LR:{:8.8f}  Bf:{:2.0f}  Beta:{:0.4f}  EPS:{:0.4f}  Loss: {:8.6f}\".format(\n",
    "                    episode, ep_reward, avg_reward, learning_rate, self.replay_buffer.get_fill(), beta,\n",
    "                    epsilon, avg_Q_loss))\n",
    "                        \n",
    "            self.should_record = False\n",
    "            \n",
    "            # if avg reward > threshold then save and stop traning:\n",
    "            if avg_reward >= self.threshold and episode > 100: \n",
    "                print(\"Ep:{:5d}  Rew:{:8.2f}  Avg Rew:{:8.2f}  LR:{:8.8f}  Bf:{:2.0f}  Beta:{:0.4f} EPS:{:0.4f}  Loss: {:8.6f}\".format(\n",
    "                    episode, ep_reward, avg_reward, learning_rate, self.replay_buffer.get_fill(), beta,\n",
    "                    epsilon, avg_Q_loss))\n",
    "                print(\"########## Solved! ###########\")\n",
    "                name = self.filename + '_solved'\n",
    "                self.policy.save(self.directory, name)\n",
    "                log_f.close()\n",
    "                self.env.close()  \n",
    "                training_time = time.time() - start_time\n",
    "                print(\"Training time: {:6.2f} sec\".format(training_time))\n",
    "                break    \n",
    "                                \n",
    "    def test(self, episodes=3, render=True, save_gif=True):              \n",
    "\n",
    "        gifdir = mkdir('.','gif')\n",
    "        algdir = mkdir(gifdir, self.algorithm_name)\n",
    "        \n",
    "        for episode in range(1, episodes+1):\n",
    "            ep_reward = 0.0\n",
    "            state = self.env.reset()    \n",
    "            epdir = mkdir(algdir, str(episode))\n",
    "                       \n",
    "            for t in range(self.max_timesteps):\n",
    "                action = self.policy.select_action(state, 0)\n",
    "                next_state, reward, done, _ = self.env.step(action)               \n",
    "                state = next_state               \n",
    "                ep_reward += reward                                  \n",
    "                \n",
    "                if save_gif:                                       \n",
    "                    img = self.env.render(mode = 'rgb_array')\n",
    "                    img = Image.fromarray(img)\n",
    "                    img.save('{}/{}.jpg'.format(epdir, t))\n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "            print('Test episode: {}\\tReward: {:4.2f}'.format(episode, ep_reward))           \n",
    "            self.env.close()        \n",
    "                   \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "NETWORK: Sequential(\n",
      "  (0): Linear(in_features=4, out_features=64, bias=True)\n",
      "  (1): ReLU()\n",
      ") Sequential(\n",
      "  (0): Linear(in_features=64, out_features=2, bias=True)\n",
      ") Sequential(\n",
      "  (0): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "Device: cpu\n",
      "NETWORK: Sequential(\n",
      "  (0): Linear(in_features=4, out_features=64, bias=True)\n",
      "  (1): ReLU()\n",
      ") Sequential(\n",
      "  (0): Linear(in_features=64, out_features=2, bias=True)\n",
      ") Sequential(\n",
      "  (0): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "Device: cpu\n",
      "Random Seed: 43\n",
      "action_space=Discrete(2)\n",
      "obs_space=Box(4,)\n",
      "threshold=475.0 \n",
      "\n",
      "DIR=./preTrained/duel_ddqn_pre NAME=duel_ddqn_pre_CartPole-v1_43\n",
      "Models loaded\n",
      "\n",
      "Training started ... \n",
      "Ep:    0  Rew:   14.00  Avg Rew:   14.00  LR:0.00100000  Bf: 0  Beta:0.3000  EPS:0.5000  Loss: 0.000000\n",
      "Ep:   10  Rew:   14.00  Avg Rew:   13.45  LR:0.00099900  Bf: 0  Beta:0.3050  EPS:0.4902  Loss: 0.000000\n",
      "Ep:   20  Rew:   10.00  Avg Rew:   13.10  LR:0.00099800  Bf: 0  Beta:0.3100  EPS:0.4808  Loss: 1.176805\n",
      "Ep:   30  Rew:   14.00  Avg Rew:   14.23  LR:0.00099701  Bf: 0  Beta:0.3150  EPS:0.4717  Loss: 3.389417\n",
      "Ep:   40  Rew:   12.00  Avg Rew:   13.85  LR:0.00099602  Bf: 0  Beta:0.3200  EPS:0.4630  Loss: 6.273001\n",
      "Ep:   50  Rew:   10.00  Avg Rew:   13.31  LR:0.00099502  Bf: 0  Beta:0.3250  EPS:0.4545  Loss: 5.334467\n",
      "Ep:   60  Rew:   10.00  Avg Rew:   13.41  LR:0.00099404  Bf: 0  Beta:0.3300  EPS:0.4464  Loss: 9.487371\n",
      "Ep:   70  Rew:    9.00  Avg Rew:   13.48  LR:0.00099305  Bf: 0  Beta:0.3350  EPS:0.4386  Loss: 24.663027\n",
      "Ep:   80  Rew:   22.00  Avg Rew:   14.78  LR:0.00099206  Bf: 0  Beta:0.3400  EPS:0.4310  Loss: 14.726550\n",
      "Ep:   90  Rew:   22.00  Avg Rew:   16.35  LR:0.00099108  Bf: 0  Beta:0.3450  EPS:0.4237  Loss: 17.123814\n",
      "Ep:  100  Rew:   26.00  Avg Rew:   17.70  LR:0.00099010  Bf: 0  Beta:0.3500  EPS:0.4167  Loss: 18.986519\n",
      "Ep:  110  Rew:   31.00  Avg Rew:   20.29  LR:0.00098912  Bf: 0  Beta:0.3550  EPS:0.4098  Loss: 19.320788\n",
      "Ep:  120  Rew:   27.00  Avg Rew:   22.22  LR:0.00098814  Bf: 0  Beta:0.3600  EPS:0.4032  Loss: 18.836780\n",
      "Ep:  130  Rew:   67.00  Avg Rew:   24.64  LR:0.00098717  Bf: 0  Beta:0.3650  EPS:0.3968  Loss: 20.096821\n",
      "Ep:  140  Rew:   50.00  Avg Rew:   31.15  LR:0.00098619  Bf: 0  Beta:0.3700  EPS:0.3906  Loss: 17.436744\n",
      "Ep:  150  Rew:  252.00  Avg Rew:   43.36  LR:0.00098522  Bf: 0  Beta:0.3750  EPS:0.3846  Loss: 13.723036\n",
      "Ep:  160  Rew:  207.00  Avg Rew:   64.80  LR:0.00098425  Bf: 0  Beta:0.3800  EPS:0.3788  Loss: 13.078970\n",
      "Ep:  170  Rew:  211.00  Avg Rew:   85.18  LR:0.00098328  Bf: 0  Beta:0.3850  EPS:0.3731  Loss: 13.757308\n",
      "Ep:  180  Rew:  212.00  Avg Rew:  102.03  LR:0.00098232  Bf: 0  Beta:0.3900  EPS:0.3676  Loss: 13.974528\n",
      "Ep:  190  Rew:  228.00  Avg Rew:  123.47  LR:0.00098135  Bf: 0  Beta:0.3950  EPS:0.3623  Loss: 13.791594\n",
      "Ep:  200  Rew:   39.00  Avg Rew:  137.17  LR:0.00098039  Bf: 0  Beta:0.4000  EPS:0.3571  Loss: 13.462182\n",
      "Ep:  210  Rew:   81.00  Avg Rew:  152.34  LR:0.00097943  Bf: 0  Beta:0.4050  EPS:0.3521  Loss: 14.004747\n",
      "Ep:  220  Rew:  200.00  Avg Rew:  170.38  LR:0.00097847  Bf: 0  Beta:0.4100  EPS:0.3472  Loss: 18.788381\n",
      "Ep:  230  Rew:  233.00  Avg Rew:  183.76  LR:0.00097752  Bf: 0  Beta:0.4150  EPS:0.3425  Loss: 17.220372\n",
      "Ep:  240  Rew:  188.00  Avg Rew:  196.11  LR:0.00097656  Bf: 0  Beta:0.4200  EPS:0.3378  Loss: 16.411359\n",
      "Ep:  250  Rew:  171.00  Avg Rew:  202.85  LR:0.00097561  Bf: 1  Beta:0.4250  EPS:0.3333  Loss: 17.637544\n",
      "Ep:  260  Rew:  254.00  Avg Rew:  204.19  LR:0.00097466  Bf: 1  Beta:0.4300  EPS:0.3289  Loss: 24.506764\n",
      "Ep:  270  Rew:  283.00  Avg Rew:  204.82  LR:0.00097371  Bf: 1  Beta:0.4350  EPS:0.3247  Loss: 20.347075\n",
      "Ep:  280  Rew:  266.00  Avg Rew:  205.40  LR:0.00097276  Bf: 1  Beta:0.4400  EPS:0.3205  Loss: 18.682363\n",
      "Ep:  290  Rew:  231.00  Avg Rew:  205.89  LR:0.00097182  Bf: 1  Beta:0.4450  EPS:0.3165  Loss: 18.464489\n",
      "Ep:  300  Rew:  249.00  Avg Rew:  210.60  LR:0.00097087  Bf: 1  Beta:0.4500  EPS:0.3125  Loss: 14.112756\n",
      "Ep:  310  Rew:  245.00  Avg Rew:  216.18  LR:0.00096993  Bf: 1  Beta:0.4550  EPS:0.3086  Loss: 15.624233\n",
      "Ep:  320  Rew:  232.00  Avg Rew:  219.81  LR:0.00096899  Bf: 1  Beta:0.4600  EPS:0.3049  Loss: 15.631808\n",
      "Ep:  330  Rew:   79.00  Avg Rew:  225.93  LR:0.00096805  Bf: 1  Beta:0.4650  EPS:0.3012  Loss: 13.229066\n",
      "Ep:  340  Rew:  264.00  Avg Rew:  235.57  LR:0.00096712  Bf: 1  Beta:0.4700  EPS:0.2976  Loss: 13.702424\n",
      "Ep:  350  Rew:  216.00  Avg Rew:  242.65  LR:0.00096618  Bf: 1  Beta:0.4750  EPS:0.2941  Loss: 14.835868\n",
      "Ep:  360  Rew:  210.00  Avg Rew:  242.98  LR:0.00096525  Bf: 1  Beta:0.4800  EPS:0.2907  Loss: 11.861861\n",
      "Ep:  370  Rew:  210.00  Avg Rew:  249.84  LR:0.00096432  Bf: 1  Beta:0.4850  EPS:0.2874  Loss: 12.851172\n",
      "Ep:  380  Rew:  135.00  Avg Rew:  261.55  LR:0.00096339  Bf: 1  Beta:0.4900  EPS:0.2841  Loss: 11.404684\n",
      "Ep:  390  Rew:  168.00  Avg Rew:  263.56  LR:0.00096246  Bf: 1  Beta:0.4950  EPS:0.2809  Loss: 14.630092\n",
      "Ep:  400  Rew:  416.00  Avg Rew:  274.99  LR:0.00096154  Bf: 1  Beta:0.5000  EPS:0.2778  Loss: 14.366694\n",
      "Ep:  410  Rew:  216.00  Avg Rew:  278.45  LR:0.00096061  Bf: 1  Beta:0.5050  EPS:0.2747  Loss: 12.889615\n",
      "Ep:  420  Rew:  500.00  Avg Rew:  291.45  LR:0.00095969  Bf: 1  Beta:0.5100  EPS:0.2717  Loss: 12.752756\n",
      "Ep:  430  Rew:  345.00  Avg Rew:  304.23  LR:0.00095877  Bf: 1  Beta:0.5150  EPS:0.2688  Loss: 12.778063\n",
      "Ep:  440  Rew:  237.00  Avg Rew:  302.28  LR:0.00095785  Bf: 2  Beta:0.5200  EPS:0.2660  Loss: 10.538824\n",
      "Ep:  450  Rew:  301.00  Avg Rew:  296.38  LR:0.00095694  Bf: 2  Beta:0.5250  EPS:0.2632  Loss: 16.497650\n",
      "Ep:  460  Rew:  296.00  Avg Rew:  301.22  LR:0.00095602  Bf: 2  Beta:0.5300  EPS:0.2604  Loss: 12.054588\n",
      "Ep:  470  Rew:  278.00  Avg Rew:  300.96  LR:0.00095511  Bf: 2  Beta:0.5350  EPS:0.2577  Loss: 13.069278\n",
      "Ep:  480  Rew:  301.00  Avg Rew:  302.96  LR:0.00095420  Bf: 2  Beta:0.5400  EPS:0.2551  Loss: 14.253420\n",
      "Ep:  490  Rew:  209.00  Avg Rew:  296.29  LR:0.00095329  Bf: 2  Beta:0.5450  EPS:0.2525  Loss: 15.743138\n",
      "Ep:  500  Rew:  202.00  Avg Rew:  281.77  LR:0.00095238  Bf: 2  Beta:0.5500  EPS:0.2500  Loss: 18.031733\n",
      "Ep:  510  Rew:   19.00  Avg Rew:  271.97  LR:0.00095147  Bf: 2  Beta:0.5550  EPS:0.2475  Loss: 18.143267\n",
      "Ep:  520  Rew:  229.00  Avg Rew:  255.98  LR:0.00095057  Bf: 2  Beta:0.5600  EPS:0.2451  Loss: 17.486348\n",
      "Ep:  530  Rew:  220.00  Avg Rew:  241.09  LR:0.00094967  Bf: 2  Beta:0.5650  EPS:0.2427  Loss: 15.166692\n",
      "Ep:  540  Rew:  301.00  Avg Rew:  232.12  LR:0.00094877  Bf: 2  Beta:0.5700  EPS:0.2404  Loss: 15.798991\n",
      "Ep:  550  Rew:  206.00  Avg Rew:  228.57  LR:0.00094787  Bf: 2  Beta:0.5750  EPS:0.2381  Loss: 15.603638\n",
      "Ep:  560  Rew:   46.00  Avg Rew:  216.89  LR:0.00094697  Bf: 2  Beta:0.5800  EPS:0.2358  Loss: 12.722268\n",
      "Ep:  570  Rew:  239.00  Avg Rew:  210.78  LR:0.00094607  Bf: 2  Beta:0.5850  EPS:0.2336  Loss: 12.978239\n",
      "Ep:  580  Rew:  405.00  Avg Rew:  196.76  LR:0.00094518  Bf: 2  Beta:0.5900  EPS:0.2315  Loss: 13.672879\n",
      "Ep:  590  Rew:  251.00  Avg Rew:  199.20  LR:0.00094429  Bf: 2  Beta:0.5950  EPS:0.2294  Loss: 16.387877\n",
      "Ep:  600  Rew:   55.00  Avg Rew:  202.24  LR:0.00094340  Bf: 2  Beta:0.6000  EPS:0.2273  Loss: 13.180405\n",
      "Ep:  610  Rew:  234.00  Avg Rew:  201.31  LR:0.00094251  Bf: 2  Beta:0.6050  EPS:0.2252  Loss: 15.340093\n",
      "Ep:  620  Rew:  290.00  Avg Rew:  204.55  LR:0.00094162  Bf: 2  Beta:0.6100  EPS:0.2232  Loss: 17.146114\n",
      "Ep:  630  Rew:  261.00  Avg Rew:  209.91  LR:0.00094073  Bf: 2  Beta:0.6150  EPS:0.2212  Loss: 9.737765\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1be0aa3d8dcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                    \u001b[0mmax_buffer_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_buffer_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_interval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                    alpha=alpha, beta_base=beta_base, beta_multiplier=beta_multiplier)\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-84bff1f31571>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0;31m# Updating policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/gym/classic control/cartpole/DuelingDDQN_PER/ddqn.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, buffer, t, batch_size, gamma, beta)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mloss_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mloss_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/gym/classic control/cartpole/DuelingDDQN_PER/buffer.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, batch_size, beta)\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmax_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0mencoded_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_sample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midxes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/gym/classic control/cartpole/DuelingDDQN_PER/buffer.py\u001b[0m in \u001b[0;36m_encode_sample\u001b[0;34m(self, idxes)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mobses_tp1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_tp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mdones\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobses_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobses_tp1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = Dueling_DDQN_PER_Trainer(env_name, config, random_seed=random_seed, lr_base=lr_base, lr_decay=lr_decay, \n",
    "                   epsilon_base=epsilon_base, epsilon_decay=epsilon_decay, gamma=gamma, batch_size=batch_size,\n",
    "                   max_episodes=max_episodes, max_timesteps=max_timesteps, \n",
    "                   max_buffer_length=max_buffer_length, log_interval=log_interval,\n",
    "                   alpha=alpha, beta_base=beta_base, beta_multiplier=beta_multiplier)\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "agent.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
