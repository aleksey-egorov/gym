{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from gym import wrappers\n",
    "from PIL import Image\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "import ptan\n",
    "\n",
    "from PG.pg import PG\n",
    "from PG.utils import mkdir\n",
    "from PG.buffer import MeanBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v1'\n",
    "lr_base = 0.0001\n",
    "lr_decay = 0.0001\n",
    "\n",
    "random_seed = 42\n",
    "gamma = 0.99                # discount for future rewards\n",
    "batch_size = 64         # num of transitions sampled from replay buffer\n",
    "\n",
    "max_episodes = 100000         # max num of episodes\n",
    "max_timesteps = 3000        # max timesteps in one episode\n",
    "log_interval = 50           # print avg reward after interval\n",
    "\n",
    "entropy_beta = 0.01\n",
    "bellman_steps = 10\n",
    "baseline_steps = 50000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = [\n",
    "     {'dim': [None, 128], 'dropout': False, 'activation': 'relu'},    \n",
    "     {'dim': [128, None], 'dropout': False, 'activation': False},    \n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PG_Trainer():\n",
    "    \n",
    "    def __init__(self, env_name, config, random_seed=42, lr_base=0.001, lr_decay=0.00005, \n",
    "                 gamma=0.99, batch_size=32, \n",
    "                 max_episodes=100000, max_timesteps=3000, \n",
    "                 log_interval=5, threshold=None, lr_minimum=1e-10, \n",
    "                 entropy_beta=0.01, bellman_steps=10, baseline_steps=50000):\n",
    "                \n",
    "        self.algorithm_name = 'pg'\n",
    "        self.env_name = env_name\n",
    "        self.env = gym.make(env_name)\n",
    "               \n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.n    \n",
    "        self.should_record = False\n",
    "        if not threshold == None:\n",
    "            self.threshold = threshold\n",
    "        else:    \n",
    "            self.threshold = self.env.spec.reward_threshold\n",
    "              \n",
    "        self.config = config\n",
    "        self.config[0]['dim'][0] = self.state_dim\n",
    "        self.config[-1]['dim'][1] = self.action_dim      \n",
    "        \n",
    "        \n",
    "        self.random_seed = random_seed\n",
    "        self.lr_base = lr_base\n",
    "        self.lr_decay = lr_decay\n",
    "        self.lr_minimum = lr_minimum        \n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size   \n",
    "        \n",
    "        self.entropy_beta = entropy_beta\n",
    "        self.bellman_steps = bellman_steps\n",
    "        self.baseline_steps = baseline_steps        \n",
    "        \n",
    "        self.max_episodes = max_episodes\n",
    "        self.max_timesteps = max_timesteps\n",
    "        self.log_interval = log_interval\n",
    "        \n",
    "        self.logdir = mkdir('.', 'log')\n",
    "        \n",
    "        prdir = mkdir('.', 'preTrained')\n",
    "        self.directory = mkdir(prdir, self.algorithm_name)\n",
    "        self.filename = \"{}_{}_{}\".format(self.algorithm_name, self.env_name, self.random_seed)\n",
    "                       \n",
    "        self.policy = PG(self.env, self.config, self.gamma, self.bellman_steps)   \n",
    "        \n",
    "        # The experience source interacts with the environment and returns (s,a,r,s') transitions\n",
    "        self.exp_source = ptan.experience.ExperienceSourceFirstLast(self.env, self.policy.ptan_agent,\n",
    "                                                                    gamma=self.gamma,\n",
    "                                                                    steps_count=self.bellman_steps)\n",
    "        \n",
    "        self.baseline_buffer = MeanBuffer(self.baseline_steps)\n",
    "      \n",
    "        \n",
    "        self.reward_history = []\n",
    "        self.make_plots = False       \n",
    "        \n",
    "        if self.random_seed:\n",
    "            print(\"Random Seed: {}\".format(self.random_seed))\n",
    "            self.env.seed(self.random_seed)\n",
    "            torch.manual_seed(self.random_seed)\n",
    "            np.random.seed(self.random_seed)\n",
    "            \n",
    "    def train(self):\n",
    "        \n",
    "        start_time = time.time()        \n",
    "        print(\"action_space={}\".format(self.env.action_space))\n",
    "        print(\"obs_space={}\".format(self.env.observation_space))\n",
    "        print(\"threshold={} \\n\".format(self.threshold))        \n",
    "\n",
    "        # loading models\n",
    "        self.policy.load(self.directory, self.filename)\n",
    "                \n",
    "        # logging variables:        \n",
    "        log_f = open(os.path.join(self.logdir, \"train_{}_{}.txt\".format(self.algorithm_name, self.random_seed)),\n",
    "                     \"w+\")\n",
    "        \n",
    "        print(\"\\nTraining started ... \")\n",
    "        avg_loss = 0\n",
    "        total_rewards = []\n",
    "        step_rewards = []        \n",
    "        step_idx = 0\n",
    "        episode = 0\n",
    "              \n",
    "        batch_states, batch_actions, batch_scales = [], [], []\n",
    "        learning_rate = self.lr_base\n",
    "        self.policy.set_optimizers(lr=learning_rate)\n",
    "\n",
    "        # each iteration runs one action in the environment and returns a (s,a,r,s') transition\n",
    "        for step_idx, exp in enumerate(self.exp_source):\n",
    "            self.baseline_buffer.add(exp.reward)\n",
    "            baseline = self.baseline_buffer.mean()\n",
    "            #writer.add_scalar(\"baseline\", baseline, step_idx)\n",
    "            \n",
    "            batch_states.append(exp.state)\n",
    "            batch_actions.append(int(exp.action))\n",
    "            batch_scales.append(exp.reward - baseline)\n",
    "\n",
    "            # handle when an episode is completed\n",
    "            episode_rewards = self.exp_source.pop_total_rewards()\n",
    "            if episode_rewards:\n",
    "                episode += 1\n",
    "                reward = episode_rewards[0]\n",
    "                total_rewards.append(reward)\n",
    "                avg_reward = float(np.mean(total_rewards[-100:]))\n",
    "                \n",
    "                if len(self.policy.loss_list) > 0:               \n",
    "                    avg_loss = np.mean(self.policy.loss_list[-100:])     \n",
    "                \n",
    "                # Print avg reward every log interval:\n",
    "                if episode % self.log_interval == 0:            \n",
    "                    self.policy.save(self.directory, self.filename)\n",
    "                    print(\"Ep:{:5d}  Rew:{:8.2f}  Avg Rew:{:8.2f}  LR:{:8.8f}  Loss: {:8.6f}\".format(\n",
    "                        episode, reward, avg_reward, learning_rate, avg_loss))\n",
    "                \n",
    "                # logging updates:        \n",
    "                log_f.write('{},{}\\n'.format(episode, reward))\n",
    "                log_f.flush()\n",
    "                \n",
    "                learning_rate = max(self.lr_base / (1.0 + episode * self.lr_decay), self.lr_minimum) \n",
    "                \n",
    "                              \n",
    "                #writer.add_scalar(\"reward\", reward, step_idx)\n",
    "                #writer.add_scalar(\"reward_100\", mean_rewards, step_idx)\n",
    "                #writer.add_scalar(\"episodes\", done_episodes, step_idx)\n",
    "                \n",
    "                # if avg reward > threshold then save and stop traning:\n",
    "                if avg_reward >= self.threshold and episode > 100: \n",
    "                    print(\"Ep:{:5d}  Rew:{:8.2f}  Avg Rew:{:8.2f}  LR:{:8.8f}  Loss: {:8.6f}\".format(\n",
    "                        episode, reward, avg_reward, learning_rate, avg_loss))\n",
    "                    print(\"########## Solved! ###########\")\n",
    "                    name = self.filename + '_solved'\n",
    "                    self.policy.save(self.directory, name)\n",
    "                    log_f.close()\n",
    "                    self.env.close()  \n",
    "                    training_time = time.time() - start_time\n",
    "                    print(\"Training time: {:6.2f} sec\".format(training_time))\n",
    "                    break    \n",
    "\n",
    "            if len(batch_states) < self.batch_size:\n",
    "                continue\n",
    "            \n",
    "            self.policy.update(batch_states, batch_actions, batch_scales, self.batch_size, self.entropy_beta)        \n",
    "                    \n",
    "            \n",
    "\n",
    "            '''    \n",
    "            writer.add_scalar(\"baseline\", baseline, step_idx)\n",
    "            writer.add_scalar(\"entropy\", entropy_v.item(), step_idx)\n",
    "            writer.add_scalar(\"batch_scales\", np.mean(batch_scales), step_idx)\n",
    "            writer.add_scalar(\"loss_entropy\", entropy_loss_v.item(), step_idx)\n",
    "            writer.add_scalar(\"loss_policy\", loss_policy_v.item(), step_idx)\n",
    "            writer.add_scalar(\"loss_total\", loss_v.item(), step_idx)\n",
    "            writer.add_scalar(\"grad_l2\", grad_means / grad_count, step_idx)\n",
    "            writer.add_scalar(\"grad_max\", grad_max, step_idx)\n",
    "            '''\n",
    "\n",
    "            batch_states.clear()\n",
    "            batch_actions.clear()\n",
    "            batch_scales.clear()\n",
    "\n",
    "            \n",
    "                                \n",
    "    def test(self, episodes=3, render=True, save_gif=True):              \n",
    "\n",
    "        gifdir = mkdir('.','gif')\n",
    "        algdir = mkdir(gifdir, self.algorithm_name)\n",
    "        \n",
    "        t = 0\n",
    "        for episode in range(1, episodes+1):\n",
    "            ep_reward = 0.0            \n",
    "            epdir = mkdir(algdir, str(episode))\n",
    "        \n",
    "            for step_idx, exp in enumerate(self.exp_source):\n",
    "                self.baseline_buffer.add(exp.reward)\n",
    "                baseline = self.baseline_buffer.mean()\n",
    "                #writer.add_scalar(\"baseline\", baseline, step_idx)\n",
    "\n",
    "                if save_gif:                                       \n",
    "                    img = self.env.render(mode = 'rgb_array')\n",
    "                    img = Image.fromarray(img)\n",
    "                    img.save('{}/{}.jpg'.format(epdir, t))\n",
    "                t+= 1\n",
    "                    \n",
    "                # handle when an episode is completed\n",
    "                episode_rewards = self.exp_source.pop_total_rewards()\n",
    "                if episode_rewards:\n",
    "                    ep_reward = episode_rewards[0]\n",
    "                    t = 0\n",
    "                    break\n",
    "        \n",
    "            print('Test episode: {}\\tReward: {:4.2f}'.format(episode, ep_reward))           \n",
    "            self.env.close()        \n",
    "                   \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NETWORK: Sequential(\n",
      "  (0): Linear(in_features=4, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=2, bias=True)\n",
      ") Device: cpu\n",
      "Random Seed: 42\n",
      "action_space=Discrete(2)\n",
      "obs_space=Box(4,)\n",
      "threshold=475.0 \n",
      "\n",
      "DIR=./preTrained/pg NAME=pg_CartPole-v1_42\n",
      "No models to load\n",
      "\n",
      "Training started ... \n",
      "Ep:   50  Rew:   11.00  Avg Rew:   21.56  LR:0.00009951  Loss: -0.014341\n",
      "Ep:  100  Rew:   13.00  Avg Rew:   20.72  LR:0.00009902  Loss: -0.028912\n",
      "Ep:  150  Rew:    9.00  Avg Rew:   19.79  LR:0.00009853  Loss: -0.045848\n",
      "Ep:  200  Rew:   10.00  Avg Rew:   19.30  LR:0.00009805  Loss: -0.052478\n",
      "Ep:  250  Rew:   29.00  Avg Rew:   19.71  LR:0.00009757  Loss: -0.033148\n",
      "Ep:  300  Rew:   26.00  Avg Rew:   22.11  LR:0.00009710  Loss: 0.013484\n",
      "Ep:  350  Rew:    9.00  Avg Rew:   21.86  LR:0.00009663  Loss: 0.004037\n",
      "Ep:  400  Rew:   30.00  Avg Rew:   21.01  LR:0.00009616  Loss: 0.011181\n",
      "Ep:  450  Rew:   17.00  Avg Rew:   21.15  LR:0.00009570  Loss: 0.019755\n",
      "Ep:  500  Rew:   13.00  Avg Rew:   21.87  LR:0.00009525  Loss: 0.072612\n",
      "Ep:  550  Rew:   47.00  Avg Rew:   24.24  LR:0.00009480  Loss: 0.078864\n",
      "Ep:  600  Rew:   25.00  Avg Rew:   24.53  LR:0.00009435  Loss: 0.085934\n",
      "Ep:  650  Rew:   40.00  Avg Rew:   22.87  LR:0.00009391  Loss: 0.087125\n",
      "Ep:  700  Rew:   14.00  Avg Rew:   25.13  LR:0.00009347  Loss: 0.142081\n",
      "Ep:  750  Rew:   39.00  Avg Rew:   26.68  LR:0.00009303  Loss: 0.141765\n",
      "Ep:  800  Rew:   14.00  Avg Rew:   26.44  LR:0.00009260  Loss: 0.150095\n",
      "Ep:  850  Rew:   21.00  Avg Rew:   25.11  LR:0.00009217  Loss: 0.111815\n",
      "Ep:  900  Rew:   15.00  Avg Rew:   25.46  LR:0.00009175  Loss: 0.148557\n",
      "Ep:  950  Rew:   96.00  Avg Rew:   29.56  LR:0.00009133  Loss: 0.157935\n",
      "Ep: 1000  Rew:   21.00  Avg Rew:   28.33  LR:0.00009092  Loss: 0.145062\n",
      "Ep: 1050  Rew:   34.00  Avg Rew:   29.95  LR:0.00009051  Loss: 0.202732\n",
      "Ep: 1100  Rew:   54.00  Avg Rew:   31.24  LR:0.00009010  Loss: 0.198556\n",
      "Ep: 1150  Rew:   14.00  Avg Rew:   30.61  LR:0.00008969  Loss: 0.218813\n",
      "Ep: 1200  Rew:   15.00  Avg Rew:   29.88  LR:0.00008929  Loss: 0.203103\n",
      "Ep: 1250  Rew:   24.00  Avg Rew:   30.47  LR:0.00008890  Loss: 0.186107\n",
      "Ep: 1300  Rew:   47.00  Avg Rew:   33.18  LR:0.00008850  Loss: 0.194811\n",
      "Ep: 1350  Rew:   21.00  Avg Rew:   31.23  LR:0.00008811  Loss: 0.155933\n",
      "Ep: 1400  Rew:   60.00  Avg Rew:   34.10  LR:0.00008773  Loss: 0.227469\n",
      "Ep: 1450  Rew:   45.00  Avg Rew:   37.88  LR:0.00008734  Loss: 0.242211\n",
      "Ep: 1500  Rew:   16.00  Avg Rew:   38.12  LR:0.00008696  Loss: 0.269273\n",
      "Ep: 1550  Rew:   25.00  Avg Rew:   36.89  LR:0.00008659  Loss: 0.263966\n",
      "Ep: 1600  Rew:   15.00  Avg Rew:   36.66  LR:0.00008621  Loss: 0.243651\n",
      "Ep: 1650  Rew:   65.00  Avg Rew:   40.85  LR:0.00008584  Loss: 0.234476\n",
      "Ep: 1700  Rew:   22.00  Avg Rew:   42.15  LR:0.00008548  Loss: 0.269545\n",
      "Ep: 1750  Rew:   59.00  Avg Rew:   43.67  LR:0.00008511  Loss: 0.285604\n",
      "Ep: 1800  Rew:   23.00  Avg Rew:   43.60  LR:0.00008475  Loss: 0.265282\n",
      "Ep: 1850  Rew:   50.00  Avg Rew:   44.61  LR:0.00008440  Loss: 0.277582\n",
      "Ep: 1900  Rew:   46.00  Avg Rew:   43.82  LR:0.00008404  Loss: 0.196095\n",
      "Ep: 1950  Rew:   65.00  Avg Rew:   46.15  LR:0.00008369  Loss: 0.210236\n",
      "Ep: 2000  Rew:   33.00  Avg Rew:   49.25  LR:0.00008334  Loss: 0.191960\n",
      "Ep: 2050  Rew:   34.00  Avg Rew:   47.68  LR:0.00008299  Loss: 0.183990\n",
      "Ep: 2100  Rew:   45.00  Avg Rew:   50.76  LR:0.00008265  Loss: 0.160256\n",
      "Ep: 2150  Rew:   17.00  Avg Rew:   55.36  LR:0.00008231  Loss: 0.184281\n",
      "Ep: 2200  Rew:  165.00  Avg Rew:   60.51  LR:0.00008197  Loss: 0.187379\n",
      "Ep: 2250  Rew:   52.00  Avg Rew:   63.06  LR:0.00008164  Loss: 0.184459\n",
      "Ep: 2300  Rew:  106.00  Avg Rew:   63.71  LR:0.00008131  Loss: 0.159796\n",
      "Ep: 2350  Rew:  122.00  Avg Rew:   66.25  LR:0.00008098  Loss: 0.133104\n",
      "Ep: 2400  Rew:   32.00  Avg Rew:   69.50  LR:0.00008065  Loss: 0.113968\n",
      "Ep: 2450  Rew:   39.00  Avg Rew:   70.12  LR:0.00008033  Loss: 0.092011\n",
      "Ep: 2500  Rew:  118.00  Avg Rew:   75.71  LR:0.00008001  Loss: 0.127852\n",
      "Ep: 2550  Rew:   41.00  Avg Rew:   75.03  LR:0.00007969  Loss: 0.082139\n",
      "Ep: 2600  Rew:   35.00  Avg Rew:   73.48  LR:0.00007937  Loss: 0.084887\n",
      "Ep: 2650  Rew:   35.00  Avg Rew:   84.88  LR:0.00007906  Loss: 0.117520\n",
      "Ep: 2700  Rew:  106.00  Avg Rew:   96.05  LR:0.00007875  Loss: 0.077286\n",
      "Ep: 2750  Rew:  266.00  Avg Rew:   98.74  LR:0.00007844  Loss: 0.084679\n",
      "Ep: 2800  Rew:  111.00  Avg Rew:   98.08  LR:0.00007813  Loss: 0.088788\n",
      "Ep: 2850  Rew:   95.00  Avg Rew:  107.51  LR:0.00007783  Loss: 0.057801\n",
      "Ep: 2900  Rew:  183.00  Avg Rew:  111.11  LR:0.00007753  Loss: 0.038578\n",
      "Ep: 2950  Rew:  125.00  Avg Rew:  110.66  LR:0.00007723  Loss: 0.035010\n",
      "Ep: 3000  Rew:  217.00  Avg Rew:  127.93  LR:0.00007693  Loss: 0.062235\n",
      "Ep: 3050  Rew:  102.00  Avg Rew:  143.84  LR:0.00007663  Loss: 0.040055\n",
      "Ep: 3100  Rew:  126.00  Avg Rew:  133.79  LR:0.00007634  Loss: -0.019887\n",
      "Ep: 3150  Rew:  128.00  Avg Rew:  127.45  LR:0.00007605  Loss: 0.000623\n",
      "Ep: 3200  Rew:  228.00  Avg Rew:  151.50  LR:0.00007576  Loss: 0.026898\n",
      "Ep: 3250  Rew:  195.00  Avg Rew:  161.96  LR:0.00007548  Loss: 0.026013\n",
      "Ep: 3300  Rew:  112.00  Avg Rew:  152.97  LR:0.00007519  Loss: -0.019264\n",
      "Ep: 3350  Rew:  196.00  Avg Rew:  165.73  LR:0.00007491  Loss: 0.014598\n",
      "Ep: 3400  Rew:   77.00  Avg Rew:  167.80  LR:0.00007463  Loss: -0.019166\n",
      "Ep: 3450  Rew:  199.00  Avg Rew:  168.07  LR:0.00007435  Loss: 0.013356\n",
      "Ep: 3500  Rew:   99.00  Avg Rew:  186.51  LR:0.00007408  Loss: 0.005981\n",
      "Ep: 3550  Rew:   35.00  Avg Rew:  198.71  LR:0.00007381  Loss: -0.010328\n",
      "Ep: 3600  Rew:  130.00  Avg Rew:  198.00  LR:0.00007353  Loss: -0.011883\n",
      "Ep: 3650  Rew:  225.00  Avg Rew:  188.87  LR:0.00007327  Loss: -0.018270\n",
      "Ep: 3700  Rew:  143.00  Avg Rew:  198.63  LR:0.00007300  Loss: -0.021198\n",
      "Ep: 3750  Rew:  285.00  Avg Rew:  214.66  LR:0.00007273  Loss: -0.001119\n",
      "Ep: 3800  Rew:  194.00  Avg Rew:  224.62  LR:0.00007247  Loss: -0.002968\n",
      "Ep: 3850  Rew:  276.00  Avg Rew:  208.72  LR:0.00007221  Loss: -0.024250\n",
      "Ep: 3900  Rew:  391.00  Avg Rew:  212.84  LR:0.00007195  Loss: -0.025878\n",
      "Ep: 3950  Rew:  500.00  Avg Rew:  246.17  LR:0.00007169  Loss: -0.003741\n",
      "Ep: 4000  Rew:  104.00  Avg Rew:  228.52  LR:0.00007143  Loss: -0.026565\n",
      "Ep: 4050  Rew:  367.00  Avg Rew:  229.03  LR:0.00007118  Loss: 0.015558\n",
      "Ep: 4100  Rew:  180.00  Avg Rew:  245.66  LR:0.00007093  Loss: -0.018773\n",
      "Ep: 4150  Rew:  436.00  Avg Rew:  258.39  LR:0.00007068  Loss: 0.015086\n",
      "Ep: 4200  Rew:  253.00  Avg Rew:  272.63  LR:0.00007043  Loss: -0.023314\n",
      "Ep: 4250  Rew:  443.00  Avg Rew:  260.13  LR:0.00007018  Loss: 0.009323\n",
      "Ep: 4300  Rew:  186.00  Avg Rew:  259.99  LR:0.00006993  Loss: -0.007672\n",
      "Ep: 4350  Rew:  370.00  Avg Rew:  257.29  LR:0.00006969  Loss: -0.005011\n",
      "Ep: 4400  Rew:  206.00  Avg Rew:  271.72  LR:0.00006945  Loss: 0.000226\n",
      "Ep: 4450  Rew:  219.00  Avg Rew:  292.72  LR:0.00006921  Loss: -0.010594\n",
      "Ep: 4500  Rew:  298.00  Avg Rew:  284.35  LR:0.00006897  Loss: -0.014777\n",
      "Ep: 4550  Rew:  160.00  Avg Rew:  271.88  LR:0.00006873  Loss: -0.035031\n",
      "Ep: 4600  Rew:  190.00  Avg Rew:  262.10  LR:0.00006850  Loss: -0.029335\n",
      "Ep: 4650  Rew:  271.00  Avg Rew:  247.95  LR:0.00006826  Loss: -0.030337\n",
      "Ep: 4700  Rew:  219.00  Avg Rew:  224.28  LR:0.00006803  Loss: -0.022454\n",
      "Ep: 4750  Rew:  376.00  Avg Rew:  232.76  LR:0.00006780  Loss: -0.001570\n",
      "Ep: 4800  Rew:  448.00  Avg Rew:  268.92  LR:0.00006757  Loss: 0.001995\n",
      "Ep: 4850  Rew:  500.00  Avg Rew:  276.27  LR:0.00006734  Loss: -0.000499\n",
      "Ep: 4900  Rew:  142.00  Avg Rew:  272.54  LR:0.00006712  Loss: -0.001700\n",
      "Ep: 4950  Rew:  237.00  Avg Rew:  260.14  LR:0.00006689  Loss: -0.008831\n",
      "Ep: 5000  Rew:  175.00  Avg Rew:  259.90  LR:0.00006667  Loss: -0.005257\n",
      "Ep: 5050  Rew:  185.00  Avg Rew:  273.08  LR:0.00006645  Loss: 0.002712\n",
      "Ep: 5100  Rew:  268.00  Avg Rew:  264.66  LR:0.00006623  Loss: -0.011188\n",
      "Ep: 5150  Rew:  500.00  Avg Rew:  278.37  LR:0.00006601  Loss: -0.010173\n",
      "Ep: 5200  Rew:  312.00  Avg Rew:  304.79  LR:0.00006579  Loss: -0.003146\n",
      "Ep: 5250  Rew:  340.00  Avg Rew:  299.71  LR:0.00006558  Loss: -0.014612\n",
      "Ep: 5300  Rew:  500.00  Avg Rew:  293.39  LR:0.00006536  Loss: 0.001468\n",
      "Ep: 5350  Rew:  500.00  Avg Rew:  295.87  LR:0.00006515  Loss: -0.008423\n",
      "Ep: 5400  Rew:  500.00  Avg Rew:  307.77  LR:0.00006494  Loss: -0.012649\n",
      "Ep: 5450  Rew:  260.00  Avg Rew:  328.59  LR:0.00006473  Loss: -0.003413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 5500  Rew:  198.00  Avg Rew:  314.96  LR:0.00006452  Loss: -0.010073\n",
      "Ep: 5550  Rew:  311.00  Avg Rew:  308.27  LR:0.00006431  Loss: -0.010473\n",
      "Ep: 5600  Rew:  321.00  Avg Rew:  335.25  LR:0.00006411  Loss: -0.005752\n",
      "Ep: 5650  Rew:  467.00  Avg Rew:  309.43  LR:0.00006390  Loss: -0.026763\n",
      "Ep: 5700  Rew:  200.00  Avg Rew:  261.88  LR:0.00006370  Loss: -0.026570\n",
      "Ep: 5750  Rew:  290.00  Avg Rew:  253.05  LR:0.00006350  Loss: -0.017915\n",
      "Ep: 5800  Rew:  263.00  Avg Rew:  268.68  LR:0.00006330  Loss: -0.007003\n",
      "Ep: 5850  Rew:  342.00  Avg Rew:  288.41  LR:0.00006310  Loss: 0.014755\n",
      "Ep: 5900  Rew:  303.00  Avg Rew:  277.39  LR:0.00006290  Loss: -0.010873\n",
      "Ep: 5950  Rew:  181.00  Avg Rew:  263.45  LR:0.00006270  Loss: -0.029372\n",
      "Ep: 6000  Rew:  344.00  Avg Rew:  254.31  LR:0.00006250  Loss: -0.023504\n",
      "Ep: 6050  Rew:  123.00  Avg Rew:  260.29  LR:0.00006231  Loss: -0.005409\n",
      "Ep: 6100  Rew:  153.00  Avg Rew:  277.12  LR:0.00006212  Loss: 0.002269\n",
      "Ep: 6150  Rew:  200.00  Avg Rew:  271.49  LR:0.00006192  Loss: -0.031950\n",
      "Ep: 6200  Rew:  165.00  Avg Rew:  272.55  LR:0.00006173  Loss: -0.005038\n",
      "Ep: 6250  Rew:  264.00  Avg Rew:  273.49  LR:0.00006154  Loss: 0.000521\n",
      "Ep: 6300  Rew:  359.00  Avg Rew:  254.54  LR:0.00006135  Loss: -0.018946\n",
      "Ep: 6350  Rew:  500.00  Avg Rew:  255.42  LR:0.00006117  Loss: -0.015305\n",
      "Ep: 6400  Rew:  345.00  Avg Rew:  261.02  LR:0.00006098  Loss: -0.017219\n",
      "Ep: 6450  Rew:  316.00  Avg Rew:  264.04  LR:0.00006079  Loss: 0.003739\n",
      "Ep: 6500  Rew:  332.00  Avg Rew:  296.01  LR:0.00006061  Loss: -0.004334\n",
      "Ep: 6550  Rew:  235.00  Avg Rew:  316.17  LR:0.00006043  Loss: -0.002693\n",
      "Ep: 6600  Rew:  302.00  Avg Rew:  313.25  LR:0.00006024  Loss: -0.017288\n",
      "Ep: 6650  Rew:  184.00  Avg Rew:  306.48  LR:0.00006006  Loss: -0.009401\n",
      "Ep: 6700  Rew:  240.00  Avg Rew:  290.76  LR:0.00005988  Loss: -0.012624\n",
      "Ep: 6750  Rew:  154.00  Avg Rew:  315.74  LR:0.00005971  Loss: 0.000943\n",
      "Ep: 6800  Rew:  235.00  Avg Rew:  327.31  LR:0.00005953  Loss: -0.007353\n",
      "Ep: 6850  Rew:  332.00  Avg Rew:  298.29  LR:0.00005935  Loss: -0.015361\n",
      "Ep: 6900  Rew:  203.00  Avg Rew:  302.36  LR:0.00005918  Loss: -0.021242\n",
      "Ep: 6950  Rew:  184.00  Avg Rew:  296.40  LR:0.00005900  Loss: -0.012354\n",
      "Ep: 7000  Rew:  300.00  Avg Rew:  264.33  LR:0.00005883  Loss: -0.017376\n",
      "Ep: 7050  Rew:  224.00  Avg Rew:  244.42  LR:0.00005865  Loss: -0.017528\n",
      "Ep: 7100  Rew:  258.00  Avg Rew:  260.73  LR:0.00005848  Loss: -0.011071\n",
      "Ep: 7150  Rew:  401.00  Avg Rew:  277.78  LR:0.00005831  Loss: 0.001767\n",
      "Ep: 7200  Rew:  230.00  Avg Rew:  286.00  LR:0.00005814  Loss: -0.008265\n",
      "Ep: 7250  Rew:  244.00  Avg Rew:  290.80  LR:0.00005797  Loss: -0.011004\n",
      "Ep: 7300  Rew:  329.00  Avg Rew:  281.69  LR:0.00005781  Loss: -0.007859\n",
      "Ep: 7350  Rew:  168.00  Avg Rew:  269.80  LR:0.00005764  Loss: -0.009883\n",
      "Ep: 7400  Rew:  500.00  Avg Rew:  265.09  LR:0.00005747  Loss: -0.002090\n",
      "Ep: 7450  Rew:  225.00  Avg Rew:  269.44  LR:0.00005731  Loss: -0.012859\n",
      "Ep: 7500  Rew:  189.00  Avg Rew:  258.67  LR:0.00005715  Loss: -0.017599\n",
      "Ep: 7550  Rew:  500.00  Avg Rew:  274.46  LR:0.00005698  Loss: 0.010024\n",
      "Ep: 7600  Rew:  157.00  Avg Rew:  291.43  LR:0.00005682  Loss: -0.012578\n",
      "Ep: 7650  Rew:  275.00  Avg Rew:  286.48  LR:0.00005666  Loss: 0.009504\n",
      "Ep: 7700  Rew:  500.00  Avg Rew:  293.66  LR:0.00005650  Loss: -0.003986\n",
      "Ep: 7750  Rew:  395.00  Avg Rew:  304.91  LR:0.00005634  Loss: -0.007799\n",
      "Ep: 7800  Rew:  262.00  Avg Rew:  334.01  LR:0.00005618  Loss: -0.005314\n",
      "Ep: 7850  Rew:  500.00  Avg Rew:  351.64  LR:0.00005603  Loss: -0.001388\n",
      "Ep: 7900  Rew:  388.00  Avg Rew:  336.32  LR:0.00005587  Loss: -0.002773\n",
      "Ep: 7950  Rew:  214.00  Avg Rew:  328.52  LR:0.00005571  Loss: -0.010918\n",
      "Ep: 8000  Rew:  204.00  Avg Rew:  341.58  LR:0.00005556  Loss: -0.001694\n",
      "Ep: 8050  Rew:  318.00  Avg Rew:  323.35  LR:0.00005540  Loss: -0.019566\n",
      "Ep: 8100  Rew:  220.00  Avg Rew:  309.35  LR:0.00005525  Loss: -0.005426\n",
      "Ep: 8150  Rew:  190.00  Avg Rew:  328.43  LR:0.00005510  Loss: -0.004299\n",
      "Ep: 8200  Rew:  317.00  Avg Rew:  325.34  LR:0.00005495  Loss: -0.020281\n",
      "Ep: 8250  Rew:  419.00  Avg Rew:  299.89  LR:0.00005480  Loss: -0.015478\n",
      "Ep: 8300  Rew:  174.00  Avg Rew:  283.32  LR:0.00005465  Loss: -0.008081\n",
      "Ep: 8350  Rew:  495.00  Avg Rew:  287.18  LR:0.00005450  Loss: 0.000063\n",
      "Ep: 8400  Rew:  361.00  Avg Rew:  308.98  LR:0.00005435  Loss: -0.003178\n",
      "Ep: 8450  Rew:  457.00  Avg Rew:  332.08  LR:0.00005420  Loss: -0.010760\n",
      "Ep: 8500  Rew:  235.00  Avg Rew:  332.45  LR:0.00005406  Loss: -0.007808\n",
      "Ep: 8550  Rew:  316.00  Avg Rew:  328.06  LR:0.00005391  Loss: 0.000570\n",
      "Ep: 8600  Rew:  186.00  Avg Rew:  318.57  LR:0.00005377  Loss: -0.016615\n",
      "Ep: 8650  Rew:  341.00  Avg Rew:  299.09  LR:0.00005362  Loss: -0.013278\n",
      "Ep: 8700  Rew:  145.00  Avg Rew:  295.38  LR:0.00005348  Loss: -0.008640\n",
      "Ep: 8750  Rew:  266.00  Avg Rew:  296.04  LR:0.00005334  Loss: -0.017383\n",
      "Ep: 8800  Rew:  332.00  Avg Rew:  287.45  LR:0.00005319  Loss: -0.011704\n",
      "Ep: 8850  Rew:  350.00  Avg Rew:  271.56  LR:0.00005305  Loss: -0.016522\n",
      "Ep: 8900  Rew:  190.00  Avg Rew:  271.92  LR:0.00005291  Loss: -0.001243\n",
      "Ep: 8950  Rew:  193.00  Avg Rew:  287.15  LR:0.00005277  Loss: -0.009290\n",
      "Ep: 9000  Rew:  315.00  Avg Rew:  313.22  LR:0.00005263  Loss: -0.007273\n",
      "Ep: 9050  Rew:  254.00  Avg Rew:  321.86  LR:0.00005250  Loss: -0.012258\n",
      "Ep: 9100  Rew:  262.00  Avg Rew:  324.02  LR:0.00005236  Loss: -0.015736\n",
      "Ep: 9150  Rew:  497.00  Avg Rew:  340.26  LR:0.00005222  Loss: 0.002760\n",
      "Ep: 9200  Rew:  229.00  Avg Rew:  337.84  LR:0.00005209  Loss: -0.009518\n",
      "Ep: 9250  Rew:  144.00  Avg Rew:  324.67  LR:0.00005195  Loss: -0.005005\n",
      "Ep: 9300  Rew:  380.00  Avg Rew:  330.21  LR:0.00005182  Loss: -0.007813\n",
      "Ep: 9350  Rew:  333.00  Avg Rew:  331.55  LR:0.00005168  Loss: -0.003873\n",
      "Ep: 9400  Rew:  250.00  Avg Rew:  329.44  LR:0.00005155  Loss: -0.002190\n",
      "Ep: 9450  Rew:  300.00  Avg Rew:  351.39  LR:0.00005142  Loss: -0.005952\n",
      "Ep: 9500  Rew:  500.00  Avg Rew:  367.76  LR:0.00005128  Loss: -0.002770\n",
      "Ep: 9550  Rew:  371.00  Avg Rew:  369.76  LR:0.00005115  Loss: -0.002330\n",
      "Ep: 9600  Rew:  500.00  Avg Rew:  353.39  LR:0.00005102  Loss: -0.020476\n",
      "Ep: 9650  Rew:  500.00  Avg Rew:  334.67  LR:0.00005089  Loss: -0.007775\n",
      "Ep: 9700  Rew:  271.00  Avg Rew:  331.84  LR:0.00005076  Loss: 0.000000\n",
      "Ep: 9750  Rew:  413.00  Avg Rew:  340.06  LR:0.00005064  Loss: -0.009912\n",
      "Ep: 9800  Rew:  389.00  Avg Rew:  348.83  LR:0.00005051  Loss: -0.004690\n",
      "Ep: 9850  Rew:  409.00  Avg Rew:  359.62  LR:0.00005038  Loss: -0.001461\n",
      "Ep: 9900  Rew:  334.00  Avg Rew:  363.06  LR:0.00005025  Loss: -0.018693\n",
      "Ep: 9950  Rew:  326.00  Avg Rew:  311.47  LR:0.00005013  Loss: -0.028284\n",
      "Ep:10000  Rew:  471.00  Avg Rew:  279.34  LR:0.00005000  Loss: -0.005779\n",
      "Ep:10050  Rew:  500.00  Avg Rew:  310.78  LR:0.00004988  Loss: -0.001553\n",
      "Ep:10100  Rew:  228.00  Avg Rew:  342.88  LR:0.00004975  Loss: 0.000560\n",
      "Ep:10150  Rew:  340.00  Avg Rew:  364.53  LR:0.00004963  Loss: 0.005936\n",
      "Ep:10200  Rew:  500.00  Avg Rew:  366.46  LR:0.00004951  Loss: -0.004178\n",
      "Ep:10250  Rew:  500.00  Avg Rew:  376.57  LR:0.00004939  Loss: -0.002935\n",
      "Ep:10300  Rew:  282.00  Avg Rew:  395.45  LR:0.00004926  Loss: -0.007431\n",
      "Ep:10350  Rew:  500.00  Avg Rew:  402.76  LR:0.00004914  Loss: 0.006563\n",
      "Ep:10400  Rew:  261.00  Avg Rew:  400.70  LR:0.00004902  Loss: -0.009405\n",
      "Ep:10450  Rew:  436.00  Avg Rew:  392.90  LR:0.00004890  Loss: 0.000885\n",
      "Ep:10500  Rew:  500.00  Avg Rew:  415.15  LR:0.00004878  Loss: -0.001413\n",
      "Ep:10550  Rew:  500.00  Avg Rew:  436.96  LR:0.00004866  Loss: -0.008399\n",
      "Ep:10600  Rew:  500.00  Avg Rew:  442.57  LR:0.00004855  Loss: 0.002141\n",
      "Ep:10650  Rew:  500.00  Avg Rew:  437.28  LR:0.00004843  Loss: -0.004898\n",
      "Ep:10700  Rew:  500.00  Avg Rew:  425.91  LR:0.00004831  Loss: -0.004619\n",
      "Ep:10750  Rew:  455.00  Avg Rew:  427.58  LR:0.00004820  Loss: -0.009554\n",
      "Ep:10800  Rew:  500.00  Avg Rew:  407.37  LR:0.00004808  Loss: -0.016884\n",
      "Ep:10850  Rew:  278.00  Avg Rew:  381.87  LR:0.00004796  Loss: -0.016522\n",
      "Ep:10900  Rew:  500.00  Avg Rew:  337.60  LR:0.00004785  Loss: -0.020987\n",
      "Ep:10950  Rew:  302.00  Avg Rew:  278.92  LR:0.00004773  Loss: -0.038376\n",
      "Ep:11000  Rew:  216.00  Avg Rew:  284.28  LR:0.00004762  Loss: -0.015798\n",
      "Ep:11050  Rew:  218.00  Avg Rew:  318.09  LR:0.00004751  Loss: 0.007158\n",
      "Ep:11100  Rew:  223.00  Avg Rew:  344.87  LR:0.00004740  Loss: 0.005045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep:11150  Rew:  354.00  Avg Rew:  344.77  LR:0.00004728  Loss: -0.014857\n",
      "Ep:11200  Rew:  500.00  Avg Rew:  320.44  LR:0.00004717  Loss: -0.025516\n",
      "Ep:11250  Rew:  289.00  Avg Rew:  303.05  LR:0.00004706  Loss: -0.021524\n",
      "Ep:11300  Rew:  500.00  Avg Rew:  324.69  LR:0.00004695  Loss: 0.011951\n",
      "Ep:11350  Rew:  286.00  Avg Rew:  343.53  LR:0.00004684  Loss: -0.015788\n",
      "Ep:11400  Rew:  281.00  Avg Rew:  344.32  LR:0.00004673  Loss: -0.011324\n",
      "Ep:11450  Rew:  500.00  Avg Rew:  349.31  LR:0.00004662  Loss: -0.011057\n",
      "Ep:11500  Rew:  306.00  Avg Rew:  373.29  LR:0.00004651  Loss: 0.004462\n",
      "Ep:11550  Rew:  486.00  Avg Rew:  392.19  LR:0.00004641  Loss: -0.001847\n",
      "Ep:11600  Rew:  357.00  Avg Rew:  329.92  LR:0.00004630  Loss: -0.036404\n",
      "Ep:11650  Rew:  255.00  Avg Rew:  296.01  LR:0.00004619  Loss: -0.011150\n",
      "Ep:11700  Rew:  250.00  Avg Rew:  301.10  LR:0.00004609  Loss: -0.021540\n",
      "Ep:11750  Rew:  473.00  Avg Rew:  280.71  LR:0.00004598  Loss: -0.025249\n",
      "Ep:11800  Rew:  239.00  Avg Rew:  270.01  LR:0.00004587  Loss: -0.012479\n",
      "Ep:11850  Rew:  158.00  Avg Rew:  264.56  LR:0.00004577  Loss: -0.015231\n",
      "Ep:11900  Rew:  230.00  Avg Rew:  288.56  LR:0.00004566  Loss: 0.007676\n",
      "Ep:11950  Rew:  344.00  Avg Rew:  327.81  LR:0.00004556  Loss: -0.007192\n",
      "Ep:12000  Rew:  143.00  Avg Rew:  310.92  LR:0.00004546  Loss: -0.012618\n",
      "Ep:12050  Rew:  427.00  Avg Rew:  296.81  LR:0.00004535  Loss: -0.004311\n",
      "Ep:12100  Rew:  401.00  Avg Rew:  287.41  LR:0.00004525  Loss: -0.007464\n",
      "Ep:12150  Rew:  169.00  Avg Rew:  272.68  LR:0.00004515  Loss: -0.023435\n",
      "Ep:12200  Rew:  188.00  Avg Rew:  264.12  LR:0.00004505  Loss: -0.018653\n",
      "Ep:12250  Rew:  221.00  Avg Rew:  234.80  LR:0.00004495  Loss: -0.028573\n",
      "Ep:12300  Rew:  274.00  Avg Rew:  222.40  LR:0.00004485  Loss: -0.016839\n",
      "Ep:12350  Rew:  500.00  Avg Rew:  249.72  LR:0.00004474  Loss: 0.013502\n",
      "Ep:12400  Rew:  266.00  Avg Rew:  277.13  LR:0.00004464  Loss: 0.004846\n",
      "Ep:12450  Rew:  242.00  Avg Rew:  285.20  LR:0.00004455  Loss: 0.000109\n",
      "Ep:12500  Rew:  338.00  Avg Rew:  294.28  LR:0.00004445  Loss: -0.004553\n",
      "Ep:12550  Rew:  319.00  Avg Rew:  292.37  LR:0.00004435  Loss: -0.020725\n",
      "Ep:12600  Rew:  368.00  Avg Rew:  288.83  LR:0.00004425  Loss: -0.003038\n",
      "Ep:12650  Rew:  201.00  Avg Rew:  287.72  LR:0.00004415  Loss: -0.018452\n",
      "Ep:12700  Rew:  500.00  Avg Rew:  288.74  LR:0.00004405  Loss: -0.004470\n",
      "Ep:12750  Rew:  275.00  Avg Rew:  311.59  LR:0.00004396  Loss: -0.001410\n",
      "Ep:12800  Rew:  299.00  Avg Rew:  320.21  LR:0.00004386  Loss: -0.015857\n",
      "Ep:12850  Rew:  354.00  Avg Rew:  327.17  LR:0.00004377  Loss: -0.005719\n",
      "Ep:12900  Rew:  251.00  Avg Rew:  339.18  LR:0.00004367  Loss: -0.001188\n",
      "Ep:12950  Rew:  190.00  Avg Rew:  319.83  LR:0.00004357  Loss: -0.017186\n",
      "Ep:13000  Rew:  411.00  Avg Rew:  300.48  LR:0.00004348  Loss: -0.017427\n",
      "Ep:13050  Rew:  222.00  Avg Rew:  281.17  LR:0.00004339  Loss: -0.020868\n",
      "Ep:13100  Rew:  224.00  Avg Rew:  284.12  LR:0.00004329  Loss: -0.001434\n",
      "Ep:13150  Rew:  284.00  Avg Rew:  321.48  LR:0.00004320  Loss: 0.003377\n",
      "Ep:13200  Rew:  436.00  Avg Rew:  339.01  LR:0.00004311  Loss: 0.005356\n",
      "Ep:13250  Rew:  400.00  Avg Rew:  324.94  LR:0.00004301  Loss: -0.005904\n",
      "Ep:13300  Rew:  149.00  Avg Rew:  303.53  LR:0.00004292  Loss: -0.019557\n",
      "Ep:13350  Rew:  380.00  Avg Rew:  292.48  LR:0.00004283  Loss: -0.023709\n",
      "Ep:13400  Rew:  353.00  Avg Rew:  287.24  LR:0.00004274  Loss: -0.012312\n",
      "Ep:13450  Rew:  322.00  Avg Rew:  283.24  LR:0.00004265  Loss: -0.005888\n",
      "Ep:13500  Rew:  366.00  Avg Rew:  283.05  LR:0.00004256  Loss: -0.000511\n",
      "Ep:13550  Rew:  182.00  Avg Rew:  297.68  LR:0.00004246  Loss: -0.005942\n",
      "Ep:13600  Rew:  199.00  Avg Rew:  306.50  LR:0.00004237  Loss: -0.010434\n",
      "Ep:13650  Rew:  252.00  Avg Rew:  279.24  LR:0.00004229  Loss: -0.008640\n",
      "Ep:13700  Rew:  205.00  Avg Rew:  255.12  LR:0.00004220  Loss: -0.022135\n",
      "Ep:13750  Rew:  208.00  Avg Rew:  273.24  LR:0.00004211  Loss: 0.009048\n",
      "Ep:13800  Rew:  251.00  Avg Rew:  293.63  LR:0.00004202  Loss: -0.007109\n",
      "Ep:13850  Rew:  239.00  Avg Rew:  300.88  LR:0.00004193  Loss: 0.009232\n",
      "Ep:13900  Rew:  226.00  Avg Rew:  303.93  LR:0.00004184  Loss: -0.003894\n",
      "Ep:13950  Rew:  296.00  Avg Rew:  304.29  LR:0.00004176  Loss: -0.010655\n",
      "Ep:14000  Rew:  293.00  Avg Rew:  295.08  LR:0.00004167  Loss: -0.008928\n",
      "Ep:14050  Rew:  333.00  Avg Rew:  288.76  LR:0.00004158  Loss: -0.004621\n",
      "Ep:14100  Rew:  416.00  Avg Rew:  282.42  LR:0.00004150  Loss: -0.009658\n",
      "Ep:14150  Rew:  165.00  Avg Rew:  254.51  LR:0.00004141  Loss: -0.024245\n",
      "Ep:14200  Rew:  195.00  Avg Rew:  243.07  LR:0.00004132  Loss: -0.009894\n",
      "Ep:14250  Rew:  255.00  Avg Rew:  240.63  LR:0.00004124  Loss: -0.010163\n",
      "Ep:14300  Rew:  207.00  Avg Rew:  244.53  LR:0.00004115  Loss: -0.010513\n",
      "Ep:14350  Rew:  158.00  Avg Rew:  222.66  LR:0.00004107  Loss: -0.044799\n",
      "Ep:14400  Rew:  232.00  Avg Rew:  197.94  LR:0.00004099  Loss: -0.024156\n",
      "Ep:14450  Rew:  203.00  Avg Rew:  206.43  LR:0.00004090  Loss: -0.017760\n",
      "Ep:14500  Rew:  237.00  Avg Rew:  232.46  LR:0.00004082  Loss: 0.020783\n",
      "Ep:14550  Rew:  336.00  Avg Rew:  265.91  LR:0.00004073  Loss: 0.017205\n",
      "Ep:14600  Rew:  147.00  Avg Rew:  268.88  LR:0.00004065  Loss: 0.005804\n",
      "Ep:14650  Rew:  246.00  Avg Rew:  261.03  LR:0.00004057  Loss: -0.005260\n",
      "Ep:14700  Rew:  171.00  Avg Rew:  240.84  LR:0.00004049  Loss: -0.023411\n",
      "Ep:14750  Rew:  500.00  Avg Rew:  223.43  LR:0.00004041  Loss: -0.007056\n",
      "Ep:14800  Rew:  198.00  Avg Rew:  234.38  LR:0.00004032  Loss: 0.004928\n",
      "Ep:14850  Rew:  384.00  Avg Rew:  253.60  LR:0.00004024  Loss: -0.007150\n",
      "Ep:14900  Rew:  237.00  Avg Rew:  258.54  LR:0.00004016  Loss: -0.008518\n",
      "Ep:14950  Rew:  198.00  Avg Rew:  242.02  LR:0.00004008  Loss: -0.016829\n",
      "Ep:15000  Rew:  134.00  Avg Rew:  223.39  LR:0.00004000  Loss: -0.013035\n",
      "Ep:15050  Rew:  237.00  Avg Rew:  230.73  LR:0.00003992  Loss: -0.001512\n",
      "Ep:15100  Rew:  188.00  Avg Rew:  247.00  LR:0.00003984  Loss: -0.003761\n",
      "Ep:15150  Rew:  218.00  Avg Rew:  263.67  LR:0.00003976  Loss: -0.003154\n",
      "Ep:15200  Rew:  261.00  Avg Rew:  270.76  LR:0.00003968  Loss: -0.000769\n",
      "Ep:15250  Rew:  214.00  Avg Rew:  269.81  LR:0.00003961  Loss: -0.003193\n",
      "Ep:15300  Rew:  500.00  Avg Rew:  300.97  LR:0.00003953  Loss: 0.004150\n",
      "Ep:15350  Rew:  269.00  Avg Rew:  333.38  LR:0.00003945  Loss: -0.007629\n",
      "Ep:15400  Rew:  308.00  Avg Rew:  328.41  LR:0.00003937  Loss: -0.002113\n",
      "Ep:15450  Rew:  291.00  Avg Rew:  323.67  LR:0.00003929  Loss: -0.007918\n",
      "Ep:15500  Rew:  217.00  Avg Rew:  321.70  LR:0.00003922  Loss: -0.006128\n",
      "Ep:15550  Rew:  418.00  Avg Rew:  316.85  LR:0.00003914  Loss: -0.010016\n",
      "Ep:15600  Rew:  439.00  Avg Rew:  326.42  LR:0.00003906  Loss: 0.004504\n",
      "Ep:15650  Rew:  500.00  Avg Rew:  336.58  LR:0.00003899  Loss: 0.003624\n",
      "Ep:15700  Rew:  315.00  Avg Rew:  329.99  LR:0.00003891  Loss: -0.005833\n",
      "Ep:15750  Rew:  225.00  Avg Rew:  323.81  LR:0.00003884  Loss: -0.009547\n",
      "Ep:15800  Rew:  462.00  Avg Rew:  339.48  LR:0.00003876  Loss: 0.001332\n",
      "Ep:15850  Rew:  383.00  Avg Rew:  346.47  LR:0.00003869  Loss: 0.005451\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-571af778b3eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                    \u001b[0mbellman_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbellman_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbaseline_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                    )\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-1904b0433a06>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# each iteration runs one action in the environment and returns a (s,a,r,s') transition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mstep_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp_source\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbaseline_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mbaseline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbaseline_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/atari-gym/lib/python3.6/site-packages/ptan/experience.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mExperienceSourceFirstLast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0mlast_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/atari-gym/lib/python3.6/site-packages/ptan/experience.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m                     \u001b[0mstates_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstates_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0mstates_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_agent_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mg_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstates_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/atari-gym/lib/python3.6/site-packages/ptan/agent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, states, agent_states)\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mprobs_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_softmax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mprobs_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/atari-gym/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/gym/classic control/cartpole/PG/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/atari-gym/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/atari-gym/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/atari-gym/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/atari-gym/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/atari-gym/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1350\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = PG_Trainer(env_name, config, random_seed=random_seed, lr_base=lr_base, lr_decay=lr_decay, \n",
    "                   gamma=gamma, batch_size=batch_size,\n",
    "                   max_episodes=max_episodes, max_timesteps=max_timesteps, \n",
    "                   log_interval=log_interval, entropy_beta=entropy_beta, \n",
    "                   bellman_steps=bellman_steps, baseline_steps=baseline_steps\n",
    "                   )\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "agent.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
