{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "from gym import wrappers\n",
    "from PIL import Image\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "import ptan\n",
    "\n",
    "from PG.pg import PG\n",
    "from PG.utils import mkdir\n",
    "from PG.buffer import MeanBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v1'\n",
    "lr_base = 0.0001\n",
    "lr_decay = 0.0001\n",
    "\n",
    "random_seed = 42\n",
    "gamma = 0.99                # discount for future rewards\n",
    "batch_size = 64         # num of transitions sampled from replay buffer\n",
    "\n",
    "max_episodes = 100000         # max num of episodes\n",
    "max_timesteps = 3000        # max timesteps in one episode\n",
    "log_interval = 50           # print avg reward after interval\n",
    "\n",
    "entropy_beta = 0.01\n",
    "bellman_steps = 50\n",
    "baseline_steps = 50000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = [\n",
    "     {'dim': [None, 128], 'dropout': False, 'activation': 'relu'},    \n",
    "     {'dim': [128, None], 'dropout': False, 'activation': False},    \n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PG_Trainer():\n",
    "    \n",
    "    def __init__(self, env_name, config, random_seed=42, lr_base=0.001, lr_decay=0.00005, \n",
    "                 gamma=0.99, batch_size=32, \n",
    "                 max_episodes=100000, max_timesteps=3000, \n",
    "                 log_interval=5, threshold=None, lr_minimum=1e-10, \n",
    "                 entropy_beta=0.01, bellman_steps=10, baseline_steps=50000, log_dir='./log/'):\n",
    "                \n",
    "        self.algorithm_name = 'pg'\n",
    "        self.env_name = env_name\n",
    "        self.env = gym.make(env_name)\n",
    "        self.log_dir = os.path.join(log_dir, self.algorithm_name)\n",
    "        self.writer = SummaryWriter(log_dir=self.log_dir, comment=\"-cartpole-pg-v1\")\n",
    "               \n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.n    \n",
    "        self.should_record = False\n",
    "        if not threshold == None:\n",
    "            self.threshold = threshold\n",
    "        else:    \n",
    "            self.threshold = self.env.spec.reward_threshold\n",
    "              \n",
    "        self.config = config\n",
    "        self.config[0]['dim'][0] = self.state_dim\n",
    "        self.config[-1]['dim'][1] = self.action_dim      \n",
    "        \n",
    "        \n",
    "        self.random_seed = random_seed\n",
    "        self.lr_base = lr_base\n",
    "        self.lr_decay = lr_decay\n",
    "        self.lr_minimum = lr_minimum        \n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size   \n",
    "        \n",
    "        self.entropy_beta = entropy_beta\n",
    "        self.bellman_steps = bellman_steps\n",
    "        self.baseline_steps = baseline_steps        \n",
    "        \n",
    "        self.max_episodes = max_episodes\n",
    "        self.max_timesteps = max_timesteps\n",
    "        self.log_interval = log_interval\n",
    "       \n",
    "        \n",
    "        prdir = mkdir('.', 'preTrained')\n",
    "        self.directory = mkdir(prdir, self.algorithm_name)\n",
    "        self.filename = \"{}_{}_{}\".format(self.algorithm_name, self.env_name, self.random_seed)\n",
    "                       \n",
    "        self.policy = PG(self.env, self.config, self.gamma, self.bellman_steps)   \n",
    "        \n",
    "        # The experience source interacts with the environment and returns (s,a,r,s') transitions\n",
    "        self.exp_source = ptan.experience.ExperienceSourceFirstLast(self.env, self.policy.ptan_agent,\n",
    "                                                                    gamma=self.gamma,\n",
    "                                                                    steps_count=self.bellman_steps)\n",
    "        \n",
    "        self.baseline_buffer = MeanBuffer(self.baseline_steps)\n",
    "      \n",
    "        \n",
    "        self.reward_history = []\n",
    "        self.make_plots = False       \n",
    "        \n",
    "        if self.random_seed:\n",
    "            print(\"Random Seed: {}\".format(self.random_seed))\n",
    "            self.env.seed(self.random_seed)\n",
    "            torch.manual_seed(self.random_seed)\n",
    "            np.random.seed(self.random_seed)\n",
    "            \n",
    "    def train(self):\n",
    "        \n",
    "        start_time = time.time()        \n",
    "        print(\"action_space={}\".format(self.env.action_space))\n",
    "        print(\"obs_space={}\".format(self.env.observation_space))\n",
    "        print(\"threshold={} \\n\".format(self.threshold))        \n",
    "\n",
    "        # loading models\n",
    "        self.policy.load(self.directory, self.filename)\n",
    "          \n",
    "        \n",
    "        print(\"\\nTraining started ... \")\n",
    "        avg_loss = 0\n",
    "        total_rewards = []\n",
    "        step_rewards = []        \n",
    "        step_idx = 0\n",
    "        episode = 0\n",
    "              \n",
    "        batch_states, batch_actions, batch_scales = [], [], []\n",
    "        learning_rate = self.lr_base\n",
    "        self.policy.set_optimizers(lr=learning_rate)\n",
    "\n",
    "        # each iteration runs one action in the environment and returns a (s,a,r,s') transition\n",
    "        for step_idx, exp in enumerate(self.exp_source):\n",
    "            self.baseline_buffer.add(exp.reward)\n",
    "            baseline = self.baseline_buffer.mean()\n",
    "            self.writer.add_scalar(\"baseline\", baseline, step_idx)\n",
    "            \n",
    "            batch_states.append(exp.state)\n",
    "            batch_actions.append(int(exp.action))\n",
    "            batch_scales.append(exp.reward - baseline)\n",
    "\n",
    "            # handle when an episode is completed\n",
    "            episode_rewards = self.exp_source.pop_total_rewards()\n",
    "            if episode_rewards:\n",
    "                episode += 1\n",
    "                reward = episode_rewards[0]\n",
    "                total_rewards.append(reward)\n",
    "                avg_reward = float(np.mean(total_rewards[-100:]))\n",
    "                \n",
    "                if len(self.policy.loss_list) > 0:               \n",
    "                    avg_loss = np.mean(self.policy.loss_list[-100:])     \n",
    "                \n",
    "                # Print avg reward every log interval:\n",
    "                if episode % self.log_interval == 0:            \n",
    "                    self.policy.save(self.directory, self.filename)\n",
    "                    print(\"Ep:{:5d}  Rew:{:8.2f}  Avg Rew:{:8.2f}  LR:{:8.8f}  Loss: {:8.6f}\".format(\n",
    "                        episode, reward, avg_reward, learning_rate, avg_loss))\n",
    "                \n",
    "                              \n",
    "                learning_rate = max(self.lr_base / (1.0 + episode * self.lr_decay), self.lr_minimum) \n",
    "                \n",
    "                              \n",
    "                self.writer.add_scalar(\"reward\", reward, step_idx)\n",
    "                self.writer.add_scalar(\"reward_100\", avg_reward, step_idx)\n",
    "                self.writer.add_scalar(\"episodes\", episode, step_idx)\n",
    "                \n",
    "                # if avg reward > threshold then save and stop traning:\n",
    "                if avg_reward >= self.threshold and episode > 100: \n",
    "                    print(\"Ep:{:5d}  Rew:{:8.2f}  Avg Rew:{:8.2f}  LR:{:8.8f}  Loss: {:8.6f}\".format(\n",
    "                        episode, reward, avg_reward, learning_rate, avg_loss))\n",
    "                    print(\"########## Solved! ###########\")\n",
    "                    name = self.filename + '_solved'\n",
    "                    self.policy.save(self.directory, name)                  \n",
    "                    self.env.close()  \n",
    "                    training_time = time.time() - start_time\n",
    "                    print(\"Training time: {:6.2f} sec\".format(training_time))\n",
    "                    break    \n",
    "\n",
    "            if len(batch_states) < self.batch_size:\n",
    "                continue\n",
    "            \n",
    "            scalars = self.policy.update(batch_states, batch_actions, batch_scales, self.batch_size, self.entropy_beta)        \n",
    "            entropy_v, entropy_loss_v, loss_policy_v, loss_v, grad_means, grad_count, grad_max = scalars        \n",
    "                           \n",
    "            self.writer.add_scalar(\"baseline\", baseline, step_idx)\n",
    "            self.writer.add_scalar(\"entropy\", entropy_v.item(), step_idx)\n",
    "            self.writer.add_scalar(\"batch_scales\", np.mean(batch_scales), step_idx)\n",
    "            self.writer.add_scalar(\"loss_entropy\", entropy_loss_v.item(), step_idx)\n",
    "            self.writer.add_scalar(\"loss_policy\", loss_policy_v.item(), step_idx)\n",
    "            self.writer.add_scalar(\"loss_total\", loss_v.item(), step_idx)\n",
    "            self.writer.add_scalar(\"grad_l2\", grad_means / grad_count, step_idx)\n",
    "            self.writer.add_scalar(\"grad_max\", grad_max, step_idx)\n",
    "            \n",
    "            batch_states.clear()\n",
    "            batch_actions.clear()\n",
    "            batch_scales.clear()\n",
    "        \n",
    "        self.writer.export_scalars_to_json(os.path.join(self.log_dir, \"all_scalars.json\"))    \n",
    "        self.writer.close()\n",
    "\n",
    "            \n",
    "                                \n",
    "    def test(self, episodes=3, render=True, save_gif=True):              \n",
    "\n",
    "        gifdir = mkdir('.','gif')\n",
    "        algdir = mkdir(gifdir, self.algorithm_name)\n",
    "        \n",
    "        t = 0\n",
    "        for episode in range(1, episodes+1):\n",
    "            ep_reward = 0.0            \n",
    "            epdir = mkdir(algdir, str(episode))\n",
    "        \n",
    "            for step_idx, exp in enumerate(self.exp_source):\n",
    "                self.baseline_buffer.add(exp.reward)\n",
    "                baseline = self.baseline_buffer.mean()               \n",
    "\n",
    "                if save_gif:                                       \n",
    "                    img = self.env.render(mode = 'rgb_array')\n",
    "                    img = Image.fromarray(img)\n",
    "                    img.save('{}/{}.jpg'.format(epdir, t))\n",
    "                t+= 1\n",
    "                    \n",
    "                # handle when an episode is completed\n",
    "                episode_rewards = self.exp_source.pop_total_rewards()\n",
    "                if episode_rewards:\n",
    "                    ep_reward = episode_rewards[0]\n",
    "                    t = 0\n",
    "                    break\n",
    "        \n",
    "            print('Test episode: {}\\tReward: {:4.2f}'.format(episode, ep_reward))           \n",
    "            self.env.close()        \n",
    "                   \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NETWORK: Sequential(\n",
      "  (0): Linear(in_features=4, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=2, bias=True)\n",
      ") Device: cuda:0\n",
      "Random Seed: 42\n",
      "action_space=Discrete(2)\n",
      "obs_space=Box(4,)\n",
      "threshold=475.0 \n",
      "\n",
      "DIR=./preTrained/pg NAME=pg_CartPole-v1_42\n",
      "No models to load\n",
      "\n",
      "Training started ... \n",
      "Ep:   50  Rew:   20.00  Avg Rew:   24.00  LR:0.00009951  Loss: 1.101730\n",
      "Ep:  100  Rew:   30.00  Avg Rew:   23.42  LR:0.00009902  Loss: 0.142415\n",
      "Ep:  150  Rew:   19.00  Avg Rew:   25.02  LR:0.00009853  Loss: 0.639050\n",
      "Ep:  200  Rew:   17.00  Avg Rew:   25.84  LR:0.00009805  Loss: 0.428035\n",
      "Ep:  250  Rew:   40.00  Avg Rew:   25.60  LR:0.00009757  Loss: 0.369544\n",
      "Ep:  300  Rew:   23.00  Avg Rew:   26.81  LR:0.00009710  Loss: 0.277121\n",
      "Ep:  350  Rew:   72.00  Avg Rew:   27.57  LR:0.00009663  Loss: 0.627325\n",
      "Ep:  400  Rew:   18.00  Avg Rew:   28.91  LR:0.00009616  Loss: 0.503391\n",
      "Ep:  450  Rew:   17.00  Avg Rew:   28.53  LR:0.00009570  Loss: 0.629363\n",
      "Ep:  500  Rew:   12.00  Avg Rew:   28.98  LR:0.00009525  Loss: 0.614331\n",
      "Ep:  550  Rew:   29.00  Avg Rew:   31.52  LR:0.00009480  Loss: 0.975131\n",
      "Ep:  600  Rew:   12.00  Avg Rew:   31.74  LR:0.00009435  Loss: 0.880228\n",
      "Ep:  650  Rew:   91.00  Avg Rew:   33.19  LR:0.00009391  Loss: 1.378516\n",
      "Ep:  700  Rew:   52.00  Avg Rew:   36.20  LR:0.00009347  Loss: 1.780601\n",
      "Ep:  750  Rew:   73.00  Avg Rew:   40.24  LR:0.00009303  Loss: 2.233536\n",
      "Ep:  800  Rew:   15.00  Avg Rew:   43.22  LR:0.00009260  Loss: 2.926062\n",
      "Ep:  850  Rew:   47.00  Avg Rew:   47.99  LR:0.00009217  Loss: 3.480417\n",
      "Ep:  900  Rew:   19.00  Avg Rew:   51.89  LR:0.00009175  Loss: 3.610446\n",
      "Ep:  950  Rew:   29.00  Avg Rew:   48.94  LR:0.00009133  Loss: 2.893150\n",
      "Ep: 1000  Rew:  119.00  Avg Rew:   49.82  LR:0.00009092  Loss: 2.873542\n",
      "Ep: 1050  Rew:   88.00  Avg Rew:   55.53  LR:0.00009051  Loss: 3.130646\n",
      "Ep: 1100  Rew:   29.00  Avg Rew:   59.33  LR:0.00009010  Loss: 3.586016\n",
      "Ep: 1150  Rew:   46.00  Avg Rew:   59.15  LR:0.00008969  Loss: 3.455509\n",
      "Ep: 1200  Rew:   58.00  Avg Rew:   64.69  LR:0.00008929  Loss: 3.640223\n",
      "Ep: 1250  Rew:   81.00  Avg Rew:   71.63  LR:0.00008890  Loss: 3.984021\n",
      "Ep: 1300  Rew:   46.00  Avg Rew:   71.06  LR:0.00008850  Loss: 3.681605\n",
      "Ep: 1350  Rew:   83.00  Avg Rew:   72.16  LR:0.00008811  Loss: 3.357676\n",
      "Ep: 1400  Rew:  109.00  Avg Rew:   79.66  LR:0.00008773  Loss: 2.602381\n",
      "Ep: 1450  Rew:  122.00  Avg Rew:   91.11  LR:0.00008734  Loss: 2.762458\n",
      "Ep: 1500  Rew:   79.00  Avg Rew:   90.46  LR:0.00008696  Loss: 2.096530\n",
      "Ep: 1550  Rew:  145.00  Avg Rew:   97.03  LR:0.00008659  Loss: 2.310884\n",
      "Ep: 1600  Rew:  134.00  Avg Rew:  115.52  LR:0.00008621  Loss: 2.207174\n",
      "Ep: 1650  Rew:   67.00  Avg Rew:  121.13  LR:0.00008584  Loss: 1.525732\n",
      "Ep: 1700  Rew:  261.00  Avg Rew:  134.42  LR:0.00008548  Loss: 1.689661\n",
      "Ep: 1750  Rew:  152.00  Avg Rew:  142.13  LR:0.00008511  Loss: 0.912464\n",
      "Ep: 1800  Rew:  146.00  Avg Rew:  152.18  LR:0.00008475  Loss: 1.155474\n",
      "Ep: 1850  Rew:  161.00  Avg Rew:  157.47  LR:0.00008440  Loss: 0.210543\n",
      "Ep: 1900  Rew:  141.00  Avg Rew:  158.55  LR:0.00008404  Loss: 0.249876\n",
      "Ep: 1950  Rew:  146.00  Avg Rew:  173.95  LR:0.00008369  Loss: 0.335021\n",
      "Ep: 2000  Rew:   70.00  Avg Rew:  166.84  LR:0.00008334  Loss: -0.380130\n",
      "Ep: 2050  Rew:  169.00  Avg Rew:  169.77  LR:0.00008299  Loss: 0.179027\n",
      "Ep: 2100  Rew:  228.00  Avg Rew:  197.55  LR:0.00008265  Loss: 0.470525\n",
      "Ep: 2150  Rew:  105.00  Avg Rew:  211.20  LR:0.00008231  Loss: 0.205930\n",
      "Ep: 2200  Rew:  124.00  Avg Rew:  213.92  LR:0.00008197  Loss: -0.007844\n",
      "Ep: 2250  Rew:  158.00  Avg Rew:  213.77  LR:0.00008164  Loss: 0.061668\n",
      "Ep: 2300  Rew:  228.00  Avg Rew:  217.54  LR:0.00008131  Loss: -0.013752\n",
      "Ep: 2350  Rew:  249.00  Avg Rew:  207.78  LR:0.00008098  Loss: -0.146685\n",
      "Ep: 2400  Rew:  246.00  Avg Rew:  202.55  LR:0.00008065  Loss: 0.223810\n",
      "Ep: 2450  Rew:  176.00  Avg Rew:  217.06  LR:0.00008033  Loss: 0.269646\n",
      "Ep: 2500  Rew:  124.00  Avg Rew:  228.95  LR:0.00008001  Loss: 0.166552\n",
      "Ep: 2550  Rew:  280.00  Avg Rew:  241.42  LR:0.00007969  Loss: 0.210401\n",
      "Ep: 2600  Rew:  215.00  Avg Rew:  257.73  LR:0.00007937  Loss: 0.101462\n",
      "Ep: 2650  Rew:  122.00  Avg Rew:  263.69  LR:0.00007906  Loss: 0.017198\n",
      "Ep: 2700  Rew:  195.00  Avg Rew:  247.71  LR:0.00007875  Loss: -0.087292\n",
      "Ep: 2750  Rew:  310.00  Avg Rew:  239.86  LR:0.00007844  Loss: -0.132320\n",
      "Ep: 2800  Rew:  354.00  Avg Rew:  257.05  LR:0.00007813  Loss: 0.079630\n",
      "Ep: 2850  Rew:  283.00  Avg Rew:  283.22  LR:0.00007783  Loss: 0.096317\n",
      "Ep: 2900  Rew:  273.00  Avg Rew:  291.26  LR:0.00007753  Loss: 0.079024\n",
      "Ep: 2950  Rew:  161.00  Avg Rew:  270.19  LR:0.00007723  Loss: -0.267263\n",
      "Ep: 3000  Rew:  476.00  Avg Rew:  255.20  LR:0.00007693  Loss: -0.271427\n",
      "Ep: 3050  Rew:  283.00  Avg Rew:  286.55  LR:0.00007663  Loss: 0.391577\n",
      "Ep: 3100  Rew:  183.00  Avg Rew:  316.59  LR:0.00007634  Loss: 0.096667\n",
      "Ep: 3150  Rew:  435.00  Avg Rew:  300.68  LR:0.00007605  Loss: -0.298540\n",
      "Ep: 3200  Rew:  245.00  Avg Rew:  291.63  LR:0.00007576  Loss: 0.055479\n",
      "Ep: 3250  Rew:  470.00  Avg Rew:  299.39  LR:0.00007548  Loss: -0.074578\n",
      "Ep: 3300  Rew:  131.00  Avg Rew:  281.45  LR:0.00007519  Loss: -0.257614\n",
      "Ep: 3350  Rew:  173.00  Avg Rew:  274.71  LR:0.00007491  Loss: -0.212137\n",
      "Ep: 3400  Rew:  355.00  Avg Rew:  273.84  LR:0.00007463  Loss: -0.109291\n",
      "Ep: 3450  Rew:  106.00  Avg Rew:  266.56  LR:0.00007435  Loss: -0.068786\n",
      "Ep: 3500  Rew:  324.00  Avg Rew:  279.63  LR:0.00007408  Loss: -0.029431\n",
      "Ep: 3550  Rew:  500.00  Avg Rew:  285.86  LR:0.00007381  Loss: 0.019117\n",
      "Ep: 3600  Rew:  500.00  Avg Rew:  311.88  LR:0.00007353  Loss: 0.160758\n",
      "Ep: 3650  Rew:  245.00  Avg Rew:  344.20  LR:0.00007327  Loss: -0.033555\n",
      "Ep: 3700  Rew:  338.00  Avg Rew:  351.29  LR:0.00007300  Loss: -0.015740\n",
      "Ep: 3750  Rew:  164.00  Avg Rew:  341.26  LR:0.00007273  Loss: -0.081934\n",
      "Ep: 3800  Rew:  188.00  Avg Rew:  316.75  LR:0.00007247  Loss: -0.226068\n",
      "Ep: 3850  Rew:  115.00  Avg Rew:  297.30  LR:0.00007221  Loss: -0.174228\n",
      "Ep: 3900  Rew:  223.00  Avg Rew:  311.67  LR:0.00007195  Loss: 0.169705\n",
      "Ep: 3950  Rew:  492.00  Avg Rew:  331.88  LR:0.00007169  Loss: 0.014221\n",
      "Ep: 4000  Rew:  244.00  Avg Rew:  348.07  LR:0.00007143  Loss: 0.166810\n",
      "Ep: 4050  Rew:  500.00  Avg Rew:  369.02  LR:0.00007118  Loss: -0.160349\n",
      "Ep: 4100  Rew:  442.00  Avg Rew:  383.91  LR:0.00007093  Loss: -0.010489\n",
      "Ep: 4150  Rew:  266.00  Avg Rew:  367.59  LR:0.00007068  Loss: -0.153599\n",
      "Ep: 4200  Rew:  259.00  Avg Rew:  310.99  LR:0.00007043  Loss: -0.277956\n",
      "Ep: 4250  Rew:  483.00  Avg Rew:  285.14  LR:0.00007018  Loss: -0.384373\n",
      "Ep: 4300  Rew:  211.00  Avg Rew:  268.15  LR:0.00006993  Loss: -0.451217\n",
      "Ep: 4350  Rew:  169.00  Avg Rew:  245.33  LR:0.00006969  Loss: -0.223028\n",
      "Ep: 4400  Rew:  474.00  Avg Rew:  259.35  LR:0.00006945  Loss: 0.250386\n",
      "Ep: 4450  Rew:  309.00  Avg Rew:  314.80  LR:0.00006921  Loss: 0.408478\n",
      "Ep: 4500  Rew:  500.00  Avg Rew:  339.88  LR:0.00006897  Loss: 0.018648\n",
      "Ep: 4550  Rew:  290.00  Avg Rew:  323.94  LR:0.00006873  Loss: -0.072184\n",
      "Ep: 4600  Rew:  200.00  Avg Rew:  319.53  LR:0.00006850  Loss: -0.048279\n",
      "Ep: 4650  Rew:  356.00  Avg Rew:  320.09  LR:0.00006826  Loss: 0.180356\n",
      "Ep: 4700  Rew:  349.00  Avg Rew:  317.92  LR:0.00006803  Loss: -0.010045\n",
      "Ep: 4750  Rew:  417.00  Avg Rew:  320.98  LR:0.00006780  Loss: 0.136971\n",
      "Ep: 4800  Rew:  355.00  Avg Rew:  366.25  LR:0.00006757  Loss: 0.198775\n",
      "Ep: 4850  Rew:  500.00  Avg Rew:  396.86  LR:0.00006734  Loss: 0.025230\n",
      "Ep: 4900  Rew:  500.00  Avg Rew:  400.74  LR:0.00006712  Loss: 0.052381\n",
      "Ep: 4950  Rew:  500.00  Avg Rew:  429.75  LR:0.00006689  Loss: 0.138885\n",
      "Ep: 5000  Rew:  500.00  Avg Rew:  417.41  LR:0.00006667  Loss: -0.185278\n",
      "Ep: 5050  Rew:  237.00  Avg Rew:  375.80  LR:0.00006645  Loss: -0.082761\n",
      "Ep: 5100  Rew:  317.00  Avg Rew:  367.68  LR:0.00006623  Loss: -0.229751\n",
      "Ep: 5150  Rew:  500.00  Avg Rew:  372.64  LR:0.00006601  Loss: 0.015379\n",
      "Ep: 5200  Rew:  168.00  Avg Rew:  360.99  LR:0.00006579  Loss: -0.078514\n",
      "Ep: 5250  Rew:  382.00  Avg Rew:  379.57  LR:0.00006558  Loss: 0.041042\n",
      "Ep: 5300  Rew:  500.00  Avg Rew:  385.27  LR:0.00006536  Loss: -0.139594\n",
      "Ep: 5350  Rew:  195.00  Avg Rew:  355.95  LR:0.00006515  Loss: -0.145005\n",
      "Ep: 5400  Rew:  317.00  Avg Rew:  335.65  LR:0.00006494  Loss: -0.077716\n",
      "Ep: 5450  Rew:  500.00  Avg Rew:  332.85  LR:0.00006473  Loss: -0.024028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 5500  Rew:  376.00  Avg Rew:  362.02  LR:0.00006452  Loss: 0.204614\n",
      "Ep: 5550  Rew:  262.00  Avg Rew:  370.41  LR:0.00006431  Loss: -0.352280\n",
      "Ep: 5600  Rew:  186.00  Avg Rew:  338.09  LR:0.00006411  Loss: -0.205176\n",
      "Ep: 5650  Rew:  389.00  Avg Rew:  302.98  LR:0.00006390  Loss: -0.549590\n",
      "Ep: 5700  Rew:  486.00  Avg Rew:  284.42  LR:0.00006370  Loss: -0.047080\n",
      "Ep: 5750  Rew:  381.00  Avg Rew:  317.40  LR:0.00006350  Loss: 0.343047\n",
      "Ep: 5800  Rew:  405.00  Avg Rew:  369.94  LR:0.00006330  Loss: 0.352814\n",
      "Ep: 5850  Rew:  152.00  Avg Rew:  389.32  LR:0.00006310  Loss: -0.007429\n",
      "Ep: 5900  Rew:  327.00  Avg Rew:  382.28  LR:0.00006290  Loss: -0.210486\n",
      "Ep: 5950  Rew:  500.00  Avg Rew:  391.06  LR:0.00006270  Loss: 0.042446\n",
      "Ep: 6000  Rew:  500.00  Avg Rew:  402.96  LR:0.00006250  Loss: -0.158148\n",
      "Ep: 6050  Rew:  278.00  Avg Rew:  413.83  LR:0.00006231  Loss: 0.119203\n",
      "Ep: 6100  Rew:  421.00  Avg Rew:  429.00  LR:0.00006212  Loss: -0.093678\n",
      "Ep: 6150  Rew:  294.00  Avg Rew:  385.88  LR:0.00006192  Loss: -0.365661\n",
      "Ep: 6200  Rew:  302.00  Avg Rew:  350.04  LR:0.00006173  Loss: -0.144239\n",
      "Ep: 6250  Rew:  292.00  Avg Rew:  383.55  LR:0.00006154  Loss: 0.167416\n",
      "Ep: 6300  Rew:  500.00  Avg Rew:  416.40  LR:0.00006135  Loss: 0.216343\n",
      "Ep: 6350  Rew:  457.00  Avg Rew:  432.15  LR:0.00006117  Loss: -0.110550\n",
      "Ep: 6400  Rew:  296.00  Avg Rew:  419.08  LR:0.00006098  Loss: -0.305715\n",
      "Ep: 6450  Rew:  259.00  Avg Rew:  366.71  LR:0.00006079  Loss: -0.050409\n",
      "Ep: 6500  Rew:  290.00  Avg Rew:  380.29  LR:0.00006061  Loss: -0.022163\n",
      "Ep: 6550  Rew:  500.00  Avg Rew:  418.07  LR:0.00006043  Loss: 0.203678\n",
      "Ep: 6600  Rew:  500.00  Avg Rew:  414.79  LR:0.00006024  Loss: 0.021452\n",
      "Ep: 6650  Rew:  500.00  Avg Rew:  421.02  LR:0.00006006  Loss: -0.107256\n",
      "Ep: 6700  Rew:  500.00  Avg Rew:  419.81  LR:0.00005988  Loss: 0.030887\n",
      "Ep: 6750  Rew:  280.00  Avg Rew:  402.73  LR:0.00005971  Loss: -0.250663\n",
      "Ep: 6800  Rew:  500.00  Avg Rew:  397.49  LR:0.00005953  Loss: -0.054766\n",
      "Ep: 6850  Rew:  500.00  Avg Rew:  373.48  LR:0.00005935  Loss: -0.089684\n",
      "Ep: 6900  Rew:  500.00  Avg Rew:  324.91  LR:0.00005918  Loss: -0.438178\n",
      "Ep: 6950  Rew:  229.00  Avg Rew:  317.72  LR:0.00005900  Loss: -0.007399\n",
      "Ep: 7000  Rew:  278.00  Avg Rew:  351.56  LR:0.00005883  Loss: 0.057749\n",
      "Ep: 7050  Rew:  169.00  Avg Rew:  351.10  LR:0.00005865  Loss: -0.331350\n",
      "Ep: 7100  Rew:  320.00  Avg Rew:  293.41  LR:0.00005848  Loss: -0.377467\n",
      "Ep: 7150  Rew:  129.00  Avg Rew:  242.82  LR:0.00005831  Loss: -0.421784\n",
      "Ep: 7200  Rew:  151.00  Avg Rew:  242.08  LR:0.00005814  Loss: 0.046533\n",
      "Ep: 7250  Rew:  214.00  Avg Rew:  274.05  LR:0.00005797  Loss: 0.193514\n",
      "Ep: 7300  Rew:  159.00  Avg Rew:  323.48  LR:0.00005781  Loss: 0.121057\n",
      "Ep: 7350  Rew:  500.00  Avg Rew:  341.90  LR:0.00005764  Loss: 0.100004\n",
      "Ep: 7400  Rew:  360.00  Avg Rew:  324.66  LR:0.00005747  Loss: -0.401186\n",
      "Ep: 7450  Rew:  240.00  Avg Rew:  314.44  LR:0.00005731  Loss: 0.029970\n",
      "Ep: 7500  Rew:  213.00  Avg Rew:  320.93  LR:0.00005715  Loss: 0.116450\n",
      "Ep: 7550  Rew:  206.00  Avg Rew:  301.31  LR:0.00005698  Loss: -0.299364\n",
      "Ep: 7600  Rew:  500.00  Avg Rew:  295.30  LR:0.00005682  Loss: 0.271172\n",
      "Ep: 7650  Rew:  380.00  Avg Rew:  336.28  LR:0.00005666  Loss: 0.031070\n",
      "Ep: 7700  Rew:  340.00  Avg Rew:  355.68  LR:0.00005650  Loss: 0.136690\n",
      "Ep: 7750  Rew:  500.00  Avg Rew:  377.41  LR:0.00005634  Loss: 0.072744\n",
      "Ep: 7800  Rew:  407.00  Avg Rew:  395.06  LR:0.00005618  Loss: 0.100465\n",
      "Ep: 7850  Rew:  466.00  Avg Rew:  401.65  LR:0.00005603  Loss: -0.016776\n",
      "Ep: 7900  Rew:  500.00  Avg Rew:  431.23  LR:0.00005587  Loss: 0.066395\n",
      "Ep: 7950  Rew:  500.00  Avg Rew:  467.50  LR:0.00005571  Loss: 0.119810\n",
      "Ep: 7961  Rew:  500.00  Avg Rew:  476.30  LR:0.00005568  Loss: 0.108868\n",
      "########## Solved! ###########\n",
      "Training time: 2619.08 sec\n"
     ]
    }
   ],
   "source": [
    "agent = PG_Trainer(env_name, config, random_seed=random_seed, lr_base=lr_base, lr_decay=lr_decay, \n",
    "                   gamma=gamma, batch_size=batch_size,\n",
    "                   max_episodes=max_episodes, max_timesteps=max_timesteps, \n",
    "                   log_interval=log_interval, entropy_beta=entropy_beta, \n",
    "                   bellman_steps=bellman_steps, baseline_steps=baseline_steps\n",
    "                   )\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test episode: 1\tReward: 500.00\n",
      "Test episode: 2\tReward: 500.00\n",
      "Test episode: 3\tReward: 282.00\n"
     ]
    }
   ],
   "source": [
    "agent.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
