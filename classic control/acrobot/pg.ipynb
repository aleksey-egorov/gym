{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "from gym import wrappers\n",
    "from PIL import Image\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "import ptan\n",
    "\n",
    "from PG.pg import PG\n",
    "from PG.utils import mkdir\n",
    "from PG.buffer import MeanBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'Acrobot-v1'\n",
    "lr_base = 0.0001\n",
    "lr_decay = 0.0001\n",
    "\n",
    "random_seed = 42\n",
    "gamma = 0.99                # discount for future rewards\n",
    "batch_size = 64         # num of transitions sampled from replay buffer\n",
    "\n",
    "max_episodes = 100000         # max num of episodes\n",
    "max_timesteps = 3000        # max timesteps in one episode\n",
    "log_interval = 50           # print avg reward after interval\n",
    "\n",
    "entropy_beta = 0.1\n",
    "bellman_steps = 50\n",
    "baseline_steps = 50000\n",
    "threshold = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = [\n",
    "     {'dim': [None, 128], 'dropout': False, 'activation': 'relu'},    \n",
    "     {'dim': [128, None], 'dropout': False, 'activation': False},    \n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PG_Trainer():\n",
    "    \n",
    "    def __init__(self, env_name, config, random_seed=42, lr_base=0.001, lr_decay=0.00005, \n",
    "                 gamma=0.99, batch_size=32, \n",
    "                 max_episodes=100000, max_timesteps=3000, \n",
    "                 log_interval=5, threshold=None, lr_minimum=1e-10, \n",
    "                 entropy_beta=0.01, bellman_steps=10, baseline_steps=50000, log_dir='./log/'):\n",
    "                \n",
    "        self.algorithm_name = 'pg'\n",
    "        self.env_name = env_name\n",
    "        self.env = gym.make(env_name)\n",
    "        self.log_dir = os.path.join(log_dir, self.algorithm_name)\n",
    "        self.writer = SummaryWriter(log_dir=self.log_dir, comment=self.algorithm_name + \"_\" + self.env_name)\n",
    "               \n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.n    \n",
    "        self.should_record = False\n",
    "        if not threshold == None:\n",
    "            self.threshold = threshold\n",
    "        else:    \n",
    "            self.threshold = self.env.spec.reward_threshold\n",
    "              \n",
    "        self.config = config\n",
    "        self.config[0]['dim'][0] = self.state_dim\n",
    "        self.config[-1]['dim'][1] = self.action_dim      \n",
    "        \n",
    "        \n",
    "        self.random_seed = random_seed\n",
    "        self.lr_base = lr_base\n",
    "        self.lr_decay = lr_decay\n",
    "        self.lr_minimum = lr_minimum        \n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size   \n",
    "        \n",
    "        self.entropy_beta = entropy_beta\n",
    "        self.bellman_steps = bellman_steps\n",
    "        self.baseline_steps = baseline_steps        \n",
    "        \n",
    "        self.max_episodes = max_episodes\n",
    "        self.max_timesteps = max_timesteps\n",
    "        self.log_interval = log_interval\n",
    "       \n",
    "        \n",
    "        prdir = mkdir('.', 'preTrained')\n",
    "        self.directory = mkdir(prdir, self.algorithm_name)\n",
    "        self.filename = \"{}_{}_{}\".format(self.algorithm_name, self.env_name, self.random_seed)\n",
    "                       \n",
    "        self.policy = PG(self.env, self.config, self.gamma, self.bellman_steps)   \n",
    "        \n",
    "        # The experience source interacts with the environment and returns (s,a,r,s') transitions\n",
    "        self.exp_source = ptan.experience.ExperienceSourceFirstLast(self.env, self.policy.ptan_agent,\n",
    "                                                                    gamma=self.gamma,\n",
    "                                                                    steps_count=self.bellman_steps)\n",
    "        \n",
    "        self.baseline_buffer = MeanBuffer(self.baseline_steps)\n",
    "      \n",
    "        \n",
    "        self.reward_history = []\n",
    "        self.make_plots = False       \n",
    "        \n",
    "        if self.random_seed:\n",
    "            print(\"Random Seed: {}\".format(self.random_seed))\n",
    "            self.env.seed(self.random_seed)\n",
    "            torch.manual_seed(self.random_seed)\n",
    "            np.random.seed(self.random_seed)\n",
    "            \n",
    "    def train(self):\n",
    "        \n",
    "        start_time = time.time()        \n",
    "        print(\"action_space={}\".format(self.env.action_space))\n",
    "        print(\"obs_space={}\".format(self.env.observation_space))\n",
    "        print(\"threshold={} \\n\".format(self.threshold))        \n",
    "\n",
    "        # loading models\n",
    "        self.policy.load(self.directory, self.filename)\n",
    "          \n",
    "        \n",
    "        print(\"\\nTraining started ... \")\n",
    "        avg_loss = 0\n",
    "        total_rewards = []\n",
    "        step_rewards = []        \n",
    "        step_idx = 0\n",
    "        episode = 0\n",
    "              \n",
    "        batch_states, batch_actions, batch_scales = [], [], []\n",
    "        learning_rate = self.lr_base\n",
    "        self.policy.set_optimizers(lr=learning_rate)\n",
    "\n",
    "        # each iteration runs one action in the environment and returns a (s,a,r,s') transition\n",
    "        for step_idx, exp in enumerate(self.exp_source):\n",
    "            self.baseline_buffer.add(exp.reward)\n",
    "            baseline = self.baseline_buffer.mean()\n",
    "            self.writer.add_scalar(\"baseline\", baseline, step_idx)\n",
    "            \n",
    "            batch_states.append(exp.state)\n",
    "            batch_actions.append(int(exp.action))\n",
    "            batch_scales.append(exp.reward - baseline)\n",
    "\n",
    "            # handle when an episode is completed\n",
    "            episode_rewards = self.exp_source.pop_total_rewards()\n",
    "            if episode_rewards:\n",
    "                episode += 1\n",
    "                reward = episode_rewards[0]\n",
    "                total_rewards.append(reward)\n",
    "                avg_reward = float(np.mean(total_rewards[-100:]))\n",
    "                \n",
    "                if len(self.policy.loss_list) > 0:               \n",
    "                    avg_loss = np.mean(self.policy.loss_list[-100:])     \n",
    "                \n",
    "                # Print avg reward every log interval:\n",
    "                if episode % self.log_interval == 0:            \n",
    "                    self.policy.save(self.directory, self.filename)\n",
    "                    print(\"Ep:{:5d}  Rew:{:8.2f}  Avg Rew:{:8.2f}  LR:{:8.8f}  Loss: {:8.6f}\".format(\n",
    "                        episode, reward, avg_reward, learning_rate, avg_loss))\n",
    "                \n",
    "                              \n",
    "                learning_rate = max(self.lr_base / (1.0 + episode * self.lr_decay), self.lr_minimum) \n",
    "                \n",
    "                              \n",
    "                self.writer.add_scalar(\"reward\", reward, step_idx)\n",
    "                self.writer.add_scalar(\"reward_100\", avg_reward, step_idx)\n",
    "                self.writer.add_scalar(\"episodes\", episode, step_idx)\n",
    "                \n",
    "                # if avg reward > threshold then save and stop traning:\n",
    "                if avg_reward >= self.threshold and episode > 100: \n",
    "                    print(\"Ep:{:5d}  Rew:{:8.2f}  Avg Rew:{:8.2f}  LR:{:8.8f}  Loss: {:8.6f}\".format(\n",
    "                        episode, reward, avg_reward, learning_rate, avg_loss))\n",
    "                    print(\"########## Solved! ###########\")\n",
    "                    name = self.filename + '_solved'\n",
    "                    self.policy.save(self.directory, name)                  \n",
    "                    self.env.close()  \n",
    "                    training_time = time.time() - start_time\n",
    "                    print(\"Training time: {:6.2f} sec\".format(training_time))\n",
    "                    break    \n",
    "\n",
    "            if len(batch_states) < self.batch_size:\n",
    "                continue\n",
    "            \n",
    "            scalars = self.policy.update(batch_states, batch_actions, batch_scales, self.batch_size, self.entropy_beta)        \n",
    "            entropy_v, entropy_loss_v, loss_policy_v, loss_v, grad_means, grad_count, grad_max = scalars        \n",
    "                           \n",
    "            self.writer.add_scalar(\"baseline\", baseline, step_idx)\n",
    "            self.writer.add_scalar(\"entropy\", entropy_v.item(), step_idx)\n",
    "            self.writer.add_scalar(\"batch_scales\", np.mean(batch_scales), step_idx)\n",
    "            self.writer.add_scalar(\"loss_entropy\", entropy_loss_v.item(), step_idx)\n",
    "            self.writer.add_scalar(\"loss_policy\", loss_policy_v.item(), step_idx)\n",
    "            self.writer.add_scalar(\"loss_total\", loss_v.item(), step_idx)\n",
    "            self.writer.add_scalar(\"grad_l2\", grad_means / grad_count, step_idx)\n",
    "            self.writer.add_scalar(\"grad_max\", grad_max, step_idx)\n",
    "            \n",
    "            batch_states.clear()\n",
    "            batch_actions.clear()\n",
    "            batch_scales.clear()\n",
    "        \n",
    "        self.writer.export_scalars_to_json(os.path.join(self.log_dir, \"all_scalars.json\"))    \n",
    "        self.writer.close()\n",
    "\n",
    "            \n",
    "                                \n",
    "    def test(self, episodes=3, render=True, save_gif=True):              \n",
    "\n",
    "        gifdir = mkdir('.','gif')\n",
    "        algdir = mkdir(gifdir, self.algorithm_name)\n",
    "        \n",
    "        t = 0\n",
    "        for episode in range(1, episodes+1):\n",
    "            ep_reward = 0.0            \n",
    "            epdir = mkdir(algdir, str(episode))\n",
    "        \n",
    "            for step_idx, exp in enumerate(self.exp_source):\n",
    "                self.baseline_buffer.add(exp.reward)\n",
    "                baseline = self.baseline_buffer.mean()               \n",
    "\n",
    "                if save_gif:                                       \n",
    "                    img = self.env.render(mode = 'rgb_array')\n",
    "                    img = Image.fromarray(img)\n",
    "                    img.save('{}/{}.jpg'.format(epdir, t))\n",
    "                t+= 1\n",
    "                    \n",
    "                # handle when an episode is completed\n",
    "                episode_rewards = self.exp_source.pop_total_rewards()\n",
    "                if episode_rewards:\n",
    "                    ep_reward = episode_rewards[0]\n",
    "                    t = 0\n",
    "                    break\n",
    "        \n",
    "            print('Test episode: {}\\tReward: {:4.2f}'.format(episode, ep_reward))           \n",
    "            self.env.close()        \n",
    "                   \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NETWORK: Sequential(\n",
      "  (0): Linear(in_features=6, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=3, bias=True)\n",
      ") Device: cuda:0\n",
      "Random Seed: 42\n",
      "action_space=Discrete(3)\n",
      "obs_space=Box(6,)\n",
      "threshold=-100 \n",
      "\n",
      "DIR=./preTrained/pg NAME=pg_Acrobot-v1_42\n",
      "Models loaded\n",
      "\n",
      "Training started ... \n",
      "Ep:   50  Rew: -500.00  Avg Rew: -500.00  LR:0.00009951  Loss: -0.181850\n",
      "Ep:  100  Rew: -500.00  Avg Rew: -500.00  LR:0.00009902  Loss: -0.197890\n",
      "Ep:  150  Rew: -500.00  Avg Rew: -500.00  LR:0.00009853  Loss: -0.148735\n",
      "Ep:  200  Rew: -500.00  Avg Rew: -500.00  LR:0.00009805  Loss: -0.229418\n",
      "Ep:  250  Rew: -500.00  Avg Rew: -500.00  LR:0.00009757  Loss: -0.090377\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ec5d2651e736>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                    \u001b[0mbellman_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbellman_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbaseline_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                    )\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-856e17831f96>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;31m# each iteration runs one action in the environment and returns a (s,a,r,s') transition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mstep_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp_source\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbaseline_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mbaseline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbaseline_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/atari-gym/lib/python3.6/site-packages/ptan/experience.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mExperienceSourceFirstLast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0mlast_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/atari-gym/lib/python3.6/site-packages/ptan/experience.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m                     \u001b[0mstates_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstates_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0mstates_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_agent_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mg_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstates_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/atari-gym/lib/python3.6/site-packages/ptan/agent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, states, agent_states)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                 \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0mprobs_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_softmax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "agent = PG_Trainer(env_name, config, random_seed=random_seed, lr_base=lr_base, lr_decay=lr_decay, \n",
    "                   gamma=gamma, batch_size=batch_size,\n",
    "                   max_episodes=max_episodes, max_timesteps=max_timesteps, \n",
    "                   log_interval=log_interval, entropy_beta=entropy_beta, \n",
    "                   bellman_steps=bellman_steps, baseline_steps=baseline_steps, threshold=threshold\n",
    "                   )\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "agent.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
