{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from gym import wrappers\n",
    "from PIL import Image\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "\n",
    "from DDQN.ddqn import DDQN\n",
    "from DDQN.utils import ReplayBuffer, mkdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'Acrobot-v1'\n",
    "lr_base = 0.001\n",
    "lr_decay = 0.0001\n",
    "epsilon_base = 0.5 \n",
    "epsilon_decay = 0.002\n",
    "\n",
    "random_seed = 42\n",
    "gamma = 0.99                # discount for future rewards\n",
    "batch_size = 256         # num of transitions sampled from replay buffer\n",
    "polyak = 0.999               # target policy update parameter (1-tau)\n",
    "max_episodes = 100000         # max num of episodes\n",
    "max_timesteps = 3000        # max timesteps in one episode\n",
    "max_buffer_length = 5000000\n",
    "log_interval = 10           # print avg reward after interval\n",
    "threshold = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_config = [\n",
    "        {'dim': [None, 64], 'dropout': False, 'activation': 'relu'},      \n",
    "        {'dim': [64, None], 'dropout': False, 'activation': False}\n",
    "    ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNTrainer():\n",
    "    \n",
    "    def __init__(self, env_name, fc_config, random_seed=42, lr_base=0.001, lr_decay=0.00005, \n",
    "                 epsilon_base=0.3, epsilon_decay=0.0001, gamma=0.99, batch_size=1024, \n",
    "                 max_episodes=100000, max_timesteps=3000, max_buffer_length=5000000, \n",
    "                 log_interval=5, threshold=None, lr_minimum=1e-10, epsilon_minimum=1e-10,\n",
    "                 record_videos=True, record_interval=100):        \n",
    "        \n",
    "        self.algorithm_name = 'ddqn'\n",
    "        self.env_name = env_name\n",
    "        self.env = gym.make(env_name)\n",
    "        self.record_videos = record_videos\n",
    "        self.record_interval = record_interval        \n",
    "        if self.record_videos == True:\n",
    "            videos_dir = mkdir('.', 'videos')\n",
    "            monitor_dir = mkdir(videos_dir, self.algorithm_name)\n",
    "            should_record = lambda i: self.should_record\n",
    "            self.env = wrappers.Monitor(self.env, monitor_dir, video_callable=should_record, force=True)            \n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.n    \n",
    "        self.should_record = False\n",
    "        if not threshold == None:\n",
    "            self.threshold = threshold\n",
    "        else:    \n",
    "            self.threshold = self.env.spec.reward_threshold\n",
    "              \n",
    "        self.fc_config = fc_config\n",
    "        self.fc_config[0]['dim'][0] = self.state_dim\n",
    "        self.fc_config[-1]['dim'][1] = self.action_dim      \n",
    "        \n",
    "        self.random_seed = random_seed\n",
    "        self.lr_base = lr_base\n",
    "        self.lr_decay = lr_decay\n",
    "        self.lr_minimum = lr_minimum  \n",
    "        self.epsilon_base = epsilon_base\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_minimum = epsilon_minimum\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size              \n",
    "        self.max_episodes = max_episodes\n",
    "        self.max_timesteps = max_timesteps\n",
    "        self.max_buffer_length = max_buffer_length\n",
    "        self.log_interval = log_interval\n",
    "        \n",
    "        prdir = mkdir('.', 'preTrained')\n",
    "        self.directory = mkdir(prdir, self.algorithm_name)\n",
    "        self.filename = \"{}_{}_{}\".format(self.algorithm_name, self.env_name, self.random_seed)\n",
    "        \n",
    "        self.policy = DDQN(self.env, fc_config)   \n",
    "        self.replay_buffer = ReplayBuffer(size=self.max_buffer_length)\n",
    "        \n",
    "        self.reward_history = []\n",
    "        self.make_plots = False       \n",
    "        \n",
    "        if self.random_seed:\n",
    "            print(\"Random Seed: {}\".format(self.random_seed))\n",
    "            self.env.seed(self.random_seed)\n",
    "            torch.manual_seed(self.random_seed)\n",
    "            np.random.seed(self.random_seed)\n",
    "            \n",
    "    def train(self):\n",
    "        \n",
    "        start_time = time.time()        \n",
    "        print(\"action_space={}\".format(self.env.action_space))\n",
    "        print(\"obs_space={}\".format(self.env.observation_space))\n",
    "        print(\"threshold={} \\n\".format(self.threshold))        \n",
    "\n",
    "        # loading models\n",
    "        self.policy.load(self.directory, self.filename)\n",
    "                \n",
    "        # logging variables:        \n",
    "        log_f = open(\"train_{}.txt\".format(self.algorithm_name), \"w+\")\n",
    "        \n",
    "        print(\"\\nTraining started ... \")\n",
    "                \n",
    "        # training procedure:        \n",
    "        for episode in range(self.max_episodes):\n",
    "            \n",
    "            # Only record video during evaluation, every n steps\n",
    "            if episode % self.record_interval == 0:\n",
    "                self.should_record = True\n",
    "            \n",
    "            ep_reward = 0.0        \n",
    "            state = self.env.reset()\n",
    "                       \n",
    "            # calculate params\n",
    "            epsilon = max(self.epsilon_base / (1.0 + episode * self.epsilon_decay), self.epsilon_minimum)\n",
    "            learning_rate = max(self.lr_base / (1.0 + episode * self.lr_decay), self.lr_minimum)      \n",
    "            self.policy.set_optimizers(lr=learning_rate)\n",
    "           \n",
    "            for t in range(self.max_timesteps):\n",
    "                \n",
    "                action = self.policy.select_action(state, epsilon)                \n",
    "                next_state, reward, done, _ = self.env.step(action)          \n",
    "                self.replay_buffer.add(state, action, reward, next_state, float(done))\n",
    "                \n",
    "                # Updating policy\n",
    "                self.policy.update(self.replay_buffer, t, self.batch_size, self.gamma)\n",
    "                \n",
    "                state = next_state               \n",
    "                ep_reward += reward            \n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "           \n",
    "            self.reward_history.append(ep_reward)\n",
    "            avg_reward = np.mean(self.reward_history[-100:]) \n",
    "                       \n",
    "            # logging updates:        \n",
    "            log_f.write('{},{}\\n'.format(episode, ep_reward))\n",
    "            log_f.flush()\n",
    "           \n",
    "            if len(self.policy.Q_loss_list) > 0:               \n",
    "                avg_Q_loss = np.mean(self.policy.Q_loss_list[-100:])     \n",
    "            \n",
    "            # Truncate training history if we don't plan to plot it later\n",
    "            if not self.make_plots:\n",
    "                self.policy.truncate_loss_lists() \n",
    "                if len(self.reward_history) > 100:\n",
    "                    self.reward_history.pop(0)    \n",
    "         \n",
    "            # Print avg reward every log interval:\n",
    "            if episode % self.log_interval == 0:            \n",
    "                self.policy.save(self.directory, self.filename)\n",
    "                print(\"Ep:{:5d}  Rew:{:8.2f}  Avg Rew:{:8.2f}  LR:{:8.8f}  Bf:{:2.0f}  EPS:{:0.4f}  Loss: {:8.6f}\".format(\n",
    "                    episode, ep_reward, avg_reward, learning_rate, self.replay_buffer.get_fill(), \n",
    "                    epsilon, avg_Q_loss))\n",
    "                        \n",
    "            self.should_record = False\n",
    "            \n",
    "            # if avg reward > threshold then save and stop traning:\n",
    "            if avg_reward >= self.threshold and episode > 100: \n",
    "                print(\"Ep:{:5d}  Rew:{:8.2f}  Avg Rew:{:8.2f}  LR:{:8.8f}  Bf:{:2.0f}  EPS:{:0.4f}  Loss: {:8.6f}\".format(\n",
    "                    episode, ep_reward, avg_reward, learning_rate, self.replay_buffer.get_fill(), \n",
    "                    epsilon, avg_Q_loss))\n",
    "                print(\"########## Solved! ###########\")\n",
    "                name = self.filename + '_solved'\n",
    "                self.policy.save(self.directory, name)\n",
    "                log_f.close()\n",
    "                self.env.close()  \n",
    "                training_time = time.time() - start_time\n",
    "                print(\"Training time: {:6.2f} sec\".format(training_time))\n",
    "                break    \n",
    "                                \n",
    "    def test(self, episodes=3, render=True, save_gif=True):              \n",
    "\n",
    "        gifdir = mkdir('.','gif')\n",
    "        algdir = mkdir(gifdir, self.algorithm_name)\n",
    "        \n",
    "        for episode in range(1, episodes+1):\n",
    "            ep_reward = 0.0\n",
    "            state = self.env.reset()    \n",
    "            epdir = mkdir(algdir, str(episode))\n",
    "                       \n",
    "            for t in range(self.max_timesteps):\n",
    "                action = self.policy.select_action(state, 0)\n",
    "                next_state, reward, done, _ = self.env.step(action)               \n",
    "                state = next_state               \n",
    "                ep_reward += reward                                  \n",
    "                \n",
    "                if save_gif:                                       \n",
    "                    img = self.env.render(mode = 'rgb_array')\n",
    "                    img = Image.fromarray(img)\n",
    "                    img.save('{}/{}.jpg'.format(epdir, t))\n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "            print('Test episode: {}\\tReward: {:4.2f}'.format(episode, ep_reward))           \n",
    "            self.env.close()        \n",
    "                   \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NETWORK=Sequential(\n",
      "  (0): Linear(in_features=6, out_features=64, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=64, out_features=3, bias=True)\n",
      ")\n",
      "Device: cuda:0\n",
      "NETWORK=Sequential(\n",
      "  (0): Linear(in_features=6, out_features=64, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=64, out_features=3, bias=True)\n",
      ")\n",
      "Device: cuda:0\n",
      "Random Seed: 42\n",
      "action_space=Discrete(3)\n",
      "obs_space=Box(6,)\n",
      "threshold=-100 \n",
      "\n",
      "DIR=./preTrained/ddqn NAME=ddqn_Acrobot-v1_42\n",
      "No models to load\n",
      "\n",
      "Training started ... \n",
      "Ep:    0  Rew: -500.00  Avg Rew: -500.00  LR:0.00100000  Bf: 0  EPS:0.5000  Loss: 0.000717\n",
      "Ep:   10  Rew: -500.00  Avg Rew: -500.00  LR:0.00099900  Bf: 0  EPS:0.4902  Loss: 0.155967\n",
      "Ep:   20  Rew: -500.00  Avg Rew: -500.00  LR:0.00099800  Bf: 0  EPS:0.4808  Loss: 0.617753\n",
      "Ep:   30  Rew: -500.00  Avg Rew: -500.00  LR:0.00099701  Bf: 0  EPS:0.4717  Loss: 1.213426\n",
      "Ep:   40  Rew: -500.00  Avg Rew: -493.68  LR:0.00099602  Bf: 0  EPS:0.4630  Loss: 1.658386\n",
      "Ep:   50  Rew: -350.00  Avg Rew: -459.10  LR:0.00099502  Bf: 0  EPS:0.4545  Loss: 2.144288\n",
      "Ep:   60  Rew: -152.00  Avg Rew: -418.03  LR:0.00099404  Bf: 1  EPS:0.4464  Loss: 2.993678\n",
      "Ep:   70  Rew: -199.00  Avg Rew: -394.45  LR:0.00099305  Bf: 1  EPS:0.4386  Loss: 3.355879\n",
      "Ep:   80  Rew: -253.00  Avg Rew: -377.00  LR:0.00099206  Bf: 1  EPS:0.4310  Loss: 3.164193\n",
      "Ep:   90  Rew: -133.00  Avg Rew: -359.64  LR:0.00099108  Bf: 1  EPS:0.4237  Loss: 4.705334\n",
      "Ep:  100  Rew: -280.00  Avg Rew: -340.99  LR:0.00099010  Bf: 1  EPS:0.4167  Loss: 4.957843\n",
      "Ep:  110  Rew: -247.00  Avg Rew: -309.62  LR:0.00098912  Bf: 1  EPS:0.4098  Loss: 5.018411\n",
      "Ep:  120  Rew: -139.00  Avg Rew: -275.86  LR:0.00098814  Bf: 1  EPS:0.4032  Loss: 5.201505\n",
      "Ep:  130  Rew: -215.00  Avg Rew: -245.60  LR:0.00098717  Bf: 1  EPS:0.3968  Loss: 5.442048\n",
      "Ep:  140  Rew: -154.00  Avg Rew: -213.07  LR:0.00098619  Bf: 1  EPS:0.3906  Loss: 4.300932\n",
      "Ep:  150  Rew: -129.00  Avg Rew: -195.59  LR:0.00098522  Bf: 1  EPS:0.3846  Loss: 4.633923\n",
      "Ep:  160  Rew: -141.00  Avg Rew: -189.47  LR:0.00098425  Bf: 1  EPS:0.3788  Loss: 6.182173\n",
      "Ep:  170  Rew: -112.00  Avg Rew: -177.39  LR:0.00098328  Bf: 1  EPS:0.3731  Loss: 6.305358\n",
      "Ep:  180  Rew: -130.00  Avg Rew: -167.32  LR:0.00098232  Bf: 1  EPS:0.3676  Loss: 4.810317\n",
      "Ep:  190  Rew: -500.00  Avg Rew: -165.75  LR:0.00098135  Bf: 1  EPS:0.3623  Loss: 5.502369\n",
      "Ep:  200  Rew: -195.00  Avg Rew: -161.00  LR:0.00098039  Bf: 1  EPS:0.3571  Loss: 5.317575\n",
      "Ep:  210  Rew: -100.00  Avg Rew: -154.52  LR:0.00097943  Bf: 1  EPS:0.3521  Loss: 4.906939\n",
      "Ep:  220  Rew: -145.00  Avg Rew: -151.12  LR:0.00097847  Bf: 1  EPS:0.3472  Loss: 5.250979\n",
      "Ep:  230  Rew: -117.00  Avg Rew: -143.11  LR:0.00097752  Bf: 1  EPS:0.3425  Loss: 5.167378\n",
      "Ep:  240  Rew: -169.00  Avg Rew: -141.51  LR:0.00097656  Bf: 1  EPS:0.3378  Loss: 4.341378\n",
      "Ep:  250  Rew: -118.00  Avg Rew: -139.33  LR:0.00097561  Bf: 1  EPS:0.3333  Loss: 5.254718\n",
      "Ep:  260  Rew: -100.00  Avg Rew: -137.47  LR:0.00097466  Bf: 1  EPS:0.3289  Loss: 5.110253\n",
      "Ep:  270  Rew: -145.00  Avg Rew: -135.84  LR:0.00097371  Bf: 1  EPS:0.3247  Loss: 4.724022\n",
      "Ep:  280  Rew: -115.00  Avg Rew: -132.25  LR:0.00097276  Bf: 1  EPS:0.3205  Loss: 4.362476\n",
      "Ep:  290  Rew: -130.00  Avg Rew: -124.32  LR:0.00097182  Bf: 1  EPS:0.3165  Loss: 4.229063\n",
      "Ep:  300  Rew: -108.00  Avg Rew: -124.03  LR:0.00097087  Bf: 1  EPS:0.3125  Loss: 3.807053\n",
      "Ep:  310  Rew: -160.00  Avg Rew: -123.17  LR:0.00096993  Bf: 1  EPS:0.3086  Loss: 3.741350\n",
      "Ep:  320  Rew: -104.00  Avg Rew: -121.56  LR:0.00096899  Bf: 1  EPS:0.3049  Loss: 3.782080\n",
      "Ep:  330  Rew: -139.00  Avg Rew: -121.50  LR:0.00096805  Bf: 1  EPS:0.3012  Loss: 3.425315\n",
      "Ep:  340  Rew: -139.00  Avg Rew: -119.56  LR:0.00096712  Bf: 1  EPS:0.2976  Loss: 3.606174\n",
      "Ep:  350  Rew: -133.00  Avg Rew: -118.94  LR:0.00096618  Bf: 1  EPS:0.2941  Loss: 4.429015\n",
      "Ep:  360  Rew: -132.00  Avg Rew: -117.19  LR:0.00096525  Bf: 1  EPS:0.2907  Loss: 3.848057\n",
      "Ep:  370  Rew: -141.00  Avg Rew: -117.99  LR:0.00096432  Bf: 1  EPS:0.2874  Loss: 4.183332\n",
      "Ep:  380  Rew:  -92.00  Avg Rew: -117.92  LR:0.00096339  Bf: 1  EPS:0.2841  Loss: 3.503169\n",
      "Ep:  390  Rew: -111.00  Avg Rew: -118.59  LR:0.00096246  Bf: 1  EPS:0.2809  Loss: 3.327655\n",
      "Ep:  400  Rew:  -98.00  Avg Rew: -115.63  LR:0.00096154  Bf: 2  EPS:0.2778  Loss: 3.376156\n",
      "Ep:  410  Rew: -114.00  Avg Rew: -116.98  LR:0.00096061  Bf: 2  EPS:0.2747  Loss: 3.624813\n",
      "Ep:  420  Rew: -109.00  Avg Rew: -117.10  LR:0.00095969  Bf: 2  EPS:0.2717  Loss: 3.828939\n",
      "Ep:  430  Rew: -106.00  Avg Rew: -116.42  LR:0.00095877  Bf: 2  EPS:0.2688  Loss: 3.977843\n",
      "Ep:  440  Rew:  -94.00  Avg Rew: -115.53  LR:0.00095785  Bf: 2  EPS:0.2660  Loss: 3.136802\n",
      "Ep:  450  Rew: -213.00  Avg Rew: -115.45  LR:0.00095694  Bf: 2  EPS:0.2632  Loss: 3.619666\n",
      "Ep:  460  Rew:  -88.00  Avg Rew: -115.34  LR:0.00095602  Bf: 2  EPS:0.2604  Loss: 3.797966\n",
      "Ep:  470  Rew:  -90.00  Avg Rew: -114.08  LR:0.00095511  Bf: 2  EPS:0.2577  Loss: 3.649581\n",
      "Ep:  480  Rew: -110.00  Avg Rew: -114.21  LR:0.00095420  Bf: 2  EPS:0.2551  Loss: 3.524670\n",
      "Ep:  490  Rew: -106.00  Avg Rew: -111.76  LR:0.00095329  Bf: 2  EPS:0.2525  Loss: 3.216221\n",
      "Ep:  500  Rew: -163.00  Avg Rew: -112.82  LR:0.00095238  Bf: 2  EPS:0.2500  Loss: 3.623344\n",
      "Ep:  510  Rew: -123.00  Avg Rew: -114.34  LR:0.00095147  Bf: 2  EPS:0.2475  Loss: 3.313494\n",
      "Ep:  520  Rew: -166.00  Avg Rew: -114.26  LR:0.00095057  Bf: 2  EPS:0.2451  Loss: 3.175563\n",
      "Ep:  530  Rew:  -90.00  Avg Rew: -113.17  LR:0.00094967  Bf: 2  EPS:0.2427  Loss: 3.601857\n",
      "Ep:  540  Rew:  -99.00  Avg Rew: -112.94  LR:0.00094877  Bf: 2  EPS:0.2404  Loss: 3.209368\n",
      "Ep:  550  Rew:  -71.00  Avg Rew: -112.00  LR:0.00094787  Bf: 2  EPS:0.2381  Loss: 3.139256\n",
      "Ep:  560  Rew: -120.00  Avg Rew: -112.56  LR:0.00094697  Bf: 2  EPS:0.2358  Loss: 3.656891\n",
      "Ep:  570  Rew: -114.00  Avg Rew: -116.05  LR:0.00094607  Bf: 2  EPS:0.2336  Loss: 3.605738\n",
      "Ep:  580  Rew: -102.00  Avg Rew: -116.05  LR:0.00094518  Bf: 2  EPS:0.2315  Loss: 3.553948\n",
      "Ep:  590  Rew: -373.00  Avg Rew: -118.57  LR:0.00094429  Bf: 2  EPS:0.2294  Loss: 3.396813\n",
      "Ep:  600  Rew:  -88.00  Avg Rew: -117.14  LR:0.00094340  Bf: 2  EPS:0.2273  Loss: 3.300877\n",
      "Ep:  610  Rew:  -88.00  Avg Rew: -113.06  LR:0.00094251  Bf: 2  EPS:0.2252  Loss: 3.254122\n",
      "Ep:  620  Rew:  -92.00  Avg Rew: -112.78  LR:0.00094162  Bf: 2  EPS:0.2232  Loss: 3.540573\n",
      "Ep:  630  Rew: -103.00  Avg Rew: -113.90  LR:0.00094073  Bf: 2  EPS:0.2212  Loss: 3.569324\n",
      "Ep:  640  Rew:  -75.00  Avg Rew: -117.90  LR:0.00093985  Bf: 2  EPS:0.2193  Loss: 3.305757\n",
      "Ep:  650  Rew:  -85.00  Avg Rew: -121.51  LR:0.00093897  Bf: 2  EPS:0.2174  Loss: 3.494665\n",
      "Ep:  660  Rew:  -82.00  Avg Rew: -120.30  LR:0.00093809  Bf: 2  EPS:0.2155  Loss: 3.634523\n",
      "Ep:  670  Rew:  -88.00  Avg Rew: -116.44  LR:0.00093721  Bf: 2  EPS:0.2137  Loss: 3.911556\n",
      "Ep:  680  Rew:  -89.00  Avg Rew: -115.22  LR:0.00093633  Bf: 2  EPS:0.2119  Loss: 3.484283\n",
      "Ep:  690  Rew:  -94.00  Avg Rew: -112.39  LR:0.00093545  Bf: 2  EPS:0.2101  Loss: 4.027385\n",
      "Ep:  700  Rew:  -94.00  Avg Rew: -111.61  LR:0.00093458  Bf: 2  EPS:0.2083  Loss: 3.936874\n",
      "Ep:  710  Rew: -101.00  Avg Rew: -111.64  LR:0.00093371  Bf: 2  EPS:0.2066  Loss: 3.189370\n",
      "Ep:  720  Rew:  -99.00  Avg Rew: -114.54  LR:0.00093284  Bf: 2  EPS:0.2049  Loss: 3.699401\n",
      "Ep:  730  Rew: -107.00  Avg Rew: -114.19  LR:0.00093197  Bf: 2  EPS:0.2033  Loss: 3.727162\n",
      "Ep:  740  Rew:  -92.00  Avg Rew: -113.59  LR:0.00093110  Bf: 2  EPS:0.2016  Loss: 3.901965\n",
      "Ep:  750  Rew: -112.00  Avg Rew: -110.76  LR:0.00093023  Bf: 2  EPS:0.2000  Loss: 3.741386\n",
      "Ep:  760  Rew: -113.00  Avg Rew: -110.89  LR:0.00092937  Bf: 2  EPS:0.1984  Loss: 3.521213\n",
      "Ep:  770  Rew:  -99.00  Avg Rew: -114.55  LR:0.00092851  Bf: 2  EPS:0.1969  Loss: 3.202140\n",
      "Ep:  780  Rew: -107.00  Avg Rew: -114.47  LR:0.00092764  Bf: 2  EPS:0.1953  Loss: 3.348829\n",
      "Ep:  790  Rew: -108.00  Avg Rew: -114.34  LR:0.00092678  Bf: 2  EPS:0.1938  Loss: 3.741327\n",
      "Ep:  800  Rew: -109.00  Avg Rew: -115.56  LR:0.00092593  Bf: 2  EPS:0.1923  Loss: 3.783783\n",
      "Ep:  810  Rew: -116.00  Avg Rew: -116.40  LR:0.00092507  Bf: 2  EPS:0.1908  Loss: 4.088473\n",
      "Ep:  820  Rew:  -84.00  Avg Rew: -112.50  LR:0.00092421  Bf: 2  EPS:0.1894  Loss: 3.671378\n",
      "Ep:  830  Rew: -114.00  Avg Rew: -112.94  LR:0.00092336  Bf: 2  EPS:0.1880  Loss: 3.839250\n",
      "Ep:  840  Rew: -143.00  Avg Rew: -109.99  LR:0.00092251  Bf: 3  EPS:0.1866  Loss: 4.062239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep:  850  Rew:  -78.00  Avg Rew: -109.29  LR:0.00092166  Bf: 3  EPS:0.1852  Loss: 3.848523\n",
      "Ep:  860  Rew: -112.00  Avg Rew: -109.17  LR:0.00092081  Bf: 3  EPS:0.1838  Loss: 3.511145\n",
      "Ep:  870  Rew: -107.00  Avg Rew: -104.80  LR:0.00091996  Bf: 3  EPS:0.1825  Loss: 3.659924\n",
      "Ep:  880  Rew: -114.00  Avg Rew: -104.93  LR:0.00091912  Bf: 3  EPS:0.1812  Loss: 4.207664\n",
      "Ep:  890  Rew:  -92.00  Avg Rew: -104.29  LR:0.00091827  Bf: 3  EPS:0.1799  Loss: 3.667697\n",
      "Ep:  900  Rew:  -82.00  Avg Rew: -103.59  LR:0.00091743  Bf: 3  EPS:0.1786  Loss: 3.601747\n",
      "Ep:  910  Rew: -125.00  Avg Rew: -103.36  LR:0.00091659  Bf: 3  EPS:0.1773  Loss: 3.853183\n",
      "Ep:  920  Rew: -142.00  Avg Rew: -103.44  LR:0.00091575  Bf: 3  EPS:0.1761  Loss: 3.808207\n",
      "Ep:  930  Rew:  -97.00  Avg Rew: -102.07  LR:0.00091491  Bf: 3  EPS:0.1748  Loss: 3.961357\n",
      "Ep:  940  Rew:  -85.00  Avg Rew: -102.49  LR:0.00091408  Bf: 3  EPS:0.1736  Loss: 3.851306\n",
      "Ep:  950  Rew: -123.00  Avg Rew: -105.53  LR:0.00091324  Bf: 3  EPS:0.1724  Loss: 3.944806\n",
      "Ep:  960  Rew:  -88.00  Avg Rew: -109.98  LR:0.00091241  Bf: 3  EPS:0.1712  Loss: 3.990400\n",
      "Ep:  970  Rew:  -89.00  Avg Rew: -109.94  LR:0.00091158  Bf: 3  EPS:0.1701  Loss: 3.413792\n",
      "Ep:  980  Rew: -107.00  Avg Rew: -111.70  LR:0.00091075  Bf: 3  EPS:0.1689  Loss: 3.472988\n",
      "Ep:  990  Rew: -100.00  Avg Rew: -112.79  LR:0.00090992  Bf: 3  EPS:0.1678  Loss: 4.006674\n",
      "Ep: 1000  Rew:  -80.00  Avg Rew: -113.77  LR:0.00090909  Bf: 3  EPS:0.1667  Loss: 3.665079\n",
      "Ep: 1010  Rew: -148.00  Avg Rew: -114.10  LR:0.00090827  Bf: 3  EPS:0.1656  Loss: 3.933697\n",
      "Ep: 1020  Rew: -116.00  Avg Rew: -113.60  LR:0.00090744  Bf: 3  EPS:0.1645  Loss: 3.554474\n",
      "Ep: 1030  Rew: -122.00  Avg Rew: -114.45  LR:0.00090662  Bf: 3  EPS:0.1634  Loss: 3.577567\n",
      "Ep: 1040  Rew: -103.00  Avg Rew: -113.44  LR:0.00090580  Bf: 3  EPS:0.1623  Loss: 3.698360\n",
      "Ep: 1050  Rew:  -71.00  Avg Rew: -110.55  LR:0.00090498  Bf: 3  EPS:0.1613  Loss: 3.848420\n",
      "Ep: 1060  Rew:  -98.00  Avg Rew: -105.34  LR:0.00090416  Bf: 3  EPS:0.1603  Loss: 3.786508\n",
      "Ep: 1070  Rew: -121.00  Avg Rew: -105.60  LR:0.00090334  Bf: 3  EPS:0.1592  Loss: 3.958840\n",
      "Ep: 1080  Rew:  -84.00  Avg Rew: -103.90  LR:0.00090253  Bf: 3  EPS:0.1582  Loss: 3.427588\n",
      "Ep: 1090  Rew: -113.00  Avg Rew: -105.91  LR:0.00090171  Bf: 3  EPS:0.1572  Loss: 3.761351\n",
      "Ep: 1100  Rew:  -84.00  Avg Rew: -104.13  LR:0.00090090  Bf: 3  EPS:0.1562  Loss: 3.804290\n",
      "Ep: 1110  Rew:  -89.00  Avg Rew: -103.08  LR:0.00090009  Bf: 3  EPS:0.1553  Loss: 3.799766\n",
      "Ep: 1120  Rew: -130.00  Avg Rew: -104.30  LR:0.00089928  Bf: 3  EPS:0.1543  Loss: 3.429965\n",
      "Ep: 1130  Rew:  -95.00  Avg Rew: -105.80  LR:0.00089847  Bf: 3  EPS:0.1534  Loss: 3.560093\n",
      "Ep: 1140  Rew: -159.00  Avg Rew: -106.81  LR:0.00089767  Bf: 3  EPS:0.1524  Loss: 3.732553\n",
      "Ep: 1150  Rew:  -77.00  Avg Rew: -106.96  LR:0.00089686  Bf: 3  EPS:0.1515  Loss: 3.981576\n",
      "Ep: 1160  Rew:  -75.00  Avg Rew: -107.44  LR:0.00089606  Bf: 3  EPS:0.1506  Loss: 3.522133\n",
      "Ep: 1170  Rew:  -83.00  Avg Rew: -106.90  LR:0.00089526  Bf: 3  EPS:0.1497  Loss: 3.414164\n",
      "Ep: 1180  Rew: -118.00  Avg Rew: -106.55  LR:0.00089445  Bf: 3  EPS:0.1488  Loss: 3.090017\n",
      "Ep: 1190  Rew: -115.00  Avg Rew: -103.46  LR:0.00089366  Bf: 3  EPS:0.1479  Loss: 3.101242\n",
      "Ep: 1200  Rew:  -98.00  Avg Rew: -103.85  LR:0.00089286  Bf: 3  EPS:0.1471  Loss: 3.600269\n",
      "Ep: 1210  Rew: -135.00  Avg Rew: -106.39  LR:0.00089206  Bf: 3  EPS:0.1462  Loss: 3.206808\n",
      "Ep: 1220  Rew:  -82.00  Avg Rew: -104.74  LR:0.00089127  Bf: 3  EPS:0.1453  Loss: 3.733250\n",
      "Ep: 1230  Rew:  -95.00  Avg Rew: -102.37  LR:0.00089047  Bf: 3  EPS:0.1445  Loss: 3.418631\n",
      "Ep: 1240  Rew:  -96.00  Avg Rew: -100.73  LR:0.00088968  Bf: 3  EPS:0.1437  Loss: 3.380725\n",
      "Ep: 1246  Rew:  -95.00  Avg Rew:  -99.63  LR:0.00088921  Bf: 3  EPS:0.1432  Loss: 3.437555\n",
      "########## Solved! ###########\n",
      "Training time: 602.09 sec\n"
     ]
    }
   ],
   "source": [
    "agent = DDQNTrainer(env_name, fc_config, random_seed=random_seed, lr_base=lr_base, lr_decay=lr_decay, \n",
    "                   epsilon_base=epsilon_base, epsilon_decay=epsilon_decay, gamma=gamma, batch_size=batch_size,\n",
    "                   max_episodes=max_episodes, max_timesteps=max_timesteps, \n",
    "                   max_buffer_length=max_buffer_length, log_interval=log_interval, threshold=threshold)\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test episode: 1\tReward: -74.00\n",
      "Test episode: 2\tReward: -71.00\n",
      "Test episode: 3\tReward: -70.00\n"
     ]
    }
   ],
   "source": [
    "agent.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
