{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from A2C_Cnt.trainer import A2C_Cnt_Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'Pendulum-v0'\n",
    "lr_base = 0.0001\n",
    "lr_decay = 0.0001\n",
    "\n",
    "random_seed = 42\n",
    "gamma = 0.99                # discount for future rewards\n",
    "batch_size = 32        # num of transitions sampled from replay buffer\n",
    "entropy_beta = 0.0001\n",
    "test_iters = 10000\n",
    "reward_steps = 2\n",
    "hidden_size = 128\n",
    "\n",
    "max_episodes = 10000         # max num of episodes\n",
    "max_timesteps = 5000        # max timesteps in one episode\n",
    "log_interval = 1000           # print avg reward after interval\n",
    "threshold = -140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODELS: Sequential(\n",
      "  (0): Linear(in_features=3, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      ") Sequential(\n",
      "  (0): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (1): Tanh()\n",
      ") Sequential(\n",
      "  (0): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (1): Softplus(beta=1, threshold=20)\n",
      ") Linear(in_features=128, out_features=1, bias=True)\n",
      "Random Seed: 42\n",
      "Action_space: Box(1,)\n",
      "Obs_space: Box(3,)\n",
      "Threshold: -140\n",
      "action_low: [-2.] action_high: [2.] \n",
      "\n",
      "DIR=./preTrained/a2c_cnt NAME=a2c_cnt_Pendulum-v0_42\n",
      "No models to load\n",
      "Training started ... \n",
      "\n",
      "Test done is 0.86 sec, reward -1099.165, steps 200\n",
      "201: done 1 episodes, mean reward -754.258, speed 195.90 f/s\n",
      "402: done 2 episodes, mean reward -1114.813, speed 1308.07 f/s\n",
      "603: done 3 episodes, mean reward -1053.956, speed 1347.61 f/s\n",
      "804: done 4 episodes, mean reward -1117.435, speed 1625.76 f/s\n",
      "1005: done 5 episodes, mean reward -1154.048, speed 1403.66 f/s\n",
      "1206: done 6 episodes, mean reward -1136.946, speed 1445.41 f/s\n",
      "1407: done 7 episodes, mean reward -1142.996, speed 1474.98 f/s\n",
      "1608: done 8 episodes, mean reward -1121.274, speed 1432.53 f/s\n",
      "1809: done 9 episodes, mean reward -1111.119, speed 1400.34 f/s\n",
      "2010: done 10 episodes, mean reward -1158.810, speed 1279.02 f/s\n",
      "2211: done 11 episodes, mean reward -1176.303, speed 1414.80 f/s\n",
      "2412: done 12 episodes, mean reward -1204.072, speed 1515.33 f/s\n",
      "2613: done 13 episodes, mean reward -1255.972, speed 1494.53 f/s\n",
      "2814: done 14 episodes, mean reward -1261.167, speed 1392.55 f/s\n",
      "3015: done 15 episodes, mean reward -1255.729, speed 1610.95 f/s\n",
      "3216: done 16 episodes, mean reward -1250.793, speed 1453.95 f/s\n",
      "3417: done 17 episodes, mean reward -1257.622, speed 1608.15 f/s\n",
      "3618: done 18 episodes, mean reward -1275.963, speed 1671.75 f/s\n",
      "3819: done 19 episodes, mean reward -1283.729, speed 1788.45 f/s\n",
      "4020: done 20 episodes, mean reward -1301.330, speed 1749.29 f/s\n",
      "4221: done 21 episodes, mean reward -1305.836, speed 1744.57 f/s\n",
      "4422: done 22 episodes, mean reward -1309.993, speed 1760.98 f/s\n",
      "4623: done 23 episodes, mean reward -1301.231, speed 1679.59 f/s\n",
      "4824: done 24 episodes, mean reward -1295.920, speed 1527.36 f/s\n",
      "5025: done 25 episodes, mean reward -1285.959, speed 1545.39 f/s\n",
      "5226: done 26 episodes, mean reward -1291.872, speed 1544.51 f/s\n",
      "5427: done 27 episodes, mean reward -1304.635, speed 1543.77 f/s\n",
      "5628: done 28 episodes, mean reward -1310.449, speed 1628.80 f/s\n",
      "5829: done 29 episodes, mean reward -1312.425, speed 1703.68 f/s\n",
      "6030: done 30 episodes, mean reward -1331.410, speed 1526.53 f/s\n",
      "6231: done 31 episodes, mean reward -1329.418, speed 1555.95 f/s\n",
      "6432: done 32 episodes, mean reward -1325.194, speed 1434.97 f/s\n",
      "6633: done 33 episodes, mean reward -1336.316, speed 1571.33 f/s\n",
      "6834: done 34 episodes, mean reward -1327.646, speed 1574.03 f/s\n",
      "7035: done 35 episodes, mean reward -1316.379, speed 1487.05 f/s\n",
      "7236: done 36 episodes, mean reward -1321.381, speed 1546.56 f/s\n",
      "7437: done 37 episodes, mean reward -1313.629, speed 1565.18 f/s\n",
      "7638: done 38 episodes, mean reward -1316.161, speed 1541.69 f/s\n",
      "7839: done 39 episodes, mean reward -1309.216, speed 1517.20 f/s\n",
      "8040: done 40 episodes, mean reward -1306.554, speed 1636.13 f/s\n",
      "8241: done 41 episodes, mean reward -1303.317, speed 1736.16 f/s\n",
      "8442: done 42 episodes, mean reward -1299.812, speed 1760.61 f/s\n",
      "8643: done 43 episodes, mean reward -1298.526, speed 1607.73 f/s\n",
      "8844: done 44 episodes, mean reward -1300.564, speed 1534.27 f/s\n",
      "9045: done 45 episodes, mean reward -1300.222, speed 1566.29 f/s\n",
      "9246: done 46 episodes, mean reward -1297.688, speed 1360.83 f/s\n",
      "9447: done 47 episodes, mean reward -1288.723, speed 1425.66 f/s\n",
      "9648: done 48 episodes, mean reward -1288.511, speed 1376.61 f/s\n",
      "9849: done 49 episodes, mean reward -1283.022, speed 1417.03 f/s\n",
      "Test done is 0.80 sec, reward -1391.845, steps 200\n",
      "10050: done 50 episodes, mean reward -1281.181, speed 212.98 f/s\n",
      "10251: done 51 episodes, mean reward -1277.320, speed 1657.80 f/s\n",
      "10452: done 52 episodes, mean reward -1287.070, speed 1073.45 f/s\n",
      "10653: done 53 episodes, mean reward -1284.838, speed 1398.89 f/s\n",
      "10854: done 54 episodes, mean reward -1280.771, speed 1329.97 f/s\n",
      "11055: done 55 episodes, mean reward -1280.253, speed 1521.32 f/s\n",
      "11256: done 56 episodes, mean reward -1276.202, speed 1372.19 f/s\n",
      "11457: done 57 episodes, mean reward -1282.245, speed 1673.40 f/s\n",
      "11658: done 58 episodes, mean reward -1275.530, speed 1567.30 f/s\n",
      "11859: done 59 episodes, mean reward -1273.717, speed 1533.16 f/s\n",
      "12060: done 60 episodes, mean reward -1268.642, speed 1541.22 f/s\n",
      "12261: done 61 episodes, mean reward -1264.752, speed 1522.99 f/s\n",
      "12462: done 62 episodes, mean reward -1271.197, speed 1608.09 f/s\n",
      "12663: done 63 episodes, mean reward -1266.444, speed 1597.50 f/s\n",
      "12864: done 64 episodes, mean reward -1262.000, speed 1520.47 f/s\n",
      "13065: done 65 episodes, mean reward -1258.085, speed 1554.81 f/s\n",
      "13266: done 66 episodes, mean reward -1258.894, speed 1579.82 f/s\n",
      "13467: done 67 episodes, mean reward -1261.442, speed 1348.18 f/s\n",
      "13668: done 68 episodes, mean reward -1266.097, speed 1384.00 f/s\n",
      "13869: done 69 episodes, mean reward -1260.941, speed 1465.31 f/s\n",
      "14070: done 70 episodes, mean reward -1268.033, speed 1447.01 f/s\n",
      "14271: done 71 episodes, mean reward -1262.377, speed 1441.17 f/s\n",
      "14472: done 72 episodes, mean reward -1256.967, speed 1381.84 f/s\n",
      "14673: done 73 episodes, mean reward -1263.442, speed 1485.29 f/s\n",
      "14874: done 74 episodes, mean reward -1264.464, speed 1461.00 f/s\n",
      "15075: done 75 episodes, mean reward -1265.335, speed 1406.57 f/s\n",
      "15276: done 76 episodes, mean reward -1272.874, speed 1562.78 f/s\n",
      "15477: done 77 episodes, mean reward -1270.523, speed 1476.99 f/s\n",
      "15678: done 78 episodes, mean reward -1272.573, speed 1503.11 f/s\n",
      "15879: done 79 episodes, mean reward -1270.051, speed 1455.68 f/s\n",
      "16080: done 80 episodes, mean reward -1268.175, speed 1385.97 f/s\n",
      "16281: done 81 episodes, mean reward -1273.923, speed 1469.55 f/s\n",
      "16482: done 82 episodes, mean reward -1277.034, speed 1257.75 f/s\n",
      "16683: done 83 episodes, mean reward -1272.804, speed 1406.10 f/s\n",
      "16884: done 84 episodes, mean reward -1277.038, speed 1373.45 f/s\n",
      "17085: done 85 episodes, mean reward -1275.551, speed 1476.04 f/s\n",
      "17286: done 86 episodes, mean reward -1276.724, speed 1383.58 f/s\n",
      "17487: done 87 episodes, mean reward -1277.431, speed 1388.09 f/s\n",
      "17688: done 88 episodes, mean reward -1272.739, speed 1397.46 f/s\n",
      "17889: done 89 episodes, mean reward -1268.861, speed 1312.31 f/s\n",
      "18090: done 90 episodes, mean reward -1268.068, speed 1516.09 f/s\n",
      "18291: done 91 episodes, mean reward -1267.334, speed 1478.77 f/s\n",
      "18492: done 92 episodes, mean reward -1262.244, speed 1465.84 f/s\n",
      "18693: done 93 episodes, mean reward -1260.180, speed 1352.64 f/s\n",
      "18894: done 94 episodes, mean reward -1262.783, speed 1456.14 f/s\n",
      "19095: done 95 episodes, mean reward -1259.687, speed 1590.25 f/s\n",
      "19296: done 96 episodes, mean reward -1258.631, speed 1439.46 f/s\n",
      "19497: done 97 episodes, mean reward -1258.988, speed 1488.05 f/s\n",
      "19698: done 98 episodes, mean reward -1260.305, speed 1372.60 f/s\n",
      "19899: done 99 episodes, mean reward -1258.975, speed 1406.71 f/s\n",
      "Test done is 0.72 sec, reward -1339.622, steps 200\n",
      "20100: done 100 episodes, mean reward -1262.805, speed 231.68 f/s\n",
      "20301: done 101 episodes, mean reward -1265.118, speed 1524.09 f/s\n",
      "20502: done 102 episodes, mean reward -1261.991, speed 1590.42 f/s\n",
      "20703: done 103 episodes, mean reward -1269.106, speed 1521.13 f/s\n",
      "20904: done 104 episodes, mean reward -1266.994, speed 1546.36 f/s\n",
      "21105: done 105 episodes, mean reward -1267.429, speed 1488.59 f/s\n",
      "21306: done 106 episodes, mean reward -1270.391, speed 1590.16 f/s\n",
      "21507: done 107 episodes, mean reward -1270.219, speed 1566.95 f/s\n",
      "21708: done 108 episodes, mean reward -1272.683, speed 1517.88 f/s\n",
      "21909: done 109 episodes, mean reward -1275.324, speed 1601.21 f/s\n",
      "22110: done 110 episodes, mean reward -1273.424, speed 1490.17 f/s\n",
      "22311: done 111 episodes, mean reward -1269.634, speed 1522.73 f/s\n",
      "22512: done 112 episodes, mean reward -1265.826, speed 1357.80 f/s\n",
      "22713: done 113 episodes, mean reward -1255.971, speed 1424.99 f/s\n",
      "22914: done 114 episodes, mean reward -1254.664, speed 1301.50 f/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23115: done 115 episodes, mean reward -1259.867, speed 1323.98 f/s\n",
      "23316: done 116 episodes, mean reward -1264.230, speed 1606.17 f/s\n",
      "23517: done 117 episodes, mean reward -1260.078, speed 1624.57 f/s\n",
      "23718: done 118 episodes, mean reward -1252.997, speed 1609.40 f/s\n",
      "23919: done 119 episodes, mean reward -1254.621, speed 1571.44 f/s\n",
      "24120: done 120 episodes, mean reward -1246.797, speed 1665.30 f/s\n",
      "24321: done 121 episodes, mean reward -1241.445, speed 1748.27 f/s\n",
      "24522: done 122 episodes, mean reward -1236.024, speed 1673.30 f/s\n",
      "24723: done 123 episodes, mean reward -1239.916, speed 1692.92 f/s\n",
      "24924: done 124 episodes, mean reward -1240.366, speed 1726.16 f/s\n",
      "25125: done 125 episodes, mean reward -1238.331, speed 1633.38 f/s\n",
      "25326: done 126 episodes, mean reward -1232.552, speed 1645.41 f/s\n",
      "25527: done 127 episodes, mean reward -1227.013, speed 1522.36 f/s\n",
      "25728: done 128 episodes, mean reward -1225.407, speed 1370.05 f/s\n",
      "25929: done 129 episodes, mean reward -1224.400, speed 1602.21 f/s\n",
      "26130: done 130 episodes, mean reward -1223.862, speed 1506.13 f/s\n",
      "26331: done 131 episodes, mean reward -1227.238, speed 1552.65 f/s\n",
      "26532: done 132 episodes, mean reward -1229.764, speed 1430.71 f/s\n",
      "26733: done 133 episodes, mean reward -1222.571, speed 1660.40 f/s\n",
      "26934: done 134 episodes, mean reward -1227.897, speed 1635.98 f/s\n",
      "27135: done 135 episodes, mean reward -1236.151, speed 1644.02 f/s\n",
      "27336: done 136 episodes, mean reward -1235.979, speed 1665.11 f/s\n",
      "27537: done 137 episodes, mean reward -1234.488, speed 1696.75 f/s\n",
      "27738: done 138 episodes, mean reward -1237.178, speed 1564.57 f/s\n",
      "27939: done 139 episodes, mean reward -1243.630, speed 1552.95 f/s\n",
      "28140: done 140 episodes, mean reward -1243.523, speed 1625.70 f/s\n",
      "28341: done 141 episodes, mean reward -1241.197, speed 1725.53 f/s\n",
      "28542: done 142 episodes, mean reward -1245.309, speed 1659.52 f/s\n",
      "28743: done 143 episodes, mean reward -1242.795, speed 1514.28 f/s\n",
      "28944: done 144 episodes, mean reward -1238.572, speed 1456.10 f/s\n",
      "29145: done 145 episodes, mean reward -1240.114, speed 1602.92 f/s\n",
      "29346: done 146 episodes, mean reward -1236.974, speed 1567.96 f/s\n",
      "29547: done 147 episodes, mean reward -1237.434, speed 1583.10 f/s\n",
      "29748: done 148 episodes, mean reward -1236.374, speed 1531.64 f/s\n",
      "29949: done 149 episodes, mean reward -1234.912, speed 1552.35 f/s\n",
      "Test done is 0.75 sec, reward -1315.066, steps 200\n",
      "30150: done 150 episodes, mean reward -1234.514, speed 229.10 f/s\n",
      "30351: done 151 episodes, mean reward -1233.241, speed 1728.19 f/s\n",
      "30552: done 152 episodes, mean reward -1233.758, speed 1662.59 f/s\n",
      "30753: done 153 episodes, mean reward -1233.647, speed 1625.67 f/s\n",
      "30954: done 154 episodes, mean reward -1233.626, speed 1661.60 f/s\n",
      "31155: done 155 episodes, mean reward -1230.561, speed 1511.18 f/s\n",
      "31356: done 156 episodes, mean reward -1228.269, speed 1634.58 f/s\n",
      "31557: done 157 episodes, mean reward -1221.772, speed 1547.13 f/s\n",
      "31758: done 158 episodes, mean reward -1224.499, speed 1652.16 f/s\n",
      "31959: done 159 episodes, mean reward -1228.000, speed 1699.90 f/s\n",
      "32160: done 160 episodes, mean reward -1233.677, speed 1563.97 f/s\n",
      "32361: done 161 episodes, mean reward -1232.009, speed 1568.21 f/s\n",
      "32562: done 162 episodes, mean reward -1225.861, speed 1374.18 f/s\n",
      "32763: done 163 episodes, mean reward -1228.090, speed 1528.91 f/s\n",
      "32964: done 164 episodes, mean reward -1231.753, speed 1496.06 f/s\n",
      "33165: done 165 episodes, mean reward -1238.668, speed 1354.07 f/s\n",
      "33366: done 166 episodes, mean reward -1237.106, speed 1328.18 f/s\n",
      "33567: done 167 episodes, mean reward -1239.627, speed 1584.57 f/s\n",
      "33768: done 168 episodes, mean reward -1234.011, speed 1540.23 f/s\n",
      "33969: done 169 episodes, mean reward -1235.531, speed 1700.57 f/s\n",
      "34170: done 170 episodes, mean reward -1234.726, speed 1594.45 f/s\n",
      "34371: done 171 episodes, mean reward -1236.750, speed 1235.84 f/s\n",
      "34572: done 172 episodes, mean reward -1239.604, speed 940.68 f/s\n",
      "34773: done 173 episodes, mean reward -1231.054, speed 1177.87 f/s\n",
      "34974: done 174 episodes, mean reward -1232.905, speed 1311.47 f/s\n",
      "35175: done 175 episodes, mean reward -1231.335, speed 1188.25 f/s\n",
      "35376: done 176 episodes, mean reward -1221.597, speed 1234.77 f/s\n",
      "35577: done 177 episodes, mean reward -1220.138, speed 1459.66 f/s\n",
      "35778: done 178 episodes, mean reward -1215.541, speed 1541.16 f/s\n",
      "35979: done 179 episodes, mean reward -1216.802, speed 1294.93 f/s\n",
      "36180: done 180 episodes, mean reward -1214.323, speed 1406.20 f/s\n",
      "36381: done 181 episodes, mean reward -1211.101, speed 1317.03 f/s\n",
      "36582: done 182 episodes, mean reward -1203.342, speed 1350.25 f/s\n",
      "36783: done 183 episodes, mean reward -1211.613, speed 1359.41 f/s\n"
     ]
    }
   ],
   "source": [
    "agent = A2C_Cnt_Trainer(env_name, hidden_size=hidden_size, random_seed=random_seed, lr_base=lr_base, lr_decay=lr_decay,\n",
    "                   gamma=gamma, batch_size=batch_size, entropy_beta=entropy_beta, test_iters=test_iters,\n",
    "                   reward_steps=reward_steps, max_episodes=max_episodes, max_timesteps=max_timesteps,\n",
    "                   log_interval=log_interval, threshold=threshold)\n",
    "agent.train()\n",
    "\n",
    "agent.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
