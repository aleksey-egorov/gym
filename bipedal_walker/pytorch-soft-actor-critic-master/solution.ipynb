{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:This caffe2 python run does not have GPU support. Will run in CPU only mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: <class 'normalized_actions.NormalizedActions'> doesn't implement 'action' method. Maybe it implements deprecated '_action' method.\u001b[0m\n",
      "Episode: 0, total numsteps: 170, reward: -133.76, average reward: -133.76\n",
      "Episode: 1, total numsteps: 1770, reward: -92.83, average reward: -113.3\n",
      "Episode: 2, total numsteps: 1861, reward: -120.72, average reward: -115.77\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import math\n",
    "import gym\n",
    "import numpy as np\n",
    "import itertools\n",
    "import torch\n",
    "from sac import SAC\n",
    "from tensorboardX import SummaryWriter\n",
    "from normalized_actions import NormalizedActions\n",
    "from replay_memory import ReplayMemory\n",
    "\n",
    "'''\n",
    "parser = argparse.ArgumentParser(description='PyTorch REINFORCE example')\n",
    "parser.add_argument('--env-name', default=\"BipedalWalker-v2\",\n",
    "                    help='name of the environment to run')\n",
    "parser.add_argument('--policy', default=\"Gaussian\",\n",
    "                    help='algorithm to use: Gaussian | Deterministic')\n",
    "parser.add_argument('--eval', type=bool, default=False,\n",
    "                    help='Evaluate a policy (default:False)')\n",
    "parser.add_argument('--gamma', type=float, default=0.99, metavar='G',\n",
    "                    help='discount factor for reward (default: 0.99)')\n",
    "parser.add_argument('--tau', type=float, default=0.005, metavar='G',\n",
    "                    help='target smoothing coefficient(τ) (default: 0.005)')\n",
    "parser.add_argument('--lr', type=float, default=0.0003, metavar='G',\n",
    "                    help='learning rate (default: 0.0003)')\n",
    "parser.add_argument('--alpha', type=float, default=0.2, metavar='G',\n",
    "                    help='Temperature parameter α determines the relative importance of the entropy term against the reward (default: 0.2)')\n",
    "parser.add_argument('--seed', type=int, default=543, metavar='N',\n",
    "                    help='random seed (default: 543)')\n",
    "parser.add_argument('--batch_size', type=int, default=256, metavar='N',\n",
    "                    help='batch size (default: 256)')\n",
    "parser.add_argument('--num_steps', type=int, default=1000000, metavar='N',\n",
    "                    help='maximum number of steps (default: 1000000)')\n",
    "parser.add_argument('--hidden_size', type=int, default=256, metavar='N',\n",
    "                    help='hidden size (default: 256)')\n",
    "parser.add_argument('--updates_per_step', type=int, default=1, metavar='N',\n",
    "                    help='model updates per simulator step (default: 1)')\n",
    "parser.add_argument('--target_update_interval', type=int, default=1, metavar='N',\n",
    "                    help='Value target update per no. of updates per step (default: 1)')\n",
    "parser.add_argument('--replay_size', type=int, default=1000000, metavar='N',\n",
    "                    help='size of replay buffer (default: 10000000)')\n",
    "args = parser.parse_args()\n",
    "'''\n",
    "\n",
    "args = {\n",
    "    \"env_name\": \"BipedalWalker-v2\",\n",
    "    \"policy\": \"Gaussian\",\n",
    "    \"eval\": False,\n",
    "    \"gamma\": 0.99,\n",
    "    \"tau\": 0.005,\n",
    "    \"lr\": 0.0003,\n",
    "    \"alpha\": 0.2,\n",
    "    \"seed\": 543,\n",
    "    \"batch_size\": 256,\n",
    "    \"num_steps\": 1000000,\n",
    "    \"steps_in_episode\": 3000,\n",
    "    \"hidden_size\": 256,\n",
    "    \"updates_per_step\": 1,\n",
    "    \"target_update_interval\": 1,\n",
    "    \"replay_size\": 1000000\n",
    "}    \n",
    "\n",
    "\n",
    "# Environment\n",
    "env = NormalizedActions(gym.make(args['env_name']))\n",
    "env.seed(args['seed'])\n",
    "torch.manual_seed(args['seed'])\n",
    "np.random.seed(args['seed'])\n",
    "\n",
    "# Agent\n",
    "agent = SAC(env.observation_space.shape[0], env.action_space, args)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Memory\n",
    "memory = ReplayMemory(args['replay_size'])\n",
    "\n",
    "# Training Loop\n",
    "rewards = []\n",
    "total_numsteps = 0\n",
    "updates = 0\n",
    "\n",
    "for i_episode in itertools.count():\n",
    "    state = env.reset()\n",
    "\n",
    "    episode_reward = 0\n",
    "    for t in range(args['steps_in_episode']):\n",
    "        action = agent.select_action(state)  # Sample action from policy\n",
    "        next_state, reward, done, _ = env.step(action)  # Step\n",
    "        mask = not done  # 1 for not done and 0 for done\n",
    "        memory.push(state, action, reward, next_state, mask)  # Append transition to memory\n",
    "        if len(memory) > args['batch_size']:\n",
    "            for i in range(args['updates_per_step']): # Number of updates per step in environment\n",
    "                # Sample a batch from memory\n",
    "                state_batch, action_batch, reward_batch, next_state_batch, mask_batch = memory.sample(args['batch_size'])\n",
    "                # Update parameters of all the networks\n",
    "                value_loss, critic_1_loss, critic_2_loss, policy_loss = agent.update_parameters(state_batch, action_batch, \n",
    "                                                                                                reward_batch, next_state_batch, \n",
    "                                                                                                mask_batch, updates)\n",
    "\n",
    "                writer.add_scalar('loss/value', value_loss, updates)\n",
    "                writer.add_scalar('loss/critic_1', critic_1_loss, updates)\n",
    "                writer.add_scalar('loss/critic_2', critic_2_loss, updates)\n",
    "                writer.add_scalar('loss/policy', policy_loss, updates)\n",
    "                updates += 1\n",
    "\n",
    "        state = next_state\n",
    "        total_numsteps += 1\n",
    "        episode_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if total_numsteps > args['num_steps']:\n",
    "        break\n",
    "\n",
    "    writer.add_scalar('reward/train', episode_reward, i_episode)\n",
    "    rewards.append(episode_reward)\n",
    "    print(\"Episode: {}, total numsteps: {}, reward: {}, average reward: {}\".format(i_episode, total_numsteps, np.round(rewards[-1],2),\n",
    "                                                                                np.round(np.mean(rewards[-100:]),2)))\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
