{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: <class 'normalized_actions.NormalizedActions'> doesn't implement 'action' method. Maybe it implements deprecated '_action' method.\u001b[0m\n",
      "Episode: 0, total numsteps: 170, reward: -133.75, average reward: -133.75\n",
      "Episode: 1, total numsteps: 1770, reward: -95.59, average reward: -114.67\n",
      "Episode: 2, total numsteps: 3370, reward: -64.86, average reward: -98.07\n",
      "Episode: 3, total numsteps: 3415, reward: -108.45, average reward: -100.66\n",
      "Episode: 4, total numsteps: 3587, reward: -132.66, average reward: -107.06\n",
      "Episode: 5, total numsteps: 3648, reward: -108.26, average reward: -107.26\n",
      "Episode: 6, total numsteps: 5248, reward: -84.3, average reward: -103.98\n",
      "Episode: 7, total numsteps: 5327, reward: -112.53, average reward: -105.05\n",
      "Episode: 8, total numsteps: 5379, reward: -111.81, average reward: -105.8\n",
      "Episode: 9, total numsteps: 5436, reward: -106.75, average reward: -105.9\n",
      "Episode: 10, total numsteps: 5670, reward: -137.75, average reward: -108.79\n",
      "Episode: 11, total numsteps: 5741, reward: -111.46, average reward: -109.01\n",
      "Episode: 12, total numsteps: 6821, reward: -183.03, average reward: -114.71\n",
      "Episode: 13, total numsteps: 8421, reward: -91.17, average reward: -113.03\n",
      "Episode: 14, total numsteps: 9582, reward: -187.86, average reward: -118.02\n",
      "Episode: 15, total numsteps: 10479, reward: -168.51, average reward: -121.17\n",
      "Episode: 16, total numsteps: 10560, reward: -103.87, average reward: -120.15\n",
      "Episode: 17, total numsteps: 12160, reward: -68.6, average reward: -117.29\n",
      "Episode: 18, total numsteps: 12236, reward: -106.51, average reward: -116.72\n",
      "Episode: 19, total numsteps: 12326, reward: -99.36, average reward: -115.85\n",
      "Episode: 20, total numsteps: 13926, reward: -91.02, average reward: -114.67\n",
      "Episode: 21, total numsteps: 13991, reward: -101.27, average reward: -114.06\n",
      "Episode: 22, total numsteps: 14060, reward: -112.19, average reward: -113.98\n",
      "Episode: 23, total numsteps: 14117, reward: -110.62, average reward: -113.84\n",
      "Episode: 24, total numsteps: 14316, reward: -103.18, average reward: -113.41\n",
      "Episode: 25, total numsteps: 14372, reward: -100.26, average reward: -112.91\n",
      "Episode: 26, total numsteps: 14448, reward: -101.27, average reward: -112.48\n",
      "Episode: 27, total numsteps: 14532, reward: -100.08, average reward: -112.03\n",
      "Episode: 28, total numsteps: 14600, reward: -102.01, average reward: -111.69\n",
      "Episode: 29, total numsteps: 16200, reward: -91.15, average reward: -111.0\n",
      "Episode: 30, total numsteps: 16393, reward: -115.95, average reward: -111.16\n",
      "Episode: 31, total numsteps: 16504, reward: -118.24, average reward: -111.38\n",
      "Episode: 32, total numsteps: 18104, reward: -59.65, average reward: -109.82\n",
      "Episode: 33, total numsteps: 18174, reward: -121.3, average reward: -110.15\n",
      "Episode: 34, total numsteps: 19774, reward: -85.19, average reward: -109.44\n",
      "Episode: 35, total numsteps: 19833, reward: -118.59, average reward: -109.7\n",
      "Episode: 36, total numsteps: 19889, reward: -115.78, average reward: -109.86\n",
      "Episode: 37, total numsteps: 20451, reward: -120.33, average reward: -110.14\n",
      "Episode: 38, total numsteps: 20510, reward: -121.99, average reward: -110.44\n",
      "Episode: 39, total numsteps: 20563, reward: -120.09, average reward: -110.68\n",
      "Episode: 40, total numsteps: 20621, reward: -119.24, average reward: -110.89\n",
      "Episode: 41, total numsteps: 20673, reward: -118.5, average reward: -111.07\n",
      "Episode: 42, total numsteps: 20730, reward: -116.82, average reward: -111.2\n",
      "Episode: 43, total numsteps: 20807, reward: -102.89, average reward: -111.02\n",
      "Episode: 44, total numsteps: 20870, reward: -105.39, average reward: -110.89\n",
      "Episode: 45, total numsteps: 20936, reward: -98.85, average reward: -110.63\n",
      "Episode: 46, total numsteps: 21038, reward: -99.98, average reward: -110.4\n",
      "Episode: 47, total numsteps: 21108, reward: -119.39, average reward: -110.59\n",
      "Episode: 48, total numsteps: 21177, reward: -120.4, average reward: -110.79\n",
      "Episode: 49, total numsteps: 21254, reward: -102.33, average reward: -110.62\n",
      "Episode: 50, total numsteps: 21352, reward: -98.3, average reward: -110.38\n",
      "Episode: 51, total numsteps: 21456, reward: -129.1, average reward: -110.74\n",
      "Episode: 52, total numsteps: 21498, reward: -103.47, average reward: -110.6\n",
      "Episode: 53, total numsteps: 21561, reward: -120.97, average reward: -110.79\n",
      "Episode: 54, total numsteps: 21616, reward: -122.2, average reward: -111.0\n",
      "Episode: 55, total numsteps: 21691, reward: -122.76, average reward: -111.21\n",
      "Episode: 56, total numsteps: 21742, reward: -120.83, average reward: -111.38\n",
      "Episode: 57, total numsteps: 21811, reward: -120.06, average reward: -111.53\n",
      "Episode: 58, total numsteps: 21913, reward: -114.16, average reward: -111.57\n",
      "Episode: 59, total numsteps: 21972, reward: -105.91, average reward: -111.48\n",
      "Episode: 60, total numsteps: 22075, reward: -98.83, average reward: -111.27\n",
      "Episode: 61, total numsteps: 22182, reward: -121.72, average reward: -111.44\n",
      "Episode: 62, total numsteps: 22256, reward: -122.83, average reward: -111.62\n",
      "Episode: 63, total numsteps: 22314, reward: -115.1, average reward: -111.68\n",
      "Episode: 64, total numsteps: 22364, reward: -118.47, average reward: -111.78\n",
      "Episode: 65, total numsteps: 22419, reward: -117.12, average reward: -111.86\n",
      "Episode: 66, total numsteps: 22495, reward: -96.66, average reward: -111.63\n",
      "Episode: 67, total numsteps: 22572, reward: -102.86, average reward: -111.51\n",
      "Episode: 68, total numsteps: 22628, reward: -121.98, average reward: -111.66\n",
      "Episode: 69, total numsteps: 22831, reward: -121.31, average reward: -111.8\n",
      "Episode: 70, total numsteps: 22882, reward: -121.51, average reward: -111.93\n",
      "Episode: 71, total numsteps: 23022, reward: -121.61, average reward: -112.07\n",
      "Episode: 72, total numsteps: 23089, reward: -115.9, average reward: -112.12\n",
      "Episode: 73, total numsteps: 23129, reward: -112.61, average reward: -112.13\n",
      "Episode: 74, total numsteps: 23437, reward: -129.29, average reward: -112.35\n",
      "Episode: 75, total numsteps: 23489, reward: -116.58, average reward: -112.41\n",
      "Episode: 76, total numsteps: 23548, reward: -117.09, average reward: -112.47\n",
      "Episode: 77, total numsteps: 23602, reward: -119.01, average reward: -112.55\n",
      "Episode: 78, total numsteps: 23642, reward: -115.06, average reward: -112.59\n",
      "Episode: 79, total numsteps: 23751, reward: -120.3, average reward: -112.68\n",
      "Episode: 80, total numsteps: 23898, reward: -102.24, average reward: -112.55\n",
      "Episode: 81, total numsteps: 24079, reward: -124.45, average reward: -112.7\n",
      "Episode: 82, total numsteps: 24124, reward: -118.48, average reward: -112.77\n",
      "Episode: 83, total numsteps: 24175, reward: -122.72, average reward: -112.89\n",
      "Episode: 84, total numsteps: 24229, reward: -119.9, average reward: -112.97\n",
      "Episode: 85, total numsteps: 24283, reward: -118.41, average reward: -113.03\n",
      "Episode: 86, total numsteps: 24359, reward: -99.07, average reward: -112.87\n",
      "Episode: 87, total numsteps: 24448, reward: -101.5, average reward: -112.74\n",
      "Episode: 88, total numsteps: 24560, reward: -121.94, average reward: -112.85\n",
      "Episode: 89, total numsteps: 24603, reward: -112.97, average reward: -112.85\n",
      "Episode: 90, total numsteps: 24659, reward: -119.76, average reward: -112.92\n",
      "Episode: 91, total numsteps: 24795, reward: -122.45, average reward: -113.03\n",
      "Episode: 92, total numsteps: 24859, reward: -119.87, average reward: -113.1\n",
      "Episode: 93, total numsteps: 24949, reward: -119.83, average reward: -113.17\n",
      "Episode: 94, total numsteps: 25020, reward: -120.4, average reward: -113.25\n",
      "Episode: 95, total numsteps: 25085, reward: -119.41, average reward: -113.31\n",
      "Episode: 96, total numsteps: 25225, reward: -121.32, average reward: -113.4\n",
      "Episode: 97, total numsteps: 25294, reward: -121.51, average reward: -113.48\n",
      "Episode: 98, total numsteps: 25336, reward: -113.31, average reward: -113.48\n",
      "Episode: 99, total numsteps: 25416, reward: -118.57, average reward: -113.53\n",
      "Episode: 100, total numsteps: 25481, reward: -119.57, average reward: -113.39\n",
      "Episode: 101, total numsteps: 25551, reward: -120.55, average reward: -113.64\n",
      "Episode: 102, total numsteps: 25603, reward: -118.17, average reward: -114.17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 103, total numsteps: 25678, reward: -125.05, average reward: -114.33\n",
      "Episode: 104, total numsteps: 25757, reward: -124.29, average reward: -114.25\n",
      "Episode: 105, total numsteps: 25842, reward: -121.81, average reward: -114.39\n",
      "Episode: 106, total numsteps: 25949, reward: -124.3, average reward: -114.79\n",
      "Episode: 107, total numsteps: 26049, reward: -100.96, average reward: -114.67\n",
      "Episode: 108, total numsteps: 26132, reward: -117.61, average reward: -114.73\n",
      "Episode: 109, total numsteps: 26234, reward: -121.81, average reward: -114.88\n",
      "Episode: 110, total numsteps: 26331, reward: -122.6, average reward: -114.73\n",
      "Episode: 111, total numsteps: 26372, reward: -109.08, average reward: -114.7\n",
      "Episode: 112, total numsteps: 26412, reward: -109.23, average reward: -113.97\n",
      "Episode: 113, total numsteps: 26519, reward: -121.79, average reward: -114.27\n",
      "Episode: 114, total numsteps: 26573, reward: -117.46, average reward: -113.57\n",
      "Episode: 115, total numsteps: 26614, reward: -108.87, average reward: -112.97\n",
      "Episode: 116, total numsteps: 26666, reward: -119.99, average reward: -113.13\n",
      "Episode: 117, total numsteps: 26706, reward: -110.4, average reward: -113.55\n",
      "Episode: 118, total numsteps: 26747, reward: -109.45, average reward: -113.58\n",
      "Episode: 119, total numsteps: 26782, reward: -109.32, average reward: -113.68\n",
      "Episode: 120, total numsteps: 26824, reward: -108.88, average reward: -113.86\n",
      "Episode: 121, total numsteps: 26878, reward: -111.18, average reward: -113.96\n",
      "Episode: 122, total numsteps: 26923, reward: -111.2, average reward: -113.95\n",
      "Episode: 123, total numsteps: 26964, reward: -109.73, average reward: -113.94\n",
      "Episode: 124, total numsteps: 27013, reward: -112.37, average reward: -114.03\n",
      "Episode: 125, total numsteps: 27052, reward: -109.8, average reward: -114.13\n",
      "Episode: 126, total numsteps: 27109, reward: -121.41, average reward: -114.33\n",
      "Episode: 127, total numsteps: 27150, reward: -109.2, average reward: -114.42\n",
      "Episode: 128, total numsteps: 27205, reward: -117.05, average reward: -114.57\n",
      "Episode: 129, total numsteps: 27256, reward: -119.97, average reward: -114.86\n",
      "Episode: 130, total numsteps: 27291, reward: -109.04, average reward: -114.79\n",
      "Episode: 131, total numsteps: 27330, reward: -108.84, average reward: -114.69\n",
      "Episode: 132, total numsteps: 27366, reward: -107.59, average reward: -115.17\n",
      "Episode: 133, total numsteps: 27405, reward: -112.66, average reward: -115.09\n",
      "Episode: 134, total numsteps: 27718, reward: -145.75, average reward: -115.69\n",
      "Episode: 135, total numsteps: 27753, reward: -109.31, average reward: -115.6\n",
      "Episode: 136, total numsteps: 29353, reward: -99.21, average reward: -115.43\n",
      "Episode: 137, total numsteps: 30953, reward: -98.32, average reward: -115.21\n",
      "Episode: 138, total numsteps: 32473, reward: -214.74, average reward: -116.14\n",
      "Episode: 139, total numsteps: 34073, reward: -78.73, average reward: -115.73\n",
      "Episode: 140, total numsteps: 35673, reward: -71.9, average reward: -115.25\n",
      "Episode: 141, total numsteps: 37273, reward: -90.25, average reward: -114.97\n",
      "Episode: 142, total numsteps: 38873, reward: -95.23, average reward: -114.76\n",
      "Episode: 143, total numsteps: 40473, reward: -97.39, average reward: -114.7\n",
      "Episode: 144, total numsteps: 40587, reward: -123.27, average reward: -114.88\n",
      "Episode: 145, total numsteps: 42187, reward: -95.46, average reward: -114.85\n",
      "Episode: 146, total numsteps: 43787, reward: -93.6, average reward: -114.78\n",
      "Episode: 147, total numsteps: 45387, reward: -92.29, average reward: -114.51\n",
      "Episode: 148, total numsteps: 46987, reward: -88.3, average reward: -114.19\n",
      "Episode: 149, total numsteps: 48587, reward: -87.44, average reward: -114.04\n",
      "Episode: 150, total numsteps: 50187, reward: -89.54, average reward: -113.95\n",
      "Episode: 151, total numsteps: 51787, reward: -87.3, average reward: -113.54\n",
      "Episode: 152, total numsteps: 53387, reward: -87.8, average reward: -113.38\n",
      "Episode: 153, total numsteps: 54987, reward: -77.85, average reward: -112.95\n",
      "Episode: 154, total numsteps: 56587, reward: -94.65, average reward: -112.67\n",
      "Episode: 155, total numsteps: 58187, reward: -78.36, average reward: -112.23\n",
      "Episode: 156, total numsteps: 59787, reward: -81.67, average reward: -111.84\n",
      "Episode: 157, total numsteps: 61387, reward: -82.97, average reward: -111.47\n",
      "Episode: 158, total numsteps: 62987, reward: -79.91, average reward: -111.12\n",
      "Episode: 159, total numsteps: 64587, reward: -81.43, average reward: -110.88\n",
      "Episode: 160, total numsteps: 66187, reward: -78.55, average reward: -110.68\n",
      "Episode: 161, total numsteps: 67787, reward: -83.34, average reward: -110.29\n",
      "Episode: 162, total numsteps: 69387, reward: -77.25, average reward: -109.84\n",
      "Episode: 163, total numsteps: 70987, reward: -70.89, average reward: -109.39\n",
      "Episode: 164, total numsteps: 72587, reward: -79.41, average reward: -109.0\n",
      "Episode: 165, total numsteps: 74187, reward: -87.17, average reward: -108.7\n",
      "Episode: 166, total numsteps: 75787, reward: -76.12, average reward: -108.5\n",
      "Episode: 167, total numsteps: 77387, reward: -79.36, average reward: -108.26\n",
      "Episode: 168, total numsteps: 78987, reward: -79.48, average reward: -107.84\n",
      "Episode: 169, total numsteps: 80587, reward: -79.2, average reward: -107.42\n",
      "Episode: 170, total numsteps: 82187, reward: -84.18, average reward: -107.04\n",
      "Episode: 171, total numsteps: 83787, reward: -80.94, average reward: -106.64\n",
      "Episode: 172, total numsteps: 85387, reward: -80.46, average reward: -106.28\n",
      "Episode: 173, total numsteps: 86987, reward: -75.67, average reward: -105.91\n",
      "Episode: 174, total numsteps: 88587, reward: -81.52, average reward: -105.44\n",
      "Episode: 175, total numsteps: 90187, reward: -76.68, average reward: -105.04\n",
      "Episode: 176, total numsteps: 91787, reward: -85.19, average reward: -104.72\n",
      "Episode: 177, total numsteps: 93387, reward: -82.33, average reward: -104.35\n",
      "Episode: 178, total numsteps: 94987, reward: -78.24, average reward: -103.98\n",
      "Episode: 179, total numsteps: 96587, reward: -83.77, average reward: -103.62\n",
      "Episode: 180, total numsteps: 98187, reward: -80.04, average reward: -103.4\n",
      "Episode: 181, total numsteps: 99787, reward: -78.87, average reward: -102.94\n",
      "Episode: 182, total numsteps: 101387, reward: -73.31, average reward: -102.49\n",
      "Episode: 183, total numsteps: 102987, reward: -84.34, average reward: -102.1\n",
      "Episode: 184, total numsteps: 104587, reward: -79.33, average reward: -101.7\n",
      "Episode: 185, total numsteps: 106187, reward: -73.7, average reward: -101.25\n",
      "Episode: 186, total numsteps: 107787, reward: -72.34, average reward: -100.98\n",
      "Episode: 187, total numsteps: 109387, reward: -70.29, average reward: -100.67\n",
      "Episode: 188, total numsteps: 110987, reward: -78.71, average reward: -100.24\n",
      "Episode: 189, total numsteps: 112587, reward: -78.79, average reward: -99.9\n",
      "Episode: 190, total numsteps: 114187, reward: -70.12, average reward: -99.4\n",
      "Episode: 191, total numsteps: 115787, reward: -79.9, average reward: -98.98\n",
      "Episode: 192, total numsteps: 117387, reward: -80.69, average reward: -98.58\n",
      "Episode: 193, total numsteps: 118987, reward: -75.24, average reward: -98.14\n",
      "Episode: 194, total numsteps: 120587, reward: -76.82, average reward: -97.7\n",
      "Episode: 195, total numsteps: 122187, reward: -74.96, average reward: -97.26\n",
      "Episode: 196, total numsteps: 123787, reward: -78.53, average reward: -96.83\n",
      "Episode: 197, total numsteps: 125387, reward: -74.53, average reward: -96.36\n",
      "Episode: 198, total numsteps: 126987, reward: -75.39, average reward: -95.98\n",
      "Episode: 199, total numsteps: 128587, reward: -73.25, average reward: -95.53\n",
      "Episode: 200, total numsteps: 130187, reward: -80.73, average reward: -95.14\n",
      "Episode: 201, total numsteps: 131787, reward: -75.35, average reward: -94.69\n",
      "Episode: 202, total numsteps: 133387, reward: -75.5, average reward: -94.26\n",
      "Episode: 203, total numsteps: 134987, reward: -77.41, average reward: -93.78\n",
      "Episode: 204, total numsteps: 136587, reward: -77.31, average reward: -93.32\n",
      "Episode: 205, total numsteps: 138187, reward: -85.99, average reward: -92.96\n",
      "Episode: 206, total numsteps: 139787, reward: -79.32, average reward: -92.51\n",
      "Episode: 207, total numsteps: 141387, reward: -70.7, average reward: -92.2\n",
      "Episode: 208, total numsteps: 142987, reward: -75.3, average reward: -91.78\n",
      "Episode: 209, total numsteps: 144587, reward: -77.33, average reward: -91.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 210, total numsteps: 146187, reward: -68.59, average reward: -90.8\n",
      "Episode: 211, total numsteps: 147787, reward: -77.56, average reward: -90.48\n",
      "Episode: 212, total numsteps: 149387, reward: -73.47, average reward: -90.12\n",
      "Episode: 213, total numsteps: 150987, reward: -82.62, average reward: -89.73\n",
      "Episode: 214, total numsteps: 152587, reward: -70.47, average reward: -89.26\n",
      "Episode: 215, total numsteps: 154187, reward: -81.09, average reward: -88.98\n",
      "Episode: 216, total numsteps: 155787, reward: -76.93, average reward: -88.55\n",
      "Episode: 217, total numsteps: 157387, reward: -75.93, average reward: -88.21\n",
      "Episode: 218, total numsteps: 158987, reward: -74.35, average reward: -87.86\n",
      "Episode: 219, total numsteps: 160587, reward: -74.05, average reward: -87.51\n",
      "Episode: 220, total numsteps: 160665, reward: -116.58, average reward: -87.58\n",
      "Episode: 221, total numsteps: 162265, reward: -79.41, average reward: -87.26\n",
      "Episode: 222, total numsteps: 163865, reward: -77.08, average reward: -86.92\n",
      "Episode: 223, total numsteps: 165465, reward: -72.56, average reward: -86.55\n",
      "Episode: 224, total numsteps: 167065, reward: -74.15, average reward: -86.17\n",
      "Episode: 225, total numsteps: 168665, reward: -79.72, average reward: -85.87\n",
      "Episode: 226, total numsteps: 170265, reward: -73.13, average reward: -85.39\n",
      "Episode: 227, total numsteps: 170340, reward: -117.36, average reward: -85.47\n",
      "Episode: 228, total numsteps: 171940, reward: -74.89, average reward: -85.05\n",
      "Episode: 229, total numsteps: 172003, reward: -118.28, average reward: -85.03\n",
      "Episode: 230, total numsteps: 173603, reward: -75.21, average reward: -84.69\n",
      "Episode: 231, total numsteps: 175203, reward: -79.33, average reward: -84.4\n",
      "Episode: 232, total numsteps: 176803, reward: -75.27, average reward: -84.07\n",
      "Episode: 233, total numsteps: 178403, reward: -79.12, average reward: -83.74\n",
      "Episode: 234, total numsteps: 180003, reward: -80.72, average reward: -83.09\n",
      "Episode: 235, total numsteps: 181603, reward: -81.68, average reward: -82.81\n",
      "Episode: 236, total numsteps: 183203, reward: -83.48, average reward: -82.65\n",
      "Episode: 237, total numsteps: 184803, reward: -82.26, average reward: -82.49\n",
      "Episode: 238, total numsteps: 186403, reward: -69.91, average reward: -81.04\n",
      "Episode: 239, total numsteps: 188003, reward: -76.17, average reward: -81.02\n",
      "Episode: 240, total numsteps: 189603, reward: -81.37, average reward: -81.11\n",
      "Episode: 241, total numsteps: 191203, reward: -81.95, average reward: -81.03\n",
      "Episode: 242, total numsteps: 192803, reward: -76.35, average reward: -80.84\n",
      "Episode: 243, total numsteps: 194403, reward: -79.04, average reward: -80.66\n",
      "Episode: 244, total numsteps: 196003, reward: -74.53, average reward: -80.17\n",
      "Episode: 245, total numsteps: 197603, reward: -73.81, average reward: -79.95\n",
      "Episode: 246, total numsteps: 199203, reward: -83.91, average reward: -79.86\n",
      "Episode: 247, total numsteps: 200803, reward: -81.18, average reward: -79.75\n",
      "Episode: 248, total numsteps: 202403, reward: -80.75, average reward: -79.67\n",
      "Episode: 249, total numsteps: 204003, reward: -79.39, average reward: -79.59\n",
      "Episode: 250, total numsteps: 205603, reward: -75.43, average reward: -79.45\n",
      "Episode: 251, total numsteps: 207203, reward: -72.44, average reward: -79.3\n",
      "Episode: 252, total numsteps: 208803, reward: -74.17, average reward: -79.16\n",
      "Episode: 253, total numsteps: 210403, reward: -74.63, average reward: -79.13\n",
      "Episode: 254, total numsteps: 212003, reward: -68.48, average reward: -78.87\n",
      "Episode: 255, total numsteps: 213603, reward: -70.76, average reward: -78.79\n",
      "Episode: 256, total numsteps: 215203, reward: -78.15, average reward: -78.76\n",
      "Episode: 257, total numsteps: 216803, reward: -79.4, average reward: -78.72\n",
      "Episode: 258, total numsteps: 218403, reward: -75.53, average reward: -78.68\n",
      "Episode: 259, total numsteps: 220003, reward: -77.86, average reward: -78.64\n",
      "Episode: 260, total numsteps: 221603, reward: -75.63, average reward: -78.61\n",
      "Episode: 261, total numsteps: 223203, reward: -83.18, average reward: -78.61\n",
      "Episode: 262, total numsteps: 223270, reward: -99.29, average reward: -78.83\n",
      "Episode: 263, total numsteps: 223937, reward: -146.97, average reward: -79.59\n",
      "Episode: 264, total numsteps: 224038, reward: -104.21, average reward: -79.84\n",
      "Episode: 265, total numsteps: 224123, reward: -99.33, average reward: -79.96\n",
      "Episode: 266, total numsteps: 224214, reward: -101.77, average reward: -80.22\n",
      "Episode: 267, total numsteps: 224317, reward: -103.2, average reward: -80.46\n",
      "Episode: 268, total numsteps: 224394, reward: -99.42, average reward: -80.66\n",
      "Episode: 269, total numsteps: 224473, reward: -100.11, average reward: -80.87\n",
      "Episode: 270, total numsteps: 224546, reward: -100.01, average reward: -81.03\n",
      "Episode: 271, total numsteps: 224612, reward: -119.37, average reward: -81.41\n",
      "Episode: 272, total numsteps: 226212, reward: -91.39, average reward: -81.52\n",
      "Episode: 273, total numsteps: 226260, reward: -102.95, average reward: -81.79\n",
      "Episode: 274, total numsteps: 226315, reward: -101.57, average reward: -81.99\n",
      "Episode: 275, total numsteps: 226387, reward: -103.48, average reward: -82.26\n",
      "Episode: 276, total numsteps: 226438, reward: -103.0, average reward: -82.44\n",
      "Episode: 277, total numsteps: 226490, reward: -101.83, average reward: -82.63\n",
      "Episode: 278, total numsteps: 226545, reward: -99.47, average reward: -82.85\n",
      "Episode: 279, total numsteps: 228145, reward: -84.24, average reward: -82.85\n",
      "Episode: 280, total numsteps: 228205, reward: -101.17, average reward: -83.06\n",
      "Episode: 281, total numsteps: 228258, reward: -101.7, average reward: -83.29\n",
      "Episode: 282, total numsteps: 228310, reward: -101.03, average reward: -83.57\n",
      "Episode: 283, total numsteps: 228362, reward: -102.74, average reward: -83.75\n",
      "Episode: 284, total numsteps: 228408, reward: -102.85, average reward: -83.99\n",
      "Episode: 285, total numsteps: 228456, reward: -102.39, average reward: -84.27\n",
      "Episode: 286, total numsteps: 228500, reward: -102.81, average reward: -84.58\n",
      "Episode: 287, total numsteps: 228555, reward: -100.98, average reward: -84.88\n",
      "Episode: 288, total numsteps: 228605, reward: -102.14, average reward: -85.12\n",
      "Episode: 289, total numsteps: 228653, reward: -102.48, average reward: -85.36\n",
      "Episode: 290, total numsteps: 228706, reward: -101.38, average reward: -85.67\n",
      "Episode: 291, total numsteps: 228758, reward: -101.74, average reward: -85.89\n",
      "Episode: 292, total numsteps: 228811, reward: -101.51, average reward: -86.1\n",
      "Episode: 293, total numsteps: 228862, reward: -101.09, average reward: -86.35\n",
      "Episode: 294, total numsteps: 228907, reward: -101.96, average reward: -86.61\n",
      "Episode: 295, total numsteps: 228951, reward: -102.25, average reward: -86.88\n",
      "Episode: 296, total numsteps: 229007, reward: -101.33, average reward: -87.11\n",
      "Episode: 297, total numsteps: 229058, reward: -101.99, average reward: -87.38\n",
      "Episode: 298, total numsteps: 229104, reward: -102.63, average reward: -87.65\n",
      "Episode: 299, total numsteps: 229153, reward: -101.65, average reward: -87.94\n",
      "Episode: 300, total numsteps: 229198, reward: -102.72, average reward: -88.16\n",
      "Episode: 301, total numsteps: 229263, reward: -98.76, average reward: -88.39\n",
      "Episode: 302, total numsteps: 229318, reward: -100.93, average reward: -88.65\n",
      "Episode: 303, total numsteps: 229368, reward: -101.64, average reward: -88.89\n",
      "Episode: 304, total numsteps: 229421, reward: -105.99, average reward: -89.17\n",
      "Episode: 305, total numsteps: 229468, reward: -103.18, average reward: -89.35\n",
      "Episode: 306, total numsteps: 229522, reward: -101.14, average reward: -89.56\n",
      "Episode: 307, total numsteps: 229577, reward: -100.47, average reward: -89.86\n",
      "Episode: 308, total numsteps: 229635, reward: -100.65, average reward: -90.12\n",
      "Episode: 309, total numsteps: 229703, reward: -100.47, average reward: -90.35\n",
      "Episode: 310, total numsteps: 229755, reward: -101.33, average reward: -90.67\n",
      "Episode: 311, total numsteps: 229805, reward: -102.44, average reward: -90.92\n",
      "Episode: 312, total numsteps: 229852, reward: -103.65, average reward: -91.22\n",
      "Episode: 313, total numsteps: 229901, reward: -102.16, average reward: -91.42\n",
      "Episode: 314, total numsteps: 229950, reward: -102.78, average reward: -91.74\n",
      "Episode: 315, total numsteps: 230000, reward: -101.98, average reward: -91.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 316, total numsteps: 230051, reward: -103.03, average reward: -92.21\n",
      "Episode: 317, total numsteps: 230096, reward: -102.11, average reward: -92.48\n",
      "Episode: 318, total numsteps: 230145, reward: -101.31, average reward: -92.74\n",
      "Episode: 319, total numsteps: 230196, reward: -101.28, average reward: -93.02\n",
      "Episode: 320, total numsteps: 230247, reward: -101.5, average reward: -92.87\n",
      "Episode: 321, total numsteps: 230292, reward: -101.2, average reward: -93.08\n",
      "Episode: 322, total numsteps: 230347, reward: -100.02, average reward: -93.31\n",
      "Episode: 323, total numsteps: 230404, reward: -99.03, average reward: -93.58\n",
      "Episode: 324, total numsteps: 230460, reward: -99.54, average reward: -93.83\n",
      "Episode: 325, total numsteps: 230511, reward: -102.94, average reward: -94.06\n",
      "Episode: 326, total numsteps: 230564, reward: -101.17, average reward: -94.34\n",
      "Episode: 327, total numsteps: 230638, reward: -96.22, average reward: -94.13\n",
      "Episode: 328, total numsteps: 230698, reward: -99.04, average reward: -94.37\n",
      "Episode: 329, total numsteps: 230760, reward: -100.12, average reward: -94.19\n",
      "Episode: 330, total numsteps: 230821, reward: -101.58, average reward: -94.46\n",
      "Episode: 331, total numsteps: 230887, reward: -105.52, average reward: -94.72\n",
      "Episode: 332, total numsteps: 230957, reward: -100.75, average reward: -94.97\n",
      "Episode: 333, total numsteps: 231012, reward: -100.96, average reward: -95.19\n",
      "Episode: 334, total numsteps: 231070, reward: -102.94, average reward: -95.41\n",
      "Episode: 335, total numsteps: 231133, reward: -101.04, average reward: -95.61\n",
      "Episode: 336, total numsteps: 231194, reward: -100.24, average reward: -95.78\n",
      "Episode: 337, total numsteps: 231251, reward: -100.98, average reward: -95.96\n",
      "Episode: 338, total numsteps: 231334, reward: -104.2, average reward: -96.31\n",
      "Episode: 339, total numsteps: 231406, reward: -106.41, average reward: -96.61\n",
      "Episode: 340, total numsteps: 231467, reward: -102.24, average reward: -96.82\n",
      "Episode: 341, total numsteps: 231521, reward: -102.13, average reward: -97.02\n",
      "Episode: 342, total numsteps: 231585, reward: -102.24, average reward: -97.28\n",
      "Episode: 343, total numsteps: 231641, reward: -103.24, average reward: -97.52\n",
      "Episode: 344, total numsteps: 231709, reward: -103.43, average reward: -97.81\n",
      "Episode: 345, total numsteps: 231772, reward: -103.73, average reward: -98.11\n",
      "Episode: 346, total numsteps: 231829, reward: -103.56, average reward: -98.3\n",
      "Episode: 347, total numsteps: 231886, reward: -107.26, average reward: -98.56\n",
      "Episode: 348, total numsteps: 231957, reward: -104.13, average reward: -98.8\n",
      "Episode: 349, total numsteps: 232019, reward: -106.08, average reward: -99.07\n",
      "Episode: 350, total numsteps: 232079, reward: -106.78, average reward: -99.38\n",
      "Episode: 351, total numsteps: 232145, reward: -102.55, average reward: -99.68\n",
      "Episode: 352, total numsteps: 232197, reward: -105.04, average reward: -99.99\n",
      "Episode: 353, total numsteps: 232244, reward: -103.54, average reward: -100.28\n",
      "Episode: 354, total numsteps: 232310, reward: -104.29, average reward: -100.64\n",
      "Episode: 355, total numsteps: 232370, reward: -104.53, average reward: -100.97\n",
      "Episode: 356, total numsteps: 232428, reward: -104.61, average reward: -101.24\n",
      "Episode: 357, total numsteps: 232500, reward: -103.83, average reward: -101.48\n",
      "Episode: 358, total numsteps: 232571, reward: -106.74, average reward: -101.79\n",
      "Episode: 359, total numsteps: 232638, reward: -108.79, average reward: -102.1\n",
      "Episode: 360, total numsteps: 232693, reward: -104.95, average reward: -102.4\n",
      "Episode: 361, total numsteps: 232749, reward: -105.7, average reward: -102.62\n",
      "Episode: 362, total numsteps: 232812, reward: -105.99, average reward: -102.69\n",
      "Episode: 363, total numsteps: 232866, reward: -107.26, average reward: -102.29\n",
      "Episode: 364, total numsteps: 232943, reward: -107.59, average reward: -102.33\n",
      "Episode: 365, total numsteps: 233004, reward: -106.49, average reward: -102.4\n",
      "Episode: 366, total numsteps: 233071, reward: -105.19, average reward: -102.43\n",
      "Episode: 367, total numsteps: 233130, reward: -103.32, average reward: -102.43\n",
      "Episode: 368, total numsteps: 233188, reward: -106.62, average reward: -102.5\n",
      "Episode: 369, total numsteps: 233241, reward: -103.86, average reward: -102.54\n",
      "Episode: 370, total numsteps: 233307, reward: -101.16, average reward: -102.55\n",
      "Episode: 371, total numsteps: 233368, reward: -103.43, average reward: -102.39\n",
      "Episode: 372, total numsteps: 233459, reward: -104.74, average reward: -102.53\n",
      "Episode: 373, total numsteps: 233529, reward: -103.77, average reward: -102.54\n",
      "Episode: 374, total numsteps: 233646, reward: -104.81, average reward: -102.57\n",
      "Episode: 375, total numsteps: 233703, reward: -104.51, average reward: -102.58\n",
      "Episode: 376, total numsteps: 233800, reward: -104.31, average reward: -102.59\n",
      "Episode: 377, total numsteps: 233870, reward: -103.85, average reward: -102.61\n",
      "Episode: 378, total numsteps: 233981, reward: -104.62, average reward: -102.66\n",
      "Episode: 379, total numsteps: 234045, reward: -101.17, average reward: -102.83\n",
      "Episode: 380, total numsteps: 234103, reward: -106.58, average reward: -102.89\n",
      "Episode: 381, total numsteps: 234179, reward: -103.6, average reward: -102.91\n",
      "Episode: 382, total numsteps: 234235, reward: -104.06, average reward: -102.94\n",
      "Episode: 383, total numsteps: 234315, reward: -102.52, average reward: -102.93\n",
      "Episode: 384, total numsteps: 234386, reward: -103.16, average reward: -102.94\n",
      "Episode: 385, total numsteps: 234478, reward: -103.41, average reward: -102.95\n",
      "Episode: 386, total numsteps: 234606, reward: -104.79, average reward: -102.97\n",
      "Episode: 387, total numsteps: 234681, reward: -103.08, average reward: -102.99\n",
      "Episode: 388, total numsteps: 234760, reward: -104.8, average reward: -103.01\n",
      "Episode: 389, total numsteps: 234832, reward: -100.92, average reward: -103.0\n",
      "Episode: 390, total numsteps: 234903, reward: -105.29, average reward: -103.04\n",
      "Episode: 391, total numsteps: 234965, reward: -106.57, average reward: -103.09\n",
      "Episode: 392, total numsteps: 235028, reward: -101.75, average reward: -103.09\n",
      "Episode: 393, total numsteps: 235120, reward: -101.72, average reward: -103.1\n",
      "Episode: 394, total numsteps: 235164, reward: -103.0, average reward: -103.11\n",
      "Episode: 395, total numsteps: 235214, reward: -101.53, average reward: -103.1\n",
      "Episode: 396, total numsteps: 235278, reward: -105.8, average reward: -103.14\n",
      "Episode: 397, total numsteps: 235379, reward: -103.29, average reward: -103.16\n",
      "Episode: 398, total numsteps: 235460, reward: -99.13, average reward: -103.12\n",
      "Episode: 399, total numsteps: 235529, reward: -102.4, average reward: -103.13\n",
      "Episode: 400, total numsteps: 235713, reward: -135.41, average reward: -103.46\n",
      "Episode: 401, total numsteps: 235781, reward: -106.16, average reward: -103.53\n",
      "Episode: 402, total numsteps: 235851, reward: -101.86, average reward: -103.54\n",
      "Episode: 403, total numsteps: 235935, reward: -102.94, average reward: -103.55\n",
      "Episode: 404, total numsteps: 236016, reward: -103.25, average reward: -103.52\n",
      "Episode: 405, total numsteps: 236089, reward: -101.5, average reward: -103.51\n",
      "Episode: 406, total numsteps: 236191, reward: -98.48, average reward: -103.48\n",
      "Episode: 407, total numsteps: 236247, reward: -100.96, average reward: -103.49\n",
      "Episode: 408, total numsteps: 236409, reward: -102.14, average reward: -103.5\n",
      "Episode: 409, total numsteps: 236529, reward: -100.31, average reward: -103.5\n",
      "Episode: 410, total numsteps: 236597, reward: -100.27, average reward: -103.49\n",
      "Episode: 411, total numsteps: 236730, reward: -98.74, average reward: -103.45\n",
      "Episode: 412, total numsteps: 236818, reward: -101.82, average reward: -103.43\n",
      "Episode: 413, total numsteps: 237062, reward: -109.7, average reward: -103.51\n",
      "Episode: 414, total numsteps: 237199, reward: -99.6, average reward: -103.48\n",
      "Episode: 415, total numsteps: 237295, reward: -101.35, average reward: -103.47\n",
      "Episode: 416, total numsteps: 237354, reward: -100.74, average reward: -103.45\n",
      "Episode: 417, total numsteps: 238954, reward: -70.41, average reward: -103.13\n",
      "Episode: 418, total numsteps: 240554, reward: -76.22, average reward: -102.88\n",
      "Episode: 419, total numsteps: 240670, reward: -107.82, average reward: -102.95\n",
      "Episode: 420, total numsteps: 240773, reward: -110.84, average reward: -103.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 421, total numsteps: 242373, reward: -86.27, average reward: -102.89\n",
      "Episode: 422, total numsteps: 243973, reward: -77.1, average reward: -102.66\n",
      "Episode: 423, total numsteps: 245573, reward: -82.87, average reward: -102.5\n",
      "Episode: 424, total numsteps: 247173, reward: -82.55, average reward: -102.33\n",
      "Episode: 425, total numsteps: 248773, reward: -71.01, average reward: -102.01\n",
      "Episode: 426, total numsteps: 250373, reward: -83.18, average reward: -101.83\n",
      "Episode: 427, total numsteps: 251973, reward: -80.06, average reward: -101.67\n",
      "Episode: 428, total numsteps: 253573, reward: -73.35, average reward: -101.41\n",
      "Episode: 429, total numsteps: 255173, reward: -81.42, average reward: -101.22\n",
      "Episode: 430, total numsteps: 256773, reward: -79.68, average reward: -101.01\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import math\n",
    "import gym\n",
    "import numpy as np\n",
    "import itertools\n",
    "import torch\n",
    "from sac import SAC\n",
    "from tensorboardX import SummaryWriter\n",
    "from normalized_actions import NormalizedActions\n",
    "from replay_memory import ReplayMemory\n",
    "\n",
    "'''\n",
    "parser = argparse.ArgumentParser(description='PyTorch REINFORCE example')\n",
    "parser.add_argument('--env-name', default=\"BipedalWalker-v2\",\n",
    "                    help='name of the environment to run')\n",
    "parser.add_argument('--policy', default=\"Gaussian\",\n",
    "                    help='algorithm to use: Gaussian | Deterministic')\n",
    "parser.add_argument('--eval', type=bool, default=False,\n",
    "                    help='Evaluate a policy (default:False)')\n",
    "parser.add_argument('--gamma', type=float, default=0.99, metavar='G',\n",
    "                    help='discount factor for reward (default: 0.99)')\n",
    "parser.add_argument('--tau', type=float, default=0.005, metavar='G',\n",
    "                    help='target smoothing coefficient(τ) (default: 0.005)')\n",
    "parser.add_argument('--lr', type=float, default=0.0003, metavar='G',\n",
    "                    help='learning rate (default: 0.0003)')\n",
    "parser.add_argument('--alpha', type=float, default=0.2, metavar='G',\n",
    "                    help='Temperature parameter α determines the relative importance of the entropy term against the reward (default: 0.2)')\n",
    "parser.add_argument('--seed', type=int, default=543, metavar='N',\n",
    "                    help='random seed (default: 543)')\n",
    "parser.add_argument('--batch_size', type=int, default=256, metavar='N',\n",
    "                    help='batch size (default: 256)')\n",
    "parser.add_argument('--num_steps', type=int, default=1000000, metavar='N',\n",
    "                    help='maximum number of steps (default: 1000000)')\n",
    "parser.add_argument('--hidden_size', type=int, default=256, metavar='N',\n",
    "                    help='hidden size (default: 256)')\n",
    "parser.add_argument('--updates_per_step', type=int, default=1, metavar='N',\n",
    "                    help='model updates per simulator step (default: 1)')\n",
    "parser.add_argument('--target_update_interval', type=int, default=1, metavar='N',\n",
    "                    help='Value target update per no. of updates per step (default: 1)')\n",
    "parser.add_argument('--replay_size', type=int, default=1000000, metavar='N',\n",
    "                    help='size of replay buffer (default: 10000000)')\n",
    "args = parser.parse_args()\n",
    "'''\n",
    "\n",
    "args = {\n",
    "    \"env_name\": \"BipedalWalker-v2\",\n",
    "    \"policy\": \"Gaussian\",\n",
    "    \"eval\": False,\n",
    "    \"gamma\": 0.99,\n",
    "    \"tau\": 0.005,\n",
    "    \"lr\": 0.0003,\n",
    "    \"alpha\": 0.2,\n",
    "    \"seed\": 543,\n",
    "    \"batch_size\": 256,\n",
    "    \"num_steps\": 1000000,\n",
    "    \"hidden_size\": 256,\n",
    "    \"updates_per_step\": 1,\n",
    "    \"target_update_interval\": 1,\n",
    "    \"replay_size\": 1000000\n",
    "}    \n",
    "\n",
    "\n",
    "# Environment\n",
    "env = NormalizedActions(gym.make(args['env_name']))\n",
    "env.seed(args['seed'])\n",
    "torch.manual_seed(args['seed'])\n",
    "np.random.seed(args['seed'])\n",
    "\n",
    "# Agent\n",
    "agent = SAC(env.observation_space.shape[0], env.action_space, args)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Memory\n",
    "memory = ReplayMemory(args['replay_size'])\n",
    "\n",
    "# Training Loop\n",
    "rewards = []\n",
    "total_numsteps = 0\n",
    "updates = 0\n",
    "\n",
    "for i_episode in itertools.count():\n",
    "    state = env.reset()\n",
    "\n",
    "    episode_reward = 0\n",
    "    while True:\n",
    "        action = agent.select_action(state)  # Sample action from policy\n",
    "        next_state, reward, done, _ = env.step(action)  # Step\n",
    "        mask = not done  # 1 for not done and 0 for done\n",
    "        memory.push(state, action, reward, next_state, mask)  # Append transition to memory\n",
    "        if len(memory) > args['batch_size']:\n",
    "            for i in range(args['updates_per_step']): # Number of updates per step in environment\n",
    "                # Sample a batch from memory\n",
    "                state_batch, action_batch, reward_batch, next_state_batch, mask_batch = memory.sample(args['batch_size'])\n",
    "                # Update parameters of all the networks\n",
    "                value_loss, critic_1_loss, critic_2_loss, policy_loss = agent.update_parameters(state_batch, action_batch, \n",
    "                                                                                                reward_batch, next_state_batch, \n",
    "                                                                                                mask_batch, updates)\n",
    "\n",
    "                writer.add_scalar('loss/value', value_loss, updates)\n",
    "                writer.add_scalar('loss/critic_1', critic_1_loss, updates)\n",
    "                writer.add_scalar('loss/critic_2', critic_2_loss, updates)\n",
    "                writer.add_scalar('loss/policy', policy_loss, updates)\n",
    "                updates += 1\n",
    "\n",
    "        state = next_state\n",
    "        total_numsteps += 1\n",
    "        episode_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if total_numsteps > args['num_steps']:\n",
    "        break\n",
    "\n",
    "    writer.add_scalar('reward/train', episode_reward, i_episode)\n",
    "    rewards.append(episode_reward)\n",
    "    print(\"Episode: {}, total numsteps: {}, reward: {}, average reward: {}\".format(i_episode, total_numsteps, np.round(rewards[-1],2),\n",
    "                                                                                np.round(np.mean(rewards[-100:]),2)))\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
