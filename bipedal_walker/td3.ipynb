{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import gym\n",
    "import numpy as np\n",
    "from gym import wrappers\n",
    "from PIL import Image\n",
    "\n",
    "from TD3.td3 import TD3\n",
    "from TD3.utils import PrioritizedReplayBuffer, mkdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'BipedalWalker-v2'\n",
    "lr_base = 0.0001\n",
    "lr_decay = 0.0001\n",
    "exp_noise_base = 0.1\n",
    "exp_noise_decay = 0.0001\n",
    "\n",
    "random_seed = 42\n",
    "gamma = 0.99                # discount for future rewards\n",
    "batch_size = 128        # num of transitions sampled from replay buffer\n",
    "polyak = 0.995              # target policy update parameter (1-tau)\n",
    "policy_noise = 0.2          # target policy smoothing noise\n",
    "noise_clip = 0.5\n",
    "policy_delay = 2            # delayed policy updates parameter\n",
    "max_episodes = 10000         # max num of episodes\n",
    "max_timesteps = 2000        # max timesteps in one episode\n",
    "max_buffer_length = 5000000\n",
    "log_interval = 10           # print avg reward after interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_config = [\n",
    "        {'dim': [None, 64], 'dropout': False, 'activation': 'relu'},\n",
    "        {'dim': [64, 64], 'dropout': False, 'activation':'relu'},\n",
    "        {'dim': [64, None], 'dropout': False, 'activation': 'tanh'}\n",
    "    ]\n",
    "    \n",
    "critic_config = [\n",
    "        {'dim': [None, 64], 'dropout': False, 'activation': 'relu'},\n",
    "        {'dim': [64, 64], 'dropout': False , 'activation':'relu'},\n",
    "        {'dim': [64, 1], 'dropout': False, 'activation': False}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3Trainer():\n",
    "    \n",
    "    def __init__(self, env_name, actor_config, critic_config, random_seed=42, lr_base=0.001, lr_decay=0.00005, \n",
    "                 exp_noise_base=0.3, exp_noise_decay=0.0001, gamma=0.99, batch_size=1024, \n",
    "                 polyak=0.9999, policy_noise=0.2, noise_clip=0.5, policy_delay=2, \n",
    "                 max_episodes=100000, max_timesteps=3000, max_buffer_length=5000000, \n",
    "                 log_interval=5, threshold=None, lr_minimum=1e-10, exp_noise_minimum=1e-10,\n",
    "                 record_videos=True, record_interval=100,  beta_multiplier=0.0001):        \n",
    "        \n",
    "        self.algorithm_name = 'td3'\n",
    "        self.env_name = env_name\n",
    "        self.env = gym.make(env_name)\n",
    "        self.record_videos = record_videos\n",
    "        self.record_interval = record_interval        \n",
    "        if self.record_videos == True:\n",
    "            videos_dir = mkdir('.', 'videos')\n",
    "            monitor_dir = mkdir(videos_dir, self.algorithm_name)\n",
    "            should_record = lambda i: self.should_record\n",
    "            self.env = wrappers.Monitor(self.env, monitor_dir, video_callable=should_record, force=True)            \n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.shape[0]\n",
    "        self.action_low = self.env.action_space.low\n",
    "        self.action_high = self.env.action_space.high        \n",
    "        self.should_record = False\n",
    "        if not threshold == None:\n",
    "            self.threshold = threshold\n",
    "        else:    \n",
    "            self.threshold = self.env.spec.reward_threshold\n",
    "        \n",
    "        self.actor_config = actor_config\n",
    "        self.critic_config = critic_config\n",
    "        self.actor_config[0]['dim'][0] = self.state_dim\n",
    "        self.actor_config[-1]['dim'][1] = self.action_dim\n",
    "        self.critic_config[0]['dim'][0] = self.state_dim + self.action_dim\n",
    "        \n",
    "        self.actor_config = actor_config\n",
    "        self.critic_config = critic_config\n",
    "        self.random_seed = random_seed\n",
    "        self.lr_base = lr_base\n",
    "        self.lr_decay = lr_decay   \n",
    "        self.lr_minimum = lr_minimum\n",
    "        self.exp_noise_base = exp_noise_base\n",
    "        self.exp_noise_decay = exp_noise_decay     \n",
    "        self.exp_noise_minimum = exp_noise_minimum                \n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size        \n",
    "        self.polyak = polyak\n",
    "        self.beta_multiplier = beta_multiplier\n",
    "        self.policy_noise = policy_noise\n",
    "        self.noise_clip = noise_clip\n",
    "        self.policy_delay = policy_delay\n",
    "        self.max_episodes = max_episodes\n",
    "        self.max_timesteps = max_timesteps\n",
    "        self.max_buffer_length = max_buffer_length\n",
    "        self.log_interval = log_interval\n",
    "        \n",
    "        prdir = mkdir('.', 'preTrained')\n",
    "        self.directory = mkdir(prdir, self.algorithm_name)\n",
    "        self.filename = \"{}_{}_{}\".format(self.algorithm_name, self.env_name, self.random_seed)\n",
    "                \n",
    "        self.policy = TD3(self.actor_config, self.critic_config, self.action_low, self.action_high)   \n",
    "        self.replay_buffer = PrioritizedReplayBuffer(size=self.max_buffer_length, alpha=0.8)\n",
    "        \n",
    "        self.reward_history = []\n",
    "        self.make_plots = False       \n",
    "        \n",
    "        if self.random_seed:\n",
    "            print(\"Random Seed: {}\".format(self.random_seed))\n",
    "            self.env.seed(self.random_seed)\n",
    "            torch.manual_seed(self.random_seed)\n",
    "            np.random.seed(self.random_seed)\n",
    "        \n",
    "    def train(self):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        print(\"Training started ... \\n\")\n",
    "        print(\"action_space={}\".format(self.env.action_space))\n",
    "        print(\"obs_space={}\".format(self.env.observation_space))\n",
    "        print(\"threshold={}\".format(self.threshold))     \n",
    "        print(\"action_low={} action_high={} \\n\".format(self.action_low, self.action_high))         \n",
    "\n",
    "        # loading models\n",
    "        self.policy.load(self.directory, self.filename)\n",
    "                \n",
    "        # logging variables:        \n",
    "        log_f = open(\"train_{}.txt\".format(self.algorithm_name), \"w+\")\n",
    "\n",
    "        # training procedure:\n",
    "        for episode in range(1, self.max_episodes+1):\n",
    "            \n",
    "            # Only record video during evaluation, every n steps\n",
    "            if episode % self.record_interval == 0:\n",
    "                self.should_record = True\n",
    "            \n",
    "            ep_reward = 0.0\n",
    "            state = self.env.reset()\n",
    "            \n",
    "            # calculate params\n",
    "            exploration_noise = max(self.exp_noise_base / (1.0 + episode * self.exp_noise_decay), self.exp_noise_minimum)\n",
    "            learning_rate = max(self.lr_base / (1.0 + episode * self.lr_decay), self.lr_minimum)     \n",
    "            beta = min(episode * self.beta_multiplier, 1)\n",
    "            self.policy.set_optimizers(lr=learning_rate)\n",
    "\n",
    "            for t in range(self.max_timesteps):\n",
    "                \n",
    "                # select action and add exploration noise:\n",
    "                action = self.policy.select_action(state)               \n",
    "                action = action + np.random.normal(0, exploration_noise, size=self.action_dim)\n",
    "                action = action.clip(self.action_low, self.action_high)\n",
    "\n",
    "                # take action in env:\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                self.replay_buffer.add(state, action, reward, next_state, float(done))\n",
    "                state = next_state\n",
    "\n",
    "                ep_reward += reward\n",
    "\n",
    "                # if episode is done then update policy:\n",
    "                if done or t==(self.max_timesteps-1):\n",
    "                    self.policy.update(self.replay_buffer, t, self.batch_size, self.gamma, self.polyak, \n",
    "                                       self.policy_noise, self.noise_clip, self.policy_delay, beta)\n",
    "                    break\n",
    "\n",
    "            self.reward_history.append(ep_reward)\n",
    "            avg_reward = np.mean(self.reward_history[-100:]) \n",
    "\n",
    "            # logging updates:        \n",
    "            log_f.write('{},{}\\n'.format(episode, ep_reward))\n",
    "            log_f.flush()\n",
    "            \n",
    "            # Calculate polyak\n",
    "            #part = (env.spec.reward_threshold - avg_reward) / (env.spec.reward_threshold + 150)\n",
    "            #if part > 1:\n",
    "            #    part = 1\n",
    "            #polyak = polyak_int[0] + (1 - part) * (polyak_int[1] - polyak_int[0])     \n",
    "\n",
    "            # Calculate LR\n",
    "            #part = min((env.spec.reward_threshold - avg_reward) / (env.spec.reward_threshold + 150), 1)\n",
    "                        \n",
    "            avg_actor_loss = np.mean(self.policy.actor_loss_list[-100:])\n",
    "            avg_Q1_loss = np.mean(self.policy.Q1_loss_list[-100:])\n",
    "            avg_Q2_loss = np.mean(self.policy.Q2_loss_list[-100:])\n",
    "\n",
    "            # Truncate training history if we don't plan to plot it later\n",
    "            if not self.make_plots:\n",
    "                self.policy.truncate_loss_lists() \n",
    "                if len(self.reward_history) > 100:\n",
    "                    self.reward_history.pop(0)    \n",
    "\n",
    "            # Print avg reward every log interval:\n",
    "            if episode % self.log_interval == 0:            \n",
    "                self.policy.save(self.directory, self.filename)\n",
    "                print(\"Ep:{}   Rew:{:3.2f}  Avg Rew:{:3.2f}  LR:{:8.8f}   Polyak:{:5.5f}  Bf:{:2.0f} {:0.4f}  EN:{:0.4f}  Loss: {:5.3f} {:5.3f} {:5.3f}\".format(\n",
    "                    episode, ep_reward, avg_reward, learning_rate, self.polyak, self.replay_buffer.get_fill(), beta, \n",
    "                    exploration_noise, avg_actor_loss, avg_Q1_loss, avg_Q2_loss))\n",
    "                \n",
    "            self.should_record = False    \n",
    "                \n",
    "            # if avg reward > threshold then save and stop traning:\n",
    "            if avg_reward >= self.threshold and episode > 100: \n",
    "                print(\"Ep:{}   Rew:{:3.2f}  Avg Rew:{:3.2f}  LR:{:8.8f}   Polyak:{:5.5f}  Bf:{:2.0f} {:0.4f}  EN:{:0.4f}  Loss: {:5.3f} {:5.3f} {:5.3f}\".format(\n",
    "                    episode, ep_reward, avg_reward, learning_rate, self.polyak, self.replay_buffer.get_fill(), beta,\n",
    "                    exploration_noise, avg_actor_loss, avg_Q1_loss, avg_Q2_loss))\n",
    "                print(\"########## Solved! ###########\")\n",
    "                name = self.filename + '_solved'\n",
    "                self.policy.save(self.directory, name)\n",
    "                log_f.close()\n",
    "                training_time = time.time() - start_time\n",
    "                print(\"Training time: {:6.2f} sec\".format(training_time))\n",
    "                break    \n",
    "       \n",
    "    def test(self, episodes=3, render=True, save_gif=True):   \n",
    "        \n",
    "        gifdir = mkdir('.','gif')\n",
    "        algdir = mkdir(gifdir, self.algorithm_name)\n",
    "\n",
    "        for episode in range(1, episodes+1):\n",
    "            ep_reward = 0.0\n",
    "            state = self.env.reset()\n",
    "            epdir = mkdir(algdir, str(episode))\n",
    "            \n",
    "            for t in range(self.max_timesteps):\n",
    "                action = self.policy.select_action(state)\n",
    "                state, reward, done, _ = self.env.step(action)\n",
    "                ep_reward += reward\n",
    "                \n",
    "                if save_gif:                                       \n",
    "                    img = self.env.render(mode = 'rgb_array')\n",
    "                    img = Image.fromarray(img)\n",
    "                    img.save('{}/{}.jpg'.format(epdir, t))\n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "\n",
    "            print('Test episode: {}\\tReward: {:4.2f}'.format(episode, ep_reward))           \n",
    "            self.env.close()        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTOR=Sequential(\n",
      "  (0): Linear(in_features=24, out_features=64, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=4, bias=True)\n",
      "  (5): Tanh()\n",
      ")\n",
      "ACTOR=Sequential(\n",
      "  (0): Linear(in_features=24, out_features=64, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=4, bias=True)\n",
      "  (5): Tanh()\n",
      ")\n",
      "CRITIC=Sequential(\n",
      "  (0): Linear(in_features=28, out_features=64, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "CRITIC=Sequential(\n",
      "  (0): Linear(in_features=28, out_features=64, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "CRITIC=Sequential(\n",
      "  (0): Linear(in_features=28, out_features=64, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "CRITIC=Sequential(\n",
      "  (0): Linear(in_features=28, out_features=64, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "Random Seed: 42\n",
      "Training started ... \n",
      "\n",
      "action_space=Box(4,)\n",
      "obs_space=Box(24,)\n",
      "threshold=300\n",
      "action_low=[-1. -1. -1. -1.] action_high=[1. 1. 1. 1.] \n",
      "\n",
      "DIR=./preTrained/td3 NAME=td3_BipedalWalker-v2_42\n",
      "No models to load\n",
      "Ep:10   Rew:-119.70  Avg Rew:-129.84  LR:0.00009990   Polyak:0.99500  Bf: 0 0.0010  EN:0.0999  Loss: -1.609 1.556 1.587\n",
      "Ep:20   Rew:-116.98  Avg Rew:-123.92  LR:0.00009980   Polyak:0.99500  Bf: 0 0.0020  EN:0.0998  Loss: -1.890 6.594 6.494\n",
      "Ep:30   Rew:-122.82  Avg Rew:-120.93  LR:0.00009970   Polyak:0.99500  Bf: 0 0.0030  EN:0.0997  Loss: -2.203 10.869 10.796\n",
      "Ep:40   Rew:-118.73  Avg Rew:-119.95  LR:0.00009960   Polyak:0.99500  Bf: 0 0.0040  EN:0.0996  Loss: -2.681 17.127 17.412\n",
      "Ep:50   Rew:-115.68  Avg Rew:-119.20  LR:0.00009950   Polyak:0.99500  Bf: 0 0.0050  EN:0.0995  Loss: -2.989 17.697 18.340\n",
      "Ep:60   Rew:-113.70  Avg Rew:-119.02  LR:0.00009940   Polyak:0.99500  Bf: 0 0.0060  EN:0.0994  Loss: -3.160 17.054 17.262\n",
      "Ep:70   Rew:-125.31  Avg Rew:-117.65  LR:0.00009930   Polyak:0.99500  Bf: 0 0.0070  EN:0.0993  Loss: -3.009 16.259 16.139\n",
      "Ep:80   Rew:-113.87  Avg Rew:-116.65  LR:0.00009921   Polyak:0.99500  Bf: 0 0.0080  EN:0.0992  Loss: -1.714 12.653 12.524\n",
      "Ep:90   Rew:-114.27  Avg Rew:-116.16  LR:0.00009911   Polyak:0.99500  Bf: 0 0.0090  EN:0.0991  Loss: -0.682 8.406 8.462\n",
      "Ep:100   Rew:-115.63  Avg Rew:-115.83  LR:0.00009901   Polyak:0.99500  Bf: 0 0.0100  EN:0.0990  Loss: 0.788 7.848 7.756\n",
      "Ep:110   Rew:-109.57  Avg Rew:-114.01  LR:0.00009891   Polyak:0.99500  Bf: 0 0.0110  EN:0.0989  Loss: 1.642 7.977 8.074\n",
      "Ep:120   Rew:-113.53  Avg Rew:-113.63  LR:0.00009881   Polyak:0.99500  Bf: 0 0.0120  EN:0.0988  Loss: 2.988 6.070 6.300\n",
      "Ep:130   Rew:-101.36  Avg Rew:-113.03  LR:0.00009872   Polyak:0.99500  Bf: 1 0.0130  EN:0.0987  Loss: 12.183 2.925 3.030\n",
      "Ep:140   Rew:-109.42  Avg Rew:-112.40  LR:0.00009862   Polyak:0.99500  Bf: 1 0.0140  EN:0.0986  Loss: 16.720 3.357 3.429\n",
      "Ep:150   Rew:-153.74  Avg Rew:-112.20  LR:0.00009852   Polyak:0.99500  Bf: 1 0.0150  EN:0.0985  Loss: 17.514 2.879 2.951\n",
      "Ep:160   Rew:-106.88  Avg Rew:-111.93  LR:0.00009843   Polyak:0.99500  Bf: 1 0.0160  EN:0.0984  Loss: 19.225 3.251 3.335\n",
      "Ep:170   Rew:-141.05  Avg Rew:-115.14  LR:0.00009833   Polyak:0.99500  Bf: 1 0.0170  EN:0.0983  Loss: 20.885 4.841 4.759\n",
      "Ep:180   Rew:-148.32  Avg Rew:-117.29  LR:0.00009823   Polyak:0.99500  Bf: 1 0.0180  EN:0.0982  Loss: 20.989 3.962 3.894\n",
      "Ep:190   Rew:-109.08  Avg Rew:-118.17  LR:0.00009814   Polyak:0.99500  Bf: 1 0.0190  EN:0.0981  Loss: 18.499 2.206 2.240\n",
      "Ep:200   Rew:-126.52  Avg Rew:-118.77  LR:0.00009804   Polyak:0.99500  Bf: 1 0.0200  EN:0.0980  Loss: 19.436 4.307 4.322\n",
      "Ep:210   Rew:-129.00  Avg Rew:-120.53  LR:0.00009794   Polyak:0.99500  Bf: 1 0.0210  EN:0.0979  Loss: 19.933 3.994 4.029\n",
      "Ep:220   Rew:-129.86  Avg Rew:-122.22  LR:0.00009785   Polyak:0.99500  Bf: 1 0.0220  EN:0.0978  Loss: 20.244 6.234 6.257\n",
      "Ep:230   Rew:-145.03  Avg Rew:-125.11  LR:0.00009775   Polyak:0.99500  Bf: 2 0.0230  EN:0.0978  Loss: 20.947 2.713 2.684\n",
      "Ep:240   Rew:-139.15  Avg Rew:-128.07  LR:0.00009766   Polyak:0.99500  Bf: 2 0.0240  EN:0.0977  Loss: 21.431 2.285 2.371\n",
      "Ep:250   Rew:-133.39  Avg Rew:-129.64  LR:0.00009756   Polyak:0.99500  Bf: 2 0.0250  EN:0.0976  Loss: 21.961 2.200 2.272\n",
      "Ep:260   Rew:-127.29  Avg Rew:-130.24  LR:0.00009747   Polyak:0.99500  Bf: 2 0.0260  EN:0.0975  Loss: 22.922 2.081 2.087\n",
      "Ep:270   Rew:-123.36  Avg Rew:-128.10  LR:0.00009737   Polyak:0.99500  Bf: 2 0.0270  EN:0.0974  Loss: 23.585 1.960 1.837\n",
      "Ep:280   Rew:-144.46  Avg Rew:-127.28  LR:0.00009728   Polyak:0.99500  Bf: 2 0.0280  EN:0.0973  Loss: 26.158 1.688 1.646\n",
      "Ep:290   Rew:-116.17  Avg Rew:-126.89  LR:0.00009718   Polyak:0.99500  Bf: 2 0.0290  EN:0.0972  Loss: 22.329 1.623 1.543\n",
      "Ep:300   Rew:-111.52  Avg Rew:-126.53  LR:0.00009709   Polyak:0.99500  Bf: 2 0.0300  EN:0.0971  Loss: 17.467 1.258 1.213\n",
      "Ep:310   Rew:-127.60  Avg Rew:-125.74  LR:0.00009699   Polyak:0.99500  Bf: 3 0.0310  EN:0.0970  Loss: 14.623 1.366 1.339\n",
      "Ep:320   Rew:-122.50  Avg Rew:-124.14  LR:0.00009690   Polyak:0.99500  Bf: 3 0.0320  EN:0.0969  Loss: 16.593 1.330 1.221\n",
      "Ep:330   Rew:-135.11  Avg Rew:-123.96  LR:0.00009681   Polyak:0.99500  Bf: 3 0.0330  EN:0.0968  Loss: 18.639 1.357 1.290\n",
      "Ep:340   Rew:-113.47  Avg Rew:-121.27  LR:0.00009671   Polyak:0.99500  Bf: 4 0.0340  EN:0.0967  Loss: 20.405 1.062 1.029\n",
      "Ep:350   Rew:-114.66  Avg Rew:-119.61  LR:0.00009662   Polyak:0.99500  Bf: 4 0.0350  EN:0.0966  Loss: 20.095 1.217 1.165\n",
      "Ep:360   Rew:-105.93  Avg Rew:-118.57  LR:0.00009653   Polyak:0.99500  Bf: 4 0.0360  EN:0.0965  Loss: 19.680 1.171 1.108\n",
      "Ep:370   Rew:-84.85  Avg Rew:-116.17  LR:0.00009643   Polyak:0.99500  Bf: 4 0.0370  EN:0.0964  Loss: 20.711 1.109 1.082\n",
      "Ep:380   Rew:-114.39  Avg Rew:-113.79  LR:0.00009634   Polyak:0.99500  Bf: 5 0.0380  EN:0.0963  Loss: 18.111 1.137 1.119\n",
      "Ep:390   Rew:-84.32  Avg Rew:-112.76  LR:0.00009625   Polyak:0.99500  Bf: 5 0.0390  EN:0.0962  Loss: 18.066 1.003 1.003\n",
      "Ep:400   Rew:-92.82  Avg Rew:-111.70  LR:0.00009615   Polyak:0.99500  Bf: 5 0.0400  EN:0.0962  Loss: 18.579 1.279 1.277\n",
      "Ep:410   Rew:-100.35  Avg Rew:-109.80  LR:0.00009606   Polyak:0.99500  Bf: 6 0.0410  EN:0.0961  Loss: 17.548 0.896 0.912\n",
      "Ep:420   Rew:-79.67  Avg Rew:-108.46  LR:0.00009597   Polyak:0.99500  Bf: 6 0.0420  EN:0.0960  Loss: 16.476 0.847 0.789\n",
      "Ep:430   Rew:-103.19  Avg Rew:-104.66  LR:0.00009588   Polyak:0.99500  Bf: 6 0.0430  EN:0.0959  Loss: 14.976 0.777 0.725\n",
      "Ep:440   Rew:-119.02  Avg Rew:-103.21  LR:0.00009579   Polyak:0.99500  Bf: 7 0.0440  EN:0.0958  Loss: 13.751 0.809 0.758\n",
      "Ep:450   Rew:-100.13  Avg Rew:-103.62  LR:0.00009569   Polyak:0.99500  Bf: 7 0.0450  EN:0.0957  Loss: 12.358 0.808 0.821\n",
      "Ep:460   Rew:-90.17  Avg Rew:-101.36  LR:0.00009560   Polyak:0.99500  Bf: 7 0.0460  EN:0.0956  Loss: 11.779 0.724 0.746\n",
      "Ep:470   Rew:-104.04  Avg Rew:-100.74  LR:0.00009551   Polyak:0.99500  Bf: 8 0.0470  EN:0.0955  Loss: 12.481 0.633 0.613\n",
      "Ep:480   Rew:-122.95  Avg Rew:-101.50  LR:0.00009542   Polyak:0.99500  Bf: 8 0.0480  EN:0.0954  Loss: 12.788 0.613 0.587\n",
      "Ep:490   Rew:-117.63  Avg Rew:-102.91  LR:0.00009533   Polyak:0.99500  Bf: 8 0.0490  EN:0.0953  Loss: 12.885 0.619 0.581\n",
      "Ep:500   Rew:-116.90  Avg Rew:-104.14  LR:0.00009524   Polyak:0.99500  Bf: 9 0.0500  EN:0.0952  Loss: 12.567 0.546 0.520\n",
      "Ep:510   Rew:-127.61  Avg Rew:-106.48  LR:0.00009515   Polyak:0.99500  Bf: 9 0.0510  EN:0.0951  Loss: 12.530 0.557 0.534\n",
      "Ep:520   Rew:-136.68  Avg Rew:-109.60  LR:0.00009506   Polyak:0.99500  Bf: 9 0.0520  EN:0.0951  Loss: 12.252 0.516 0.525\n",
      "Ep:530   Rew:-79.74  Avg Rew:-110.54  LR:0.00009497   Polyak:0.99500  Bf:10 0.0530  EN:0.0950  Loss: 12.186 0.590 0.556\n",
      "Ep:540   Rew:-117.19  Avg Rew:-110.26  LR:0.00009488   Polyak:0.99500  Bf:10 0.0540  EN:0.0949  Loss: 12.087 0.449 0.407\n",
      "Ep:550   Rew:-116.38  Avg Rew:-111.76  LR:0.00009479   Polyak:0.99500  Bf:10 0.0550  EN:0.0948  Loss: 11.291 0.483 0.498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep:560   Rew:-104.25  Avg Rew:-115.37  LR:0.00009470   Polyak:0.99500  Bf:10 0.0560  EN:0.0947  Loss: 11.857 0.611 0.578\n",
      "Ep:570   Rew:-150.69  Avg Rew:-119.69  LR:0.00009461   Polyak:0.99500  Bf:11 0.0570  EN:0.0946  Loss: 11.776 0.538 0.507\n",
      "Ep:580   Rew:-120.96  Avg Rew:-121.81  LR:0.00009452   Polyak:0.99500  Bf:11 0.0580  EN:0.0945  Loss: 11.707 0.381 0.343\n",
      "Ep:590   Rew:-72.95  Avg Rew:-121.44  LR:0.00009443   Polyak:0.99500  Bf:11 0.0590  EN:0.0944  Loss: 12.073 0.409 0.393\n",
      "Ep:600   Rew:-98.51  Avg Rew:-122.68  LR:0.00009434   Polyak:0.99500  Bf:12 0.0600  EN:0.0943  Loss: 11.667 0.438 0.407\n",
      "Ep:610   Rew:-115.65  Avg Rew:-121.88  LR:0.00009425   Polyak:0.99500  Bf:12 0.0610  EN:0.0943  Loss: 10.893 0.401 0.332\n",
      "Ep:620   Rew:-130.82  Avg Rew:-121.76  LR:0.00009416   Polyak:0.99500  Bf:12 0.0620  EN:0.0942  Loss: 10.171 0.386 0.339\n",
      "Ep:630   Rew:-129.93  Avg Rew:-123.07  LR:0.00009407   Polyak:0.99500  Bf:13 0.0630  EN:0.0941  Loss: 9.496 0.306 0.314\n",
      "Ep:640   Rew:-86.43  Avg Rew:-123.68  LR:0.00009398   Polyak:0.99500  Bf:13 0.0640  EN:0.0940  Loss: 9.020 0.331 0.294\n",
      "Ep:650   Rew:-91.22  Avg Rew:-120.78  LR:0.00009390   Polyak:0.99500  Bf:13 0.0650  EN:0.0939  Loss: 8.743 0.313 0.302\n",
      "Ep:660   Rew:-122.81  Avg Rew:-120.24  LR:0.00009381   Polyak:0.99500  Bf:14 0.0660  EN:0.0938  Loss: 8.822 0.260 0.241\n",
      "Ep:670   Rew:-147.39  Avg Rew:-118.89  LR:0.00009372   Polyak:0.99500  Bf:14 0.0670  EN:0.0937  Loss: 8.752 0.320 0.302\n",
      "Ep:680   Rew:-129.60  Avg Rew:-119.06  LR:0.00009363   Polyak:0.99500  Bf:14 0.0680  EN:0.0936  Loss: 8.579 0.324 0.307\n",
      "Ep:690   Rew:-109.00  Avg Rew:-119.48  LR:0.00009355   Polyak:0.99500  Bf:15 0.0690  EN:0.0935  Loss: 8.330 0.558 0.494\n",
      "Ep:700   Rew:-75.46  Avg Rew:-116.84  LR:0.00009346   Polyak:0.99500  Bf:15 0.0700  EN:0.0935  Loss: 8.473 0.328 0.332\n",
      "Ep:710   Rew:-110.13  Avg Rew:-114.85  LR:0.00009337   Polyak:0.99500  Bf:15 0.0710  EN:0.0934  Loss: 8.556 0.478 0.455\n",
      "Ep:720   Rew:-131.81  Avg Rew:-112.41  LR:0.00009328   Polyak:0.99500  Bf:16 0.0720  EN:0.0933  Loss: 8.776 0.391 0.359\n",
      "Ep:730   Rew:-116.28  Avg Rew:-111.45  LR:0.00009320   Polyak:0.99500  Bf:16 0.0730  EN:0.0932  Loss: 8.450 0.276 0.261\n",
      "Ep:740   Rew:-90.64  Avg Rew:-111.25  LR:0.00009311   Polyak:0.99500  Bf:16 0.0740  EN:0.0931  Loss: 8.603 0.285 0.262\n",
      "Ep:750   Rew:-71.17  Avg Rew:-109.37  LR:0.00009302   Polyak:0.99500  Bf:17 0.0750  EN:0.0930  Loss: 8.332 0.285 0.274\n",
      "Ep:760   Rew:-81.22  Avg Rew:-105.53  LR:0.00009294   Polyak:0.99500  Bf:17 0.0760  EN:0.0929  Loss: 8.296 0.418 0.413\n",
      "Ep:770   Rew:-80.65  Avg Rew:-101.95  LR:0.00009285   Polyak:0.99500  Bf:17 0.0770  EN:0.0929  Loss: 7.956 0.281 0.262\n",
      "Ep:780   Rew:-87.71  Avg Rew:-98.46  LR:0.00009276   Polyak:0.99500  Bf:18 0.0780  EN:0.0928  Loss: 7.672 0.394 0.378\n",
      "Ep:790   Rew:-93.90  Avg Rew:-95.88  LR:0.00009268   Polyak:0.99500  Bf:18 0.0790  EN:0.0927  Loss: 7.636 0.324 0.316\n",
      "Ep:800   Rew:-104.16  Avg Rew:-95.18  LR:0.00009259   Polyak:0.99500  Bf:18 0.0800  EN:0.0926  Loss: 7.894 0.382 0.397\n",
      "Ep:810   Rew:-99.90  Avg Rew:-94.82  LR:0.00009251   Polyak:0.99500  Bf:18 0.0810  EN:0.0925  Loss: 8.661 0.284 0.283\n",
      "Ep:820   Rew:-110.30  Avg Rew:-94.86  LR:0.00009242   Polyak:0.99500  Bf:19 0.0820  EN:0.0924  Loss: 8.979 0.449 0.413\n",
      "Ep:830   Rew:-118.19  Avg Rew:-95.19  LR:0.00009234   Polyak:0.99500  Bf:19 0.0830  EN:0.0923  Loss: 9.245 0.519 0.501\n",
      "Ep:840   Rew:-136.62  Avg Rew:-97.14  LR:0.00009225   Polyak:0.99500  Bf:19 0.0840  EN:0.0923  Loss: 9.166 0.269 0.224\n",
      "Ep:850   Rew:-112.12  Avg Rew:-99.59  LR:0.00009217   Polyak:0.99500  Bf:20 0.0850  EN:0.0922  Loss: 9.013 0.408 0.385\n",
      "Ep:860   Rew:-80.26  Avg Rew:-101.62  LR:0.00009208   Polyak:0.99500  Bf:20 0.0860  EN:0.0921  Loss: 9.116 0.292 0.270\n",
      "Ep:870   Rew:-99.04  Avg Rew:-103.72  LR:0.00009200   Polyak:0.99500  Bf:20 0.0870  EN:0.0920  Loss: 9.555 0.285 0.276\n",
      "Ep:880   Rew:-93.49  Avg Rew:-104.37  LR:0.00009191   Polyak:0.99500  Bf:21 0.0880  EN:0.0919  Loss: 9.629 0.372 0.342\n",
      "Ep:890   Rew:-89.63  Avg Rew:-103.74  LR:0.00009183   Polyak:0.99500  Bf:21 0.0890  EN:0.0918  Loss: 9.607 0.291 0.295\n",
      "Ep:900   Rew:-115.33  Avg Rew:-104.53  LR:0.00009174   Polyak:0.99500  Bf:21 0.0900  EN:0.0917  Loss: 9.483 0.427 0.422\n",
      "Ep:910   Rew:-117.60  Avg Rew:-106.39  LR:0.00009166   Polyak:0.99500  Bf:22 0.0910  EN:0.0917  Loss: 9.407 0.308 0.316\n",
      "Ep:920   Rew:-112.55  Avg Rew:-106.35  LR:0.00009158   Polyak:0.99500  Bf:22 0.0920  EN:0.0916  Loss: 9.412 0.260 0.231\n",
      "Ep:930   Rew:-109.36  Avg Rew:-104.98  LR:0.00009149   Polyak:0.99500  Bf:22 0.0930  EN:0.0915  Loss: 9.299 0.326 0.300\n",
      "Ep:940   Rew:-116.12  Avg Rew:-105.20  LR:0.00009141   Polyak:0.99500  Bf:23 0.0940  EN:0.0914  Loss: 9.001 0.331 0.319\n",
      "Ep:950   Rew:-113.96  Avg Rew:-106.61  LR:0.00009132   Polyak:0.99500  Bf:23 0.0950  EN:0.0913  Loss: 8.705 0.245 0.240\n",
      "Ep:960   Rew:-114.83  Avg Rew:-107.58  LR:0.00009124   Polyak:0.99500  Bf:23 0.0960  EN:0.0912  Loss: 8.701 0.282 0.276\n",
      "Ep:970   Rew:-111.68  Avg Rew:-108.57  LR:0.00009116   Polyak:0.99500  Bf:24 0.0970  EN:0.0912  Loss: 9.067 0.245 0.242\n",
      "Ep:980   Rew:-164.09  Avg Rew:-109.93  LR:0.00009107   Polyak:0.99500  Bf:24 0.0980  EN:0.0911  Loss: 8.643 0.211 0.203\n",
      "Ep:990   Rew:-114.67  Avg Rew:-112.78  LR:0.00009099   Polyak:0.99500  Bf:24 0.0990  EN:0.0910  Loss: 8.748 0.163 0.180\n",
      "Ep:1000   Rew:-112.50  Avg Rew:-115.30  LR:0.00009091   Polyak:0.99500  Bf:24 0.1000  EN:0.0909  Loss: 8.778 0.219 0.194\n",
      "Ep:1010   Rew:-99.00  Avg Rew:-114.99  LR:0.00009083   Polyak:0.99500  Bf:25 0.1010  EN:0.0908  Loss: 8.118 0.326 0.294\n",
      "Ep:1020   Rew:-113.74  Avg Rew:-115.50  LR:0.00009074   Polyak:0.99500  Bf:25 0.1020  EN:0.0907  Loss: 7.410 0.266 0.266\n",
      "Ep:1030   Rew:-136.55  Avg Rew:-116.80  LR:0.00009066   Polyak:0.99500  Bf:25 0.1030  EN:0.0907  Loss: 7.125 0.247 0.244\n",
      "Ep:1040   Rew:-134.49  Avg Rew:-117.17  LR:0.00009058   Polyak:0.99500  Bf:26 0.1040  EN:0.0906  Loss: 7.022 0.252 0.236\n",
      "Ep:1050   Rew:-94.18  Avg Rew:-116.53  LR:0.00009050   Polyak:0.99500  Bf:26 0.1050  EN:0.0905  Loss: 6.811 0.266 0.205\n",
      "Ep:1060   Rew:-87.55  Avg Rew:-114.33  LR:0.00009042   Polyak:0.99500  Bf:26 0.1060  EN:0.0904  Loss: 6.891 0.232 0.203\n",
      "Ep:1070   Rew:-111.95  Avg Rew:-112.57  LR:0.00009033   Polyak:0.99500  Bf:26 0.1070  EN:0.0903  Loss: 6.681 0.377 0.356\n",
      "Ep:1080   Rew:-109.02  Avg Rew:-112.15  LR:0.00009025   Polyak:0.99500  Bf:26 0.1080  EN:0.0903  Loss: 6.861 0.301 0.301\n",
      "Ep:1090   Rew:-111.70  Avg Rew:-111.39  LR:0.00009017   Polyak:0.99500  Bf:26 0.1090  EN:0.0902  Loss: 6.766 0.270 0.271\n",
      "Ep:1100   Rew:-111.60  Avg Rew:-109.60  LR:0.00009009   Polyak:0.99500  Bf:26 0.1100  EN:0.0901  Loss: 6.831 0.447 0.422\n",
      "Ep:1110   Rew:-109.20  Avg Rew:-109.65  LR:0.00009001   Polyak:0.99500  Bf:26 0.1110  EN:0.0900  Loss: 6.700 0.386 0.410\n",
      "Ep:1120   Rew:-104.60  Avg Rew:-109.35  LR:0.00008993   Polyak:0.99500  Bf:27 0.1120  EN:0.0899  Loss: 6.605 0.435 0.405\n",
      "Ep:1130   Rew:-103.75  Avg Rew:-106.31  LR:0.00008985   Polyak:0.99500  Bf:27 0.1130  EN:0.0898  Loss: 6.165 0.775 0.806\n",
      "Ep:1140   Rew:-106.72  Avg Rew:-104.28  LR:0.00008977   Polyak:0.99500  Bf:27 0.1140  EN:0.0898  Loss: 6.286 0.731 0.715\n",
      "Ep:1150   Rew:-103.12  Avg Rew:-103.07  LR:0.00008969   Polyak:0.99500  Bf:27 0.1150  EN:0.0897  Loss: 6.063 1.077 1.098\n",
      "Ep:1160   Rew:-106.01  Avg Rew:-104.33  LR:0.00008961   Polyak:0.99500  Bf:27 0.1160  EN:0.0896  Loss: 5.865 0.867 0.793\n",
      "Ep:1170   Rew:-98.85  Avg Rew:-105.28  LR:0.00008953   Polyak:0.99500  Bf:27 0.1170  EN:0.0895  Loss: 5.482 1.058 1.062\n",
      "Ep:1180   Rew:-106.65  Avg Rew:-105.29  LR:0.00008945   Polyak:0.99500  Bf:27 0.1180  EN:0.0894  Loss: 2.931 1.865 1.936\n",
      "Ep:1190   Rew:-146.13  Avg Rew:-105.70  LR:0.00008937   Polyak:0.99500  Bf:27 0.1190  EN:0.0894  Loss: -5.971 2.668 2.650\n",
      "Ep:1200   Rew:-170.41  Avg Rew:-108.75  LR:0.00008929   Polyak:0.99500  Bf:27 0.1200  EN:0.0893  Loss: -26.542 1.584 1.576\n",
      "Ep:1210   Rew:-170.50  Avg Rew:-113.50  LR:0.00008921   Polyak:0.99500  Bf:27 0.1210  EN:0.0892  Loss: -19.173 0.792 0.792\n",
      "Ep:1220   Rew:-114.87  Avg Rew:-117.38  LR:0.00008913   Polyak:0.99500  Bf:28 0.1220  EN:0.0891  Loss: -10.378 0.536 0.609\n",
      "Ep:1230   Rew:-169.49  Avg Rew:-125.78  LR:0.00008905   Polyak:0.99500  Bf:28 0.1230  EN:0.0890  Loss: -5.722 0.558 0.494\n",
      "Ep:1240   Rew:-87.63  Avg Rew:-128.68  LR:0.00008897   Polyak:0.99500  Bf:28 0.1240  EN:0.0890  Loss: -1.069 0.483 0.481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep:1250   Rew:-119.60  Avg Rew:-127.12  LR:0.00008889   Polyak:0.99500  Bf:29 0.1250  EN:0.0889  Loss: 1.824 0.445 0.426\n",
      "Ep:1260   Rew:-74.75  Avg Rew:-125.24  LR:0.00008881   Polyak:0.99500  Bf:29 0.1260  EN:0.0888  Loss: 3.200 0.252 0.263\n",
      "Ep:1270   Rew:-56.19  Avg Rew:-120.86  LR:0.00008873   Polyak:0.99500  Bf:29 0.1270  EN:0.0887  Loss: 4.309 0.547 0.605\n",
      "Ep:1280   Rew:-48.97  Avg Rew:-116.25  LR:0.00008865   Polyak:0.99500  Bf:30 0.1280  EN:0.0887  Loss: 5.053 0.263 0.262\n",
      "Ep:1290   Rew:-77.66  Avg Rew:-112.32  LR:0.00008857   Polyak:0.99500  Bf:30 0.1290  EN:0.0886  Loss: 5.698 0.405 0.408\n",
      "Ep:1300   Rew:-38.91  Avg Rew:-103.18  LR:0.00008850   Polyak:0.99500  Bf:30 0.1300  EN:0.0885  Loss: 6.415 0.333 0.305\n",
      "Ep:1310   Rew:-64.68  Avg Rew:-92.02  LR:0.00008842   Polyak:0.99500  Bf:31 0.1310  EN:0.0884  Loss: 6.675 0.292 0.325\n",
      "Ep:1320   Rew:-38.52  Avg Rew:-83.00  LR:0.00008834   Polyak:0.99500  Bf:31 0.1320  EN:0.0883  Loss: 6.887 0.246 0.244\n",
      "Ep:1330   Rew:-47.20  Avg Rew:-70.72  LR:0.00008826   Polyak:0.99500  Bf:31 0.1330  EN:0.0883  Loss: 7.161 0.398 0.373\n",
      "Ep:1340   Rew:-39.93  Avg Rew:-62.43  LR:0.00008818   Polyak:0.99500  Bf:31 0.1340  EN:0.0882  Loss: 7.161 0.330 0.362\n",
      "Ep:1350   Rew:-27.70  Avg Rew:-57.65  LR:0.00008811   Polyak:0.99500  Bf:32 0.1350  EN:0.0881  Loss: 7.398 0.248 0.240\n",
      "Ep:1360   Rew:-41.42  Avg Rew:-53.55  LR:0.00008803   Polyak:0.99500  Bf:32 0.1360  EN:0.0880  Loss: 7.169 0.252 0.263\n",
      "Ep:1370   Rew:-40.35  Avg Rew:-51.45  LR:0.00008795   Polyak:0.99500  Bf:32 0.1370  EN:0.0880  Loss: 6.959 0.332 0.294\n",
      "Ep:1380   Rew:-128.87  Avg Rew:-56.09  LR:0.00008787   Polyak:0.99500  Bf:33 0.1380  EN:0.0879  Loss: 6.993 0.232 0.218\n",
      "Ep:1390   Rew:-109.61  Avg Rew:-58.42  LR:0.00008780   Polyak:0.99500  Bf:33 0.1390  EN:0.0878  Loss: 7.052 0.191 0.146\n",
      "Ep:1400   Rew:-81.30  Avg Rew:-59.61  LR:0.00008772   Polyak:0.99500  Bf:33 0.1400  EN:0.0877  Loss: 7.023 0.323 0.320\n",
      "Ep:1410   Rew:-114.85  Avg Rew:-62.07  LR:0.00008764   Polyak:0.99500  Bf:34 0.1410  EN:0.0876  Loss: 6.864 0.245 0.276\n",
      "Ep:1420   Rew:-35.25  Avg Rew:-63.15  LR:0.00008757   Polyak:0.99500  Bf:34 0.1420  EN:0.0876  Loss: 6.698 0.199 0.181\n",
      "Ep:1430   Rew:-109.93  Avg Rew:-67.50  LR:0.00008749   Polyak:0.99500  Bf:34 0.1430  EN:0.0875  Loss: 6.601 0.326 0.322\n",
      "Ep:1440   Rew:-41.02  Avg Rew:-67.01  LR:0.00008741   Polyak:0.99500  Bf:35 0.1440  EN:0.0874  Loss: 6.327 0.271 0.243\n",
      "Ep:1450   Rew:-42.40  Avg Rew:-69.34  LR:0.00008734   Polyak:0.99500  Bf:35 0.1450  EN:0.0873  Loss: 6.287 0.236 0.244\n",
      "Ep:1460   Rew:-86.71  Avg Rew:-71.63  LR:0.00008726   Polyak:0.99500  Bf:35 0.1460  EN:0.0873  Loss: 6.143 0.231 0.253\n",
      "Ep:1470   Rew:-93.70  Avg Rew:-75.84  LR:0.00008718   Polyak:0.99500  Bf:36 0.1470  EN:0.0872  Loss: 6.227 0.242 0.221\n",
      "Ep:1480   Rew:-76.79  Avg Rew:-73.76  LR:0.00008711   Polyak:0.99500  Bf:36 0.1480  EN:0.0871  Loss: 6.359 0.161 0.140\n",
      "Ep:1490   Rew:-95.67  Avg Rew:-73.72  LR:0.00008703   Polyak:0.99500  Bf:36 0.1490  EN:0.0870  Loss: 6.262 0.158 0.151\n",
      "Ep:1500   Rew:-65.90  Avg Rew:-75.79  LR:0.00008696   Polyak:0.99500  Bf:37 0.1500  EN:0.0870  Loss: 6.239 0.136 0.128\n",
      "Ep:1510   Rew:-102.15  Avg Rew:-77.29  LR:0.00008688   Polyak:0.99500  Bf:37 0.1510  EN:0.0869  Loss: 6.370 0.164 0.143\n",
      "Ep:1520   Rew:-64.57  Avg Rew:-78.58  LR:0.00008681   Polyak:0.99500  Bf:37 0.1520  EN:0.0868  Loss: 6.573 0.185 0.183\n",
      "Ep:1530   Rew:-109.98  Avg Rew:-78.56  LR:0.00008673   Polyak:0.99500  Bf:38 0.1530  EN:0.0867  Loss: 6.332 0.223 0.235\n",
      "Ep:1540   Rew:-50.40  Avg Rew:-78.30  LR:0.00008666   Polyak:0.99500  Bf:38 0.1540  EN:0.0867  Loss: 6.364 0.178 0.181\n",
      "Ep:1550   Rew:-10.78  Avg Rew:-76.60  LR:0.00008658   Polyak:0.99500  Bf:38 0.1550  EN:0.0866  Loss: 6.239 0.159 0.155\n",
      "Ep:1560   Rew:-18.68  Avg Rew:-72.84  LR:0.00008651   Polyak:0.99500  Bf:39 0.1560  EN:0.0865  Loss: 6.373 0.191 0.197\n",
      "Ep:1570   Rew:-19.82  Avg Rew:-66.89  LR:0.00008643   Polyak:0.99500  Bf:39 0.1570  EN:0.0864  Loss: 6.160 0.132 0.131\n",
      "Ep:1580   Rew:-54.45  Avg Rew:-59.61  LR:0.00008636   Polyak:0.99500  Bf:39 0.1580  EN:0.0864  Loss: 5.920 0.168 0.187\n",
      "Ep:1590   Rew:-29.92  Avg Rew:-53.48  LR:0.00008628   Polyak:0.99500  Bf:39 0.1590  EN:0.0863  Loss: 5.465 0.148 0.153\n",
      "Ep:1600   Rew:1.86  Avg Rew:-55.55  LR:0.00008621   Polyak:0.99500  Bf:40 0.1600  EN:0.0862  Loss: 5.240 0.154 0.146\n",
      "Ep:1610   Rew:-70.66  Avg Rew:-49.37  LR:0.00008613   Polyak:0.99500  Bf:40 0.1610  EN:0.0861  Loss: 5.417 0.138 0.136\n",
      "Ep:1620   Rew:-169.85  Avg Rew:-55.31  LR:0.00008606   Polyak:0.99500  Bf:40 0.1620  EN:0.0861  Loss: 5.732 0.196 0.178\n",
      "Ep:1630   Rew:-157.08  Avg Rew:-62.92  LR:0.00008598   Polyak:0.99500  Bf:40 0.1630  EN:0.0860  Loss: 6.095 0.151 0.132\n",
      "Ep:1640   Rew:-143.20  Avg Rew:-72.98  LR:0.00008591   Polyak:0.99500  Bf:40 0.1640  EN:0.0859  Loss: 6.379 0.194 0.192\n",
      "Ep:1650   Rew:-157.96  Avg Rew:-83.17  LR:0.00008584   Polyak:0.99500  Bf:41 0.1650  EN:0.0858  Loss: 6.515 0.244 0.241\n",
      "Ep:1660   Rew:-147.75  Avg Rew:-96.62  LR:0.00008576   Polyak:0.99500  Bf:41 0.1660  EN:0.0858  Loss: 6.754 0.256 0.243\n",
      "Ep:1670   Rew:-158.72  Avg Rew:-109.89  LR:0.00008569   Polyak:0.99500  Bf:41 0.1670  EN:0.0857  Loss: 6.999 0.247 0.245\n",
      "Ep:1680   Rew:-21.12  Avg Rew:-117.94  LR:0.00008562   Polyak:0.99500  Bf:41 0.1680  EN:0.0856  Loss: 7.041 0.221 0.192\n",
      "Ep:1690   Rew:-94.32  Avg Rew:-117.14  LR:0.00008554   Polyak:0.99500  Bf:41 0.1690  EN:0.0855  Loss: 6.739 0.164 0.156\n",
      "Ep:1700   Rew:15.32  Avg Rew:-108.91  LR:0.00008547   Polyak:0.99500  Bf:42 0.1700  EN:0.0855  Loss: 6.846 0.235 0.247\n",
      "Ep:1710   Rew:-64.59  Avg Rew:-111.51  LR:0.00008540   Polyak:0.99500  Bf:42 0.1710  EN:0.0854  Loss: 7.043 0.196 0.195\n",
      "Ep:1720   Rew:-24.42  Avg Rew:-101.65  LR:0.00008532   Polyak:0.99500  Bf:42 0.1720  EN:0.0853  Loss: 7.017 0.221 0.219\n",
      "Ep:1730   Rew:32.10  Avg Rew:-90.35  LR:0.00008525   Polyak:0.99500  Bf:43 0.1730  EN:0.0853  Loss: 6.694 0.670 0.709\n",
      "Ep:1740   Rew:-111.30  Avg Rew:-86.79  LR:0.00008518   Polyak:0.99500  Bf:43 0.1740  EN:0.0852  Loss: 6.945 0.830 0.857\n",
      "Ep:1750   Rew:22.52  Avg Rew:-81.21  LR:0.00008511   Polyak:0.99500  Bf:43 0.1750  EN:0.0851  Loss: 7.286 0.372 0.348\n",
      "Ep:1760   Rew:-30.56  Avg Rew:-65.43  LR:0.00008503   Polyak:0.99500  Bf:44 0.1760  EN:0.0850  Loss: 7.187 0.273 0.272\n",
      "Ep:1770   Rew:-57.75  Avg Rew:-53.04  LR:0.00008496   Polyak:0.99500  Bf:44 0.1770  EN:0.0850  Loss: 7.061 0.267 0.257\n",
      "Ep:1780   Rew:-20.96  Avg Rew:-44.34  LR:0.00008489   Polyak:0.99500  Bf:44 0.1780  EN:0.0849  Loss: 6.914 0.849 0.841\n",
      "Ep:1790   Rew:-38.60  Avg Rew:-43.57  LR:0.00008482   Polyak:0.99500  Bf:45 0.1790  EN:0.0848  Loss: 6.271 0.666 0.654\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d712ea419883>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                    \u001b[0mmax_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_timesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_buffer_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_buffer_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                    log_interval=log_interval)\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-54c191ed4cbd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mep_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;31m# calculate params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36m_after_reset\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_video_recorder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;31m# Bump *after* all reset activity has finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mreset_video_recorder\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_video_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         )\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_close_video_recorder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36mcapture_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_ansi_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_image_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36m_encode_image_frame\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_encode_image_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframes_per_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoder_version'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, output_path, frame_shape, frames_per_sec)\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDependencyNotInstalled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\"Found neither the ffmpeg nor avconv executables. On OS X, you can install ffmpeg via `brew install ffmpeg`. On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`.\"\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Starting ffmpeg with \"%s\"'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmdline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'setsid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#setsid not present on Windows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmdline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreexec_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetsid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmdline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gym/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors)\u001b[0m\n\u001b[1;32m    727\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    730\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0;31m# Cleanup if the child failed starting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gym/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1293\u001b[0m                             \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m                             \u001b[0merrpipe_read\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrpipe_write\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m                             restore_signals, start_new_session, preexec_fn)\n\u001b[0m\u001b[1;32m   1296\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_child_created\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "agent = TD3Trainer(env_name, actor_config, critic_config, random_seed=random_seed, lr_base=lr_base, lr_decay=lr_decay, \n",
    "                   exp_noise_base=exp_noise_base, exp_noise_decay=exp_noise_decay, gamma=gamma, batch_size=batch_size,\n",
    "                   polyak=polyak, policy_noise=policy_noise, noise_clip=noise_clip, policy_delay=policy_delay, \n",
    "                   max_episodes=max_episodes, max_timesteps=max_timesteps, max_buffer_length=max_buffer_length, \n",
    "                   log_interval=log_interval)\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
