{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import gym\n",
    "import numpy as np\n",
    "from gym import wrappers\n",
    "from PIL import Image\n",
    "\n",
    "from TD3.td3 import TD3\n",
    "from TD3.utils import ReplayBuffer, mkdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'BipedalWalker-v2'\n",
    "lr_base = 0.0005\n",
    "lr_decay = 0.0001\n",
    "exp_noise_base = 0.1\n",
    "exp_noise_decay = 0.0001\n",
    "\n",
    "random_seed = 42\n",
    "gamma = 0.99                # discount for future rewards\n",
    "batch_size = 1024        # num of transitions sampled from replay buffer\n",
    "polyak = 0.999              # target policy update parameter (1-tau)\n",
    "policy_noise = 0.2          # target policy smoothing noise\n",
    "noise_clip = 0.5\n",
    "policy_delay = 2            # delayed policy updates parameter\n",
    "max_episodes = 100000         # max num of episodes\n",
    "max_timesteps = 3000        # max timesteps in one episode\n",
    "max_buffer_length = 5000000\n",
    "log_interval = 10           # print avg reward after interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_config = [\n",
    "        {'dim': [None, 400], 'dropout': False, 'activation': 'relu'},\n",
    "        {'dim': [400, 300], 'dropout': False, 'activation':'relu'},\n",
    "        {'dim': [300, None], 'dropout': False, 'activation': 'sigmoid'}\n",
    "    ]\n",
    "    \n",
    "critic_config = [\n",
    "        {'dim': [None, 400], 'dropout': False, 'activation': 'relu'},\n",
    "        {'dim': [400, 300], 'dropout': False , 'activation':'relu'},\n",
    "        {'dim': [300, 1], 'dropout': False, 'activation': False}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3Trainer():\n",
    "    \n",
    "    def __init__(self, env_name, actor_config, critic_config, random_seed=42, lr_base=0.001, lr_decay=0.00005, \n",
    "                 exp_noise_base=0.3, exp_noise_decay=0.0001, gamma=0.99, batch_size=1024, \n",
    "                 polyak=0.9999, policy_noise=0.2, noise_clip=0.5, policy_delay=2, \n",
    "                 max_episodes=100000, max_timesteps=3000, max_buffer_length=5000000, \n",
    "                 log_interval=5, threshold=None, lr_minimum=1e-10, exp_noise_minimum=1e-10,\n",
    "                 record_videos=True, record_interval=100):        \n",
    "        \n",
    "        self.algorithm_name = 'td3'\n",
    "        self.env_name = env_name\n",
    "        self.env = gym.make(env_name)\n",
    "        self.record_videos = record_videos\n",
    "        self.record_interval = record_interval        \n",
    "        if self.record_videos == True:\n",
    "            videos_dir = mkdir('.', 'videos')\n",
    "            monitor_dir = mkdir(videos_dir, self.algorithm_name)\n",
    "            should_record = lambda i: self.should_record\n",
    "            self.env = wrappers.Monitor(self.env, monitor_dir, video_callable=should_record, force=True)            \n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.shape[0]\n",
    "        self.action_low = self.env.action_space.low\n",
    "        self.action_high = self.env.action_space.high        \n",
    "        self.should_record = False\n",
    "        if not threshold == None:\n",
    "            self.threshold = threshold\n",
    "        else:    \n",
    "            self.threshold = self.env.spec.reward_threshold\n",
    "        \n",
    "        self.actor_config = actor_config\n",
    "        self.critic_config = critic_config\n",
    "        self.actor_config[0]['dim'][0] = self.state_dim\n",
    "        self.actor_config[-1]['dim'][1] = self.action_dim\n",
    "        self.critic_config[0]['dim'][0] = self.state_dim + self.action_dim\n",
    "        \n",
    "        self.actor_config = actor_config\n",
    "        self.critic_config = critic_config\n",
    "        self.random_seed = random_seed\n",
    "        self.lr_base = lr_base\n",
    "        self.lr_decay = lr_decay   \n",
    "        self.lr_minimum = lr_minimum\n",
    "        self.exp_noise_base = exp_noise_base\n",
    "        self.exp_noise_decay = exp_noise_decay     \n",
    "        self.exp_noise_minimum = exp_noise_minimum                \n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size        \n",
    "        self.polyak = polyak\n",
    "        self.policy_noise = policy_noise\n",
    "        self.noise_clip = noise_clip\n",
    "        self.policy_delay = policy_delay\n",
    "        self.max_episodes = max_episodes\n",
    "        self.max_timesteps = max_timesteps\n",
    "        self.max_buffer_length = max_buffer_length\n",
    "        self.log_interval = log_interval\n",
    "        \n",
    "        prdir = mkdir('.', 'preTrained')\n",
    "        self.directory = mkdir(prdir, self.algorithm_name)\n",
    "        self.filename = \"{}_{}_{}\".format(self.algorithm_name, self.env_name, self.random_seed)\n",
    "                \n",
    "        self.policy = TD3(self.actor_config, self.critic_config, self.action_low, self.action_high)   \n",
    "        self.replay_buffer = ReplayBuffer(max_length=self.max_buffer_length)\n",
    "        \n",
    "        self.reward_history = []\n",
    "        self.make_plots = False       \n",
    "        \n",
    "        if self.random_seed:\n",
    "            print(\"Random Seed: {}\".format(self.random_seed))\n",
    "            self.env.seed(self.random_seed)\n",
    "            torch.manual_seed(self.random_seed)\n",
    "            np.random.seed(self.random_seed)\n",
    "        \n",
    "    def train(self):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        print(\"Training started ... \\n\")\n",
    "        print(\"action_space={}\".format(self.env.action_space))\n",
    "        print(\"obs_space={}\".format(self.env.observation_space))\n",
    "        print(\"threshold={}\".format(self.threshold))     \n",
    "        print(\"action_low={} action_high={} \\n\".format(self.action_low, self.action_high))         \n",
    "\n",
    "        # loading models\n",
    "        self.policy.load(self.directory, self.filename)\n",
    "                \n",
    "        # logging variables:        \n",
    "        log_f = open(\"train_{}.txt\".format(self.algorithm_name), \"w+\")\n",
    "\n",
    "        # training procedure:\n",
    "        for episode in range(1, self.max_episodes+1):\n",
    "            \n",
    "            # Only record video during evaluation, every n steps\n",
    "            if episode % self.record_interval == 0:\n",
    "                self.should_record = True\n",
    "            \n",
    "            ep_reward = 0.0\n",
    "            state = self.env.reset()\n",
    "            \n",
    "            # calculate params\n",
    "            exploration_noise = max(self.exp_noise_base / (1.0 + episode * self.exp_noise_decay), self.exp_noise_minimum)\n",
    "            learning_rate = max(self.lr_base / (1.0 + episode * self.lr_decay), self.lr_minimum)            \n",
    "            self.policy.set_optimizers(lr=learning_rate)\n",
    "\n",
    "            for t in range(self.max_timesteps):\n",
    "                \n",
    "                # select action and add exploration noise:\n",
    "                action = self.policy.select_action(state)               \n",
    "                action = action + np.random.normal(0, exploration_noise, size=self.action_dim)\n",
    "                action = action.clip(self.action_low, self.action_high)\n",
    "\n",
    "                # take action in env:\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                self.replay_buffer.add((state, action, reward, next_state, float(done)))\n",
    "                state = next_state\n",
    "\n",
    "                ep_reward += reward\n",
    "\n",
    "                # if episode is done then update policy:\n",
    "                if done or t==(self.max_timesteps-1):\n",
    "                    self.policy.update(self.replay_buffer, t, self.batch_size, self.gamma, self.polyak, \n",
    "                                       self.policy_noise, self.noise_clip, self.policy_delay)\n",
    "                    break\n",
    "\n",
    "            self.reward_history.append(ep_reward)\n",
    "            avg_reward = np.mean(self.reward_history[-100:]) \n",
    "\n",
    "            # logging updates:        \n",
    "            log_f.write('{},{}\\n'.format(episode, ep_reward))\n",
    "            log_f.flush()\n",
    "            \n",
    "            # Calculate polyak\n",
    "            #part = (env.spec.reward_threshold - avg_reward) / (env.spec.reward_threshold + 150)\n",
    "            #if part > 1:\n",
    "            #    part = 1\n",
    "            #polyak = polyak_int[0] + (1 - part) * (polyak_int[1] - polyak_int[0])     \n",
    "\n",
    "            # Calculate LR\n",
    "            #part = min((env.spec.reward_threshold - avg_reward) / (env.spec.reward_threshold + 150), 1)\n",
    "                        \n",
    "            avg_actor_loss = np.mean(self.policy.actor_loss_list[-100:])\n",
    "            avg_Q1_loss = np.mean(self.policy.Q1_loss_list[-100:])\n",
    "            avg_Q2_loss = np.mean(self.policy.Q2_loss_list[-100:])\n",
    "\n",
    "            if not self.make_plots and len(self.policy.actor_loss_list) > 200:\n",
    "                self.policy.actor_loss_list.pop(0)\n",
    "                self.policy.Q1_loss_list.pop(0)\n",
    "                self.policy.Q2_loss_list.pop(0)  \n",
    "                self.reward_history.pop(0)    \n",
    "\n",
    "            # Print avg reward every log interval:\n",
    "            if episode % self.log_interval == 0:            \n",
    "                self.policy.save(self.directory, self.filename)\n",
    "                print(\"Ep:{}   Rew:{:3.2f}  Avg Rew:{:3.2f}  LR:{:8.8f}   Polyak:{:5.5f}  Bf:{:2.0f}  EN:{:0.4f}  Loss: {:5.3f} {:5.3f} {:5.3f}\".format(\n",
    "                    episode, ep_reward, avg_reward, learning_rate, self.polyak, self.replay_buffer.get_fill(), \n",
    "                    exploration_noise, avg_actor_loss, avg_Q1_loss, avg_Q2_loss))\n",
    "                \n",
    "            self.should_record = False    \n",
    "                \n",
    "            # if avg reward > threshold then save and stop traning:\n",
    "            if avg_reward >= self.threshold and episode > 100: \n",
    "                print(\"Ep:{}   Rew:{:3.2f}  Avg Rew:{:3.2f}  LR:{:8.8f}   Polyak:{:5.5f}  Bf:{:2.0f}  EN:{:0.4f}  Loss: {:5.3f} {:5.3f} {:5.3f}\".format(\n",
    "                    episode, ep_reward, avg_reward, learning_rate, self.polyak, self.replay_buffer.get_fill(), \n",
    "                    exploration_noise, avg_actor_loss, avg_Q1_loss, avg_Q2_loss))\n",
    "                print(\"########## Solved! ###########\")\n",
    "                name = self.filename + '_solved'\n",
    "                self.policy.save(self.directory, name)\n",
    "                log_f.close()\n",
    "                training_time = time.time() - start_time\n",
    "                print(\"Training time: {:6.2f} sec\".format(training_time))\n",
    "                break    \n",
    "       \n",
    "    def test(self, episodes=3, render=True, save_gif=True):   \n",
    "        \n",
    "        gifdir = mkdir('.','gif')\n",
    "        algdir = mkdir(gifdir, self.algorithm_name)\n",
    "\n",
    "        for episode in range(1, episodes+1):\n",
    "            ep_reward = 0.0\n",
    "            state = self.env.reset()\n",
    "            epdir = mkdir(algdir, str(episode))\n",
    "            \n",
    "            for t in range(self.max_timesteps):\n",
    "                action = self.policy.select_action(state)\n",
    "                state, reward, done, _ = self.env.step(action)\n",
    "                ep_reward += reward\n",
    "                \n",
    "                if save_gif:                                       \n",
    "                    img = self.env.render(mode = 'rgb_array')\n",
    "                    img = Image.fromarray(img)\n",
    "                    img.save('{}/{}.jpg'.format(epdir, t))\n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "\n",
    "            print('Test episode: {}\\tReward: {:4.2f}'.format(episode, ep_reward))           \n",
    "            self.env.close()        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "ACTOR=Sequential(\n",
      "  (0): Linear(in_features=24, out_features=400, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=400, out_features=300, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=300, out_features=4, bias=True)\n",
      "  (5): Sigmoid()\n",
      ")\n",
      "ACTOR=Sequential(\n",
      "  (0): Linear(in_features=24, out_features=400, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=400, out_features=300, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=300, out_features=4, bias=True)\n",
      "  (5): Sigmoid()\n",
      ")\n",
      "CRITIC=Sequential(\n",
      "  (0): Linear(in_features=28, out_features=400, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=400, out_features=300, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=300, out_features=1, bias=True)\n",
      ")\n",
      "CRITIC=Sequential(\n",
      "  (0): Linear(in_features=28, out_features=400, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=400, out_features=300, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=300, out_features=1, bias=True)\n",
      ")\n",
      "CRITIC=Sequential(\n",
      "  (0): Linear(in_features=28, out_features=400, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=400, out_features=300, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=300, out_features=1, bias=True)\n",
      ")\n",
      "CRITIC=Sequential(\n",
      "  (0): Linear(in_features=28, out_features=400, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=400, out_features=300, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=300, out_features=1, bias=True)\n",
      ")\n",
      "Random Seed: 42\n",
      "Training started ... \n",
      "\n",
      "action_space=Box(4,)\n",
      "obs_space=Box(24,)\n",
      "threshold=300\n",
      "action_low=[-1. -1. -1. -1.] action_high=[1. 1. 1. 1.] \n",
      "\n",
      "DIR=./preTrained/td3 NAME=td3_BipedalWalker-v2_42\n",
      "No models to load\n",
      "Ep:10   Rew:-10.63  Avg Rew:-10.63  LR:0.00049950   Polyak:0.99900  Bf: 0  EN:0.0999  Loss: -0.584 0.207 0.202\n",
      "Ep:20   Rew:-112.91  Avg Rew:-112.91  LR:0.00049900   Polyak:0.99900  Bf: 0  EN:0.0998  Loss: 0.084 4.954 4.971\n",
      "Ep:30   Rew:-148.13  Avg Rew:-148.13  LR:0.00049850   Polyak:0.99900  Bf: 0  EN:0.0997  Loss: -1.268 0.599 0.625\n",
      "Ep:40   Rew:-118.20  Avg Rew:-118.20  LR:0.00049801   Polyak:0.99900  Bf: 0  EN:0.0996  Loss: 0.122 0.612 0.735\n",
      "Ep:50   Rew:-118.18  Avg Rew:-118.18  LR:0.00049751   Polyak:0.99900  Bf: 1  EN:0.0995  Loss: 0.541 1.082 1.167\n",
      "Ep:60   Rew:-115.05  Avg Rew:-115.05  LR:0.00049702   Polyak:0.99900  Bf: 1  EN:0.0994  Loss: 1.053 0.885 0.971\n",
      "Ep:70   Rew:-104.99  Avg Rew:-104.99  LR:0.00049652   Polyak:0.99900  Bf: 1  EN:0.0993  Loss: 1.556 1.092 1.086\n",
      "Ep:80   Rew:-115.60  Avg Rew:-115.60  LR:0.00049603   Polyak:0.99900  Bf: 1  EN:0.0992  Loss: 1.819 1.775 1.958\n",
      "Ep:90   Rew:-89.25  Avg Rew:-89.25  LR:0.00049554   Polyak:0.99900  Bf: 1  EN:0.0991  Loss: 2.131 1.708 1.837\n",
      "Ep:100   Rew:-123.15  Avg Rew:-123.15  LR:0.00049505   Polyak:0.99900  Bf: 1  EN:0.0990  Loss: 2.559 1.859 2.077\n",
      "Ep:110   Rew:-117.97  Avg Rew:-117.97  LR:0.00049456   Polyak:0.99900  Bf: 1  EN:0.0989  Loss: 3.003 1.347 1.554\n",
      "Ep:120   Rew:-121.19  Avg Rew:-121.19  LR:0.00049407   Polyak:0.99900  Bf: 1  EN:0.0988  Loss: 3.476 0.968 1.122\n",
      "Ep:130   Rew:-109.52  Avg Rew:-109.52  LR:0.00049358   Polyak:0.99900  Bf: 1  EN:0.0987  Loss: 3.865 1.072 1.251\n",
      "Ep:140   Rew:-111.21  Avg Rew:-111.21  LR:0.00049310   Polyak:0.99900  Bf: 1  EN:0.0986  Loss: 3.935 1.061 1.436\n",
      "Ep:150   Rew:-115.55  Avg Rew:-115.55  LR:0.00049261   Polyak:0.99900  Bf: 1  EN:0.0985  Loss: 4.250 0.909 1.052\n",
      "Ep:160   Rew:-113.20  Avg Rew:-113.20  LR:0.00049213   Polyak:0.99900  Bf: 1  EN:0.0984  Loss: 4.597 1.031 1.210\n",
      "Ep:170   Rew:-113.05  Avg Rew:-113.05  LR:0.00049164   Polyak:0.99900  Bf: 1  EN:0.0983  Loss: 4.838 1.149 1.330\n",
      "Ep:180   Rew:-108.40  Avg Rew:-108.40  LR:0.00049116   Polyak:0.99900  Bf: 1  EN:0.0982  Loss: 5.188 1.212 1.305\n",
      "Ep:190   Rew:-108.10  Avg Rew:-108.10  LR:0.00049068   Polyak:0.99900  Bf: 1  EN:0.0981  Loss: 5.422 1.240 1.478\n",
      "Ep:200   Rew:-26.29  Avg Rew:-26.29  LR:0.00049020   Polyak:0.99900  Bf: 1  EN:0.0980  Loss: 5.999 0.746 0.800\n",
      "Ep:210   Rew:-109.31  Avg Rew:-109.31  LR:0.00048972   Polyak:0.99900  Bf: 1  EN:0.0979  Loss: 6.294 0.988 1.134\n",
      "Ep:220   Rew:-40.90  Avg Rew:-40.90  LR:0.00048924   Polyak:0.99900  Bf: 1  EN:0.0978  Loss: 6.995 0.779 0.817\n",
      "Ep:230   Rew:-125.63  Avg Rew:-125.63  LR:0.00048876   Polyak:0.99900  Bf: 1  EN:0.0978  Loss: 7.271 0.899 1.033\n",
      "Ep:240   Rew:-120.81  Avg Rew:-120.81  LR:0.00048828   Polyak:0.99900  Bf: 1  EN:0.0977  Loss: 7.679 0.949 1.095\n",
      "Ep:250   Rew:-117.89  Avg Rew:-117.89  LR:0.00048780   Polyak:0.99900  Bf: 1  EN:0.0976  Loss: 8.044 0.906 0.920\n",
      "Ep:260   Rew:-122.53  Avg Rew:-122.53  LR:0.00048733   Polyak:0.99900  Bf: 1  EN:0.0975  Loss: 8.253 0.953 1.007\n",
      "Ep:270   Rew:-102.50  Avg Rew:-102.50  LR:0.00048685   Polyak:0.99900  Bf: 2  EN:0.0974  Loss: 8.839 0.765 0.772\n",
      "Ep:280   Rew:-94.11  Avg Rew:-94.11  LR:0.00048638   Polyak:0.99900  Bf: 2  EN:0.0973  Loss: 9.206 0.757 0.765\n",
      "Ep:290   Rew:-103.05  Avg Rew:-103.05  LR:0.00048591   Polyak:0.99900  Bf: 2  EN:0.0972  Loss: 9.321 0.794 0.807\n",
      "Ep:300   Rew:-73.15  Avg Rew:-73.15  LR:0.00048544   Polyak:0.99900  Bf: 2  EN:0.0971  Loss: 10.000 0.937 0.958\n",
      "Ep:310   Rew:-114.79  Avg Rew:-114.79  LR:0.00048497   Polyak:0.99900  Bf: 3  EN:0.0970  Loss: 9.699 1.099 1.102\n",
      "Ep:320   Rew:-110.41  Avg Rew:-110.41  LR:0.00048450   Polyak:0.99900  Bf: 3  EN:0.0969  Loss: 9.079 0.978 0.960\n",
      "Ep:330   Rew:-87.65  Avg Rew:-87.65  LR:0.00048403   Polyak:0.99900  Bf: 3  EN:0.0968  Loss: 8.766 1.019 0.973\n",
      "Ep:340   Rew:-111.12  Avg Rew:-111.12  LR:0.00048356   Polyak:0.99900  Bf: 4  EN:0.0967  Loss: 8.750 0.968 0.954\n",
      "Ep:350   Rew:-119.08  Avg Rew:-119.08  LR:0.00048309   Polyak:0.99900  Bf: 4  EN:0.0966  Loss: 8.817 1.036 0.980\n",
      "Ep:360   Rew:-138.27  Avg Rew:-138.27  LR:0.00048263   Polyak:0.99900  Bf: 4  EN:0.0965  Loss: 8.364 1.104 1.116\n",
      "Ep:370   Rew:-108.51  Avg Rew:-108.51  LR:0.00048216   Polyak:0.99900  Bf: 4  EN:0.0964  Loss: 8.273 1.154 1.144\n",
      "Ep:380   Rew:-109.21  Avg Rew:-109.21  LR:0.00048170   Polyak:0.99900  Bf: 4  EN:0.0963  Loss: 8.275 1.162 1.148\n",
      "Ep:390   Rew:-60.73  Avg Rew:-60.73  LR:0.00048123   Polyak:0.99900  Bf: 5  EN:0.0962  Loss: 8.246 1.082 1.095\n",
      "Ep:400   Rew:-123.17  Avg Rew:-123.17  LR:0.00048077   Polyak:0.99900  Bf: 5  EN:0.0962  Loss: 8.402 1.145 1.096\n",
      "Ep:410   Rew:-100.91  Avg Rew:-100.91  LR:0.00048031   Polyak:0.99900  Bf: 5  EN:0.0961  Loss: 8.309 1.149 1.120\n",
      "Ep:420   Rew:-79.10  Avg Rew:-79.10  LR:0.00047985   Polyak:0.99900  Bf: 5  EN:0.0960  Loss: 8.311 1.189 1.161\n",
      "Ep:430   Rew:-74.13  Avg Rew:-74.13  LR:0.00047939   Polyak:0.99900  Bf: 6  EN:0.0959  Loss: 8.546 1.064 1.068\n",
      "Ep:440   Rew:-122.12  Avg Rew:-122.12  LR:0.00047893   Polyak:0.99900  Bf: 6  EN:0.0958  Loss: 8.584 1.096 1.075\n",
      "Ep:450   Rew:-72.39  Avg Rew:-72.39  LR:0.00047847   Polyak:0.99900  Bf: 6  EN:0.0957  Loss: 8.405 0.935 0.922\n",
      "Ep:460   Rew:-91.20  Avg Rew:-91.20  LR:0.00047801   Polyak:0.99900  Bf: 7  EN:0.0956  Loss: 8.099 0.786 0.815\n",
      "Ep:470   Rew:-67.66  Avg Rew:-67.66  LR:0.00047755   Polyak:0.99900  Bf: 7  EN:0.0955  Loss: 7.454 0.697 0.716\n",
      "Ep:480   Rew:-48.09  Avg Rew:-48.09  LR:0.00047710   Polyak:0.99900  Bf: 7  EN:0.0954  Loss: 6.729 0.697 0.656\n",
      "Ep:490   Rew:-77.74  Avg Rew:-77.74  LR:0.00047664   Polyak:0.99900  Bf: 8  EN:0.0953  Loss: 6.023 0.672 0.668\n",
      "Ep:500   Rew:-66.94  Avg Rew:-66.94  LR:0.00047619   Polyak:0.99900  Bf: 8  EN:0.0952  Loss: 5.734 0.799 0.786\n",
      "Ep:510   Rew:-124.00  Avg Rew:-124.00  LR:0.00047574   Polyak:0.99900  Bf: 8  EN:0.0951  Loss: 5.557 0.849 0.795\n",
      "Ep:520   Rew:-123.52  Avg Rew:-123.52  LR:0.00047529   Polyak:0.99900  Bf: 8  EN:0.0951  Loss: 5.482 0.864 0.820\n",
      "Ep:530   Rew:-124.19  Avg Rew:-124.19  LR:0.00047483   Polyak:0.99900  Bf: 8  EN:0.0950  Loss: 5.413 0.938 0.911\n",
      "Ep:540   Rew:-120.65  Avg Rew:-120.65  LR:0.00047438   Polyak:0.99900  Bf: 8  EN:0.0949  Loss: 5.402 1.084 1.049\n",
      "Ep:550   Rew:-120.66  Avg Rew:-120.66  LR:0.00047393   Polyak:0.99900  Bf: 8  EN:0.0948  Loss: 5.404 1.176 1.057\n",
      "Ep:560   Rew:-110.45  Avg Rew:-110.45  LR:0.00047348   Polyak:0.99900  Bf: 8  EN:0.0947  Loss: 5.362 1.419 1.309\n",
      "Ep:570   Rew:-120.93  Avg Rew:-120.93  LR:0.00047304   Polyak:0.99900  Bf: 8  EN:0.0946  Loss: 5.384 1.320 1.295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep:580   Rew:-121.84  Avg Rew:-121.84  LR:0.00047259   Polyak:0.99900  Bf: 8  EN:0.0945  Loss: 5.301 1.553 1.480\n",
      "Ep:590   Rew:-111.87  Avg Rew:-111.87  LR:0.00047214   Polyak:0.99900  Bf: 8  EN:0.0944  Loss: 5.319 1.585 1.558\n",
      "Ep:600   Rew:-121.38  Avg Rew:-121.38  LR:0.00047170   Polyak:0.99900  Bf: 8  EN:0.0943  Loss: 5.275 1.862 1.735\n",
      "Ep:610   Rew:-122.05  Avg Rew:-122.05  LR:0.00047125   Polyak:0.99900  Bf: 8  EN:0.0943  Loss: 5.356 1.716 1.721\n",
      "Ep:620   Rew:-115.97  Avg Rew:-115.97  LR:0.00047081   Polyak:0.99900  Bf: 8  EN:0.0942  Loss: 5.333 1.902 1.781\n",
      "Ep:630   Rew:-120.49  Avg Rew:-120.49  LR:0.00047037   Polyak:0.99900  Bf: 8  EN:0.0941  Loss: 5.306 1.614 1.479\n",
      "Ep:640   Rew:-122.26  Avg Rew:-122.26  LR:0.00046992   Polyak:0.99900  Bf: 8  EN:0.0940  Loss: 5.269 1.731 1.669\n",
      "Ep:650   Rew:-119.16  Avg Rew:-119.16  LR:0.00046948   Polyak:0.99900  Bf: 8  EN:0.0939  Loss: 5.252 1.713 1.651\n",
      "Ep:660   Rew:-124.08  Avg Rew:-124.08  LR:0.00046904   Polyak:0.99900  Bf: 8  EN:0.0938  Loss: 5.255 1.691 1.603\n",
      "Ep:670   Rew:-124.79  Avg Rew:-124.79  LR:0.00046860   Polyak:0.99900  Bf: 9  EN:0.0937  Loss: 5.164 1.664 1.578\n",
      "Ep:680   Rew:-118.41  Avg Rew:-118.41  LR:0.00046816   Polyak:0.99900  Bf: 9  EN:0.0936  Loss: 5.157 1.442 1.395\n",
      "Ep:690   Rew:-128.51  Avg Rew:-128.51  LR:0.00046773   Polyak:0.99900  Bf: 9  EN:0.0935  Loss: 5.100 1.491 1.477\n",
      "Ep:700   Rew:-124.29  Avg Rew:-124.29  LR:0.00046729   Polyak:0.99900  Bf: 9  EN:0.0935  Loss: 5.070 1.251 1.229\n",
      "Ep:710   Rew:-126.04  Avg Rew:-126.04  LR:0.00046685   Polyak:0.99900  Bf: 9  EN:0.0934  Loss: 5.018 1.359 1.285\n",
      "Ep:720   Rew:-126.69  Avg Rew:-126.69  LR:0.00046642   Polyak:0.99900  Bf: 9  EN:0.0933  Loss: 4.989 1.408 1.344\n",
      "Ep:730   Rew:-116.95  Avg Rew:-116.95  LR:0.00046598   Polyak:0.99900  Bf: 9  EN:0.0932  Loss: 4.907 1.381 1.302\n",
      "Ep:740   Rew:-125.17  Avg Rew:-125.17  LR:0.00046555   Polyak:0.99900  Bf: 9  EN:0.0931  Loss: 4.902 1.295 1.282\n",
      "Ep:750   Rew:-125.01  Avg Rew:-125.01  LR:0.00046512   Polyak:0.99900  Bf: 9  EN:0.0930  Loss: 4.916 1.355 1.380\n",
      "Ep:760   Rew:-125.34  Avg Rew:-125.34  LR:0.00046468   Polyak:0.99900  Bf: 9  EN:0.0929  Loss: 4.759 1.327 1.323\n",
      "Ep:770   Rew:-124.78  Avg Rew:-124.78  LR:0.00046425   Polyak:0.99900  Bf: 9  EN:0.0929  Loss: 4.745 1.364 1.298\n",
      "Ep:780   Rew:-124.89  Avg Rew:-124.89  LR:0.00046382   Polyak:0.99900  Bf: 9  EN:0.0928  Loss: 4.768 1.351 1.337\n",
      "Ep:790   Rew:-125.25  Avg Rew:-125.25  LR:0.00046339   Polyak:0.99900  Bf: 9  EN:0.0927  Loss: 4.708 1.383 1.355\n",
      "Ep:800   Rew:-126.51  Avg Rew:-126.51  LR:0.00046296   Polyak:0.99900  Bf: 9  EN:0.0926  Loss: 4.735 1.416 1.336\n",
      "Ep:810   Rew:-124.16  Avg Rew:-124.16  LR:0.00046253   Polyak:0.99900  Bf: 9  EN:0.0925  Loss: 4.642 1.339 1.257\n",
      "Ep:820   Rew:-124.67  Avg Rew:-124.67  LR:0.00046211   Polyak:0.99900  Bf: 9  EN:0.0924  Loss: 4.572 1.386 1.371\n",
      "Ep:830   Rew:-124.05  Avg Rew:-124.05  LR:0.00046168   Polyak:0.99900  Bf: 9  EN:0.0923  Loss: 4.656 1.319 1.286\n",
      "Ep:840   Rew:-123.96  Avg Rew:-123.96  LR:0.00046125   Polyak:0.99900  Bf: 9  EN:0.0923  Loss: 4.477 1.268 1.248\n",
      "Ep:850   Rew:-124.91  Avg Rew:-124.91  LR:0.00046083   Polyak:0.99900  Bf: 9  EN:0.0922  Loss: 4.457 1.284 1.176\n",
      "Ep:860   Rew:-123.52  Avg Rew:-123.52  LR:0.00046041   Polyak:0.99900  Bf: 9  EN:0.0921  Loss: 4.538 1.287 1.245\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d712ea419883>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                    \u001b[0mmax_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_timesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_buffer_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_buffer_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                    log_interval=log_interval)\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-08cddeeffad2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;31m# select action and add exploration noise:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexploration_noise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_low\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_high\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/gym/bipedal_walker/TD3/td3.py\u001b[0m in \u001b[0;36mselect_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolyak\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_noise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_clip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_delay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/gym/bipedal_walker/TD3/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_range\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_low\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = TD3Trainer(env_name, actor_config, critic_config, random_seed=random_seed, lr_base=lr_base, lr_decay=lr_decay, \n",
    "                   exp_noise_base=exp_noise_base, exp_noise_decay=exp_noise_decay, gamma=gamma, batch_size=batch_size,\n",
    "                   polyak=polyak, policy_noise=policy_noise, noise_clip=noise_clip, policy_delay=policy_delay, \n",
    "                   max_episodes=max_episodes, max_timesteps=max_timesteps, max_buffer_length=max_buffer_length, \n",
    "                   log_interval=log_interval)\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
