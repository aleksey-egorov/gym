{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "action_space=Box(4,)\n",
      "obs_space=Box(24,)\n",
      "threshold=300 \n",
      "\n",
      "Episode 0\tLast length:   731\t Reward: -143.03\t Avg Reward:   -7.15\n",
      "Episode 20\tLast length:   287\t Reward: -129.52\t Avg Reward: -110.86\n",
      "Episode 40\tLast length:   661\t Reward: -136.48\t Avg Reward: -112.61\n",
      "Episode 60\tLast length:   169\t Reward: -100.64\t Avg Reward: -111.70\n",
      "Episode 80\tLast length:    71\t Reward: -103.18\t Avg Reward: -110.22\n",
      "Episode 100\tLast length:    46\t Reward: -102.24\t Avg Reward: -105.24\n",
      "Episode 120\tLast length:    83\t Reward: -121.83\t Avg Reward: -117.94\n",
      "Episode 140\tLast length:    52\t Reward: -116.26\t Avg Reward: -115.47\n",
      "Episode 160\tLast length:   144\t Reward: -121.31\t Avg Reward: -114.23\n",
      "Episode 180\tLast length:    74\t Reward:  -95.58\t Avg Reward: -110.36\n",
      "Episode 200\tLast length:    60\t Reward: -115.28\t Avg Reward: -112.93\n",
      "Episode 220\tLast length:    81\t Reward: -101.16\t Avg Reward: -109.66\n",
      "Episode 240\tLast length:    73\t Reward: -124.11\t Avg Reward: -108.14\n",
      "Episode 260\tLast length:    75\t Reward:  -94.67\t Avg Reward: -110.66\n",
      "Episode 280\tLast length:    58\t Reward: -105.86\t Avg Reward: -114.47\n",
      "Episode 300\tLast length:    64\t Reward:  -99.82\t Avg Reward: -112.93\n",
      "Episode 320\tLast length:  1147\t Reward: -121.70\t Avg Reward: -116.27\n",
      "Episode 340\tLast length:   103\t Reward: -109.19\t Avg Reward: -110.57\n",
      "Episode 360\tLast length:    69\t Reward:  -98.86\t Avg Reward: -118.50\n",
      "Episode 380\tLast length:    34\t Reward: -107.54\t Avg Reward: -113.45\n",
      "Episode 400\tLast length:    58\t Reward: -102.50\t Avg Reward: -112.08\n",
      "Episode 420\tLast length:    65\t Reward: -106.77\t Avg Reward: -108.60\n",
      "Episode 440\tLast length:    89\t Reward:  -92.61\t Avg Reward: -110.81\n",
      "Episode 460\tLast length:    77\t Reward: -104.73\t Avg Reward: -112.20\n",
      "Episode 480\tLast length:    46\t Reward: -107.33\t Avg Reward: -106.10\n",
      "Episode 500\tLast length:   460\t Reward: -114.59\t Avg Reward: -109.62\n",
      "Episode 520\tLast length:    73\t Reward:  -98.12\t Avg Reward: -115.71\n",
      "Episode 540\tLast length:    65\t Reward: -100.14\t Avg Reward: -107.20\n",
      "Episode 560\tLast length:    43\t Reward: -110.42\t Avg Reward: -106.84\n",
      "Episode 580\tLast length:   143\t Reward: -106.15\t Avg Reward: -111.29\n",
      "Episode 600\tLast length:   415\t Reward: -135.29\t Avg Reward:  -98.93\n",
      "Episode 620\tLast length:    63\t Reward: -101.42\t Avg Reward: -110.75\n",
      "Episode 640\tLast length:   100\t Reward: -112.61\t Avg Reward: -113.97\n",
      "Episode 660\tLast length:    62\t Reward: -100.09\t Avg Reward: -110.51\n",
      "Episode 680\tLast length:   169\t Reward: -104.55\t Avg Reward: -112.90\n",
      "Episode 700\tLast length:   128\t Reward: -100.78\t Avg Reward: -113.08\n",
      "Episode 720\tLast length:    66\t Reward: -105.30\t Avg Reward: -119.06\n",
      "Episode 740\tLast length:   144\t Reward: -119.46\t Avg Reward: -110.94\n",
      "Episode 760\tLast length:   651\t Reward: -110.49\t Avg Reward: -115.67\n",
      "Episode 780\tLast length:    65\t Reward:  -98.15\t Avg Reward: -109.88\n",
      "Episode 800\tLast length:    89\t Reward:  -96.25\t Avg Reward: -115.01\n",
      "Episode 820\tLast length:    92\t Reward: -116.41\t Avg Reward: -112.27\n",
      "Episode 840\tLast length:   346\t Reward: -127.77\t Avg Reward: -116.71\n",
      "Episode 860\tLast length:    86\t Reward:  -99.97\t Avg Reward: -108.30\n",
      "Episode 880\tLast length:   858\t Reward: -140.06\t Avg Reward: -114.05\n",
      "Episode 900\tLast length:    98\t Reward:  -98.40\t Avg Reward: -112.07\n",
      "Episode 920\tLast length:    66\t Reward:  -99.05\t Avg Reward: -111.18\n",
      "Episode 940\tLast length:   347\t Reward: -129.60\t Avg Reward: -110.12\n",
      "Episode 960\tLast length:   498\t Reward: -138.76\t Avg Reward: -113.73\n",
      "Episode 980\tLast length:   111\t Reward:  -99.71\t Avg Reward: -110.62\n",
      "Episode 1000\tLast length:  1014\t Reward: -137.87\t Avg Reward: -113.71\n",
      "Episode 1020\tLast length:   111\t Reward:  -92.45\t Avg Reward:  -92.50\n",
      "Episode 1040\tLast length:   112\t Reward:  -92.54\t Avg Reward:  -92.55\n",
      "Episode 1060\tLast length:   117\t Reward:  -92.61\t Avg Reward:  -92.56\n",
      "Episode 1080\tLast length:   114\t Reward:  -92.53\t Avg Reward:  -92.54\n",
      "Episode 1100\tLast length:   113\t Reward:  -92.61\t Avg Reward:  -92.53\n",
      "Episode 1120\tLast length:   115\t Reward:  -92.61\t Avg Reward:  -92.55\n",
      "Episode 1140\tLast length:   118\t Reward:  -92.69\t Avg Reward:  -92.52\n",
      "Episode 1160\tLast length:   114\t Reward:  -92.60\t Avg Reward:  -92.55\n",
      "Episode 1180\tLast length:   113\t Reward:  -92.54\t Avg Reward:  -92.53\n",
      "Episode 1200\tLast length:   115\t Reward:  -92.47\t Avg Reward:  -92.55\n",
      "Episode 1220\tLast length:   117\t Reward:  -92.62\t Avg Reward:  -92.54\n",
      "Episode 1240\tLast length:   115\t Reward:  -92.55\t Avg Reward:  -92.56\n",
      "Episode 1260\tLast length:   113\t Reward:  -92.39\t Avg Reward:  -92.51\n",
      "Episode 1280\tLast length:   115\t Reward:  -92.60\t Avg Reward:  -92.56\n",
      "Episode 1300\tLast length:   112\t Reward:  -92.51\t Avg Reward:  -92.57\n",
      "Episode 1320\tLast length:   113\t Reward:  -92.47\t Avg Reward:  -92.53\n",
      "Episode 1340\tLast length:   108\t Reward:  -92.53\t Avg Reward:  -92.49\n",
      "Episode 1360\tLast length:   111\t Reward:  -92.56\t Avg Reward:  -92.55\n",
      "Episode 1380\tLast length:   116\t Reward:  -92.45\t Avg Reward:  -92.52\n",
      "Episode 1400\tLast length:   112\t Reward:  -92.66\t Avg Reward:  -92.53\n",
      "Episode 1420\tLast length:   111\t Reward:  -92.42\t Avg Reward:  -92.52\n",
      "Episode 1440\tLast length:   114\t Reward:  -92.50\t Avg Reward:  -92.55\n",
      "Episode 1460\tLast length:   111\t Reward:  -92.41\t Avg Reward:  -92.56\n",
      "Episode 1480\tLast length:   116\t Reward:  -92.68\t Avg Reward:  -92.52\n",
      "Episode 1500\tLast length:   114\t Reward:  -92.41\t Avg Reward:  -92.50\n",
      "Episode 1520\tLast length:   114\t Reward:  -92.69\t Avg Reward:  -92.54\n",
      "Episode 1540\tLast length:   116\t Reward:  -92.56\t Avg Reward:  -92.51\n",
      "Episode 1560\tLast length:   115\t Reward:  -92.60\t Avg Reward:  -92.52\n",
      "Episode 1580\tLast length:   112\t Reward:  -92.53\t Avg Reward:  -92.54\n",
      "Episode 1600\tLast length:   115\t Reward:  -92.57\t Avg Reward:  -92.50\n",
      "Episode 1620\tLast length:   113\t Reward:  -92.61\t Avg Reward:  -92.56\n",
      "Episode 1640\tLast length:   118\t Reward:  -92.71\t Avg Reward:  -92.55\n",
      "Episode 1660\tLast length:   108\t Reward:  -92.60\t Avg Reward:  -92.57\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2c3c3650d5fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"threshold={} \\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-2c3c3650d5fc>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Don't infinite loop while learning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_episode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'render'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/learning/udacity/Deep Learning Nanodegree/gym-master/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/learning/udacity/Deep Learning Nanodegree/gym-master/gym/envs/box2d/bipedal_walker.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxMotorTorque\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMOTORS_TORQUE\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mFPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhull\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "\n",
    "from DDPG.ddpg import DDPG\n",
    "\n",
    "args = {\n",
    "    'gamma': 0.99,\n",
    "    'render': True,\n",
    "    'log_interval': 20\n",
    "}\n",
    "\n",
    "env = gym.make('BipedalWalker-v2')\n",
    "episodes = 100000\n",
    "print(\"action_space={}\".format(env.action_space))\n",
    "print(\"obs_space={}\".format(env.observation_space))\n",
    "\n",
    "\n",
    "def main():   \n",
    "    task = {\n",
    "        'state_size': 24,\n",
    "        'action_size': 4,\n",
    "        'action_high': 1,\n",
    "        'action_low': -1\n",
    "    }\n",
    "    agent = DDPG(task)\n",
    "    sum_reward = 0   \n",
    "    for i_episode in range(episodes):\n",
    "        running_reward = 0        \n",
    "        state = env.reset()\n",
    "        for t in range(10000):  # Don't infinite loop while learning\n",
    "            action = agent.act(state, i_episode)                \n",
    "            state, reward, done, _ = env.step(action)            \n",
    "            if args['render']:\n",
    "                env.render()                   \n",
    "            running_reward += reward            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        sum_reward += running_reward\n",
    "        \n",
    "        if i_episode % args['log_interval'] == 0:\n",
    "            avg_reward = sum_reward / args['log_interval']\n",
    "            sum_reward = 0\n",
    "            print('Episode {}\\tLast length: {:5d}\\t Reward: {:7.2f}\\t Avg Reward: {:7.2f}'.format(\n",
    "                i_episode, t, running_reward, avg_reward))\n",
    "        if running_reward > env.spec.reward_threshold:\n",
    "            print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "            break\n",
    "\n",
    "print(\"threshold={} \\n\".format(env.spec.reward_threshold))\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
