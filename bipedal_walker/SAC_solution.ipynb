{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: <class 'SAC.normalized_actions.NormalizedActions'> doesn't implement 'action' method. Maybe it implements deprecated '_action' method.\u001b[0m\n",
      "Episode: 0, total numsteps: 1600, reward: -67.62, average reward: -67.62\n",
      "Episode: 1, total numsteps: 3200, reward: -68.79, average reward: -68.21\n",
      "Episode: 2, total numsteps: 3252, reward: -110.21, average reward: -82.21\n",
      "Episode: 3, total numsteps: 3311, reward: -113.16, average reward: -89.95\n",
      "Episode: 4, total numsteps: 3395, reward: -119.08, average reward: -95.77\n",
      "Episode: 5, total numsteps: 3465, reward: -105.71, average reward: -97.43\n",
      "Episode: 6, total numsteps: 3536, reward: -124.84, average reward: -101.34\n",
      "Episode: 7, total numsteps: 3671, reward: -100.81, average reward: -101.28\n",
      "Episode: 8, total numsteps: 3737, reward: -103.12, average reward: -101.48\n",
      "Episode: 9, total numsteps: 3795, reward: -100.83, average reward: -101.42\n",
      "Episode: 10, total numsteps: 3847, reward: -101.2, average reward: -101.4\n",
      "Episode: 11, total numsteps: 3915, reward: -105.87, average reward: -101.77\n",
      "Episode: 12, total numsteps: 3973, reward: -103.92, average reward: -101.94\n",
      "Episode: 13, total numsteps: 5573, reward: -98.7, average reward: -101.7\n",
      "Episode: 14, total numsteps: 7173, reward: -70.3, average reward: -99.61\n",
      "Episode: 15, total numsteps: 8773, reward: -77.42, average reward: -98.22\n",
      "Episode: 16, total numsteps: 10373, reward: -67.33, average reward: -96.41\n",
      "Episode: 17, total numsteps: 11973, reward: -74.88, average reward: -95.21\n",
      "Episode: 18, total numsteps: 13573, reward: -75.18, average reward: -94.16\n",
      "Episode: 19, total numsteps: 13699, reward: -125.91, average reward: -95.74\n",
      "Episode: 20, total numsteps: 13778, reward: -106.77, average reward: -96.27\n",
      "Episode: 21, total numsteps: 13832, reward: -107.26, average reward: -96.77\n",
      "Episode: 22, total numsteps: 13899, reward: -111.3, average reward: -97.4\n",
      "Episode: 23, total numsteps: 13969, reward: -105.85, average reward: -97.75\n",
      "Episode: 24, total numsteps: 15569, reward: -81.6, average reward: -97.11\n",
      "Episode: 25, total numsteps: 15631, reward: -104.42, average reward: -97.39\n",
      "Episode: 26, total numsteps: 15698, reward: -105.48, average reward: -97.69\n",
      "Episode: 27, total numsteps: 15764, reward: -104.1, average reward: -97.92\n",
      "Episode: 28, total numsteps: 15845, reward: -108.66, average reward: -98.29\n",
      "Episode: 29, total numsteps: 15949, reward: -117.29, average reward: -98.92\n",
      "Episode: 30, total numsteps: 16023, reward: -108.8, average reward: -99.24\n",
      "Episode: 31, total numsteps: 16078, reward: -104.7, average reward: -99.41\n",
      "Episode: 32, total numsteps: 16159, reward: -100.73, average reward: -99.45\n",
      "Episode: 33, total numsteps: 16210, reward: -103.29, average reward: -99.56\n",
      "Episode: 34, total numsteps: 16288, reward: -100.63, average reward: -99.59\n",
      "Episode: 35, total numsteps: 16761, reward: -129.07, average reward: -100.41\n",
      "Episode: 36, total numsteps: 17013, reward: -111.78, average reward: -100.72\n",
      "Episode: 37, total numsteps: 18613, reward: -69.35, average reward: -99.89\n",
      "Episode: 38, total numsteps: 18730, reward: -106.33, average reward: -100.06\n",
      "Episode: 39, total numsteps: 18792, reward: -99.32, average reward: -100.04\n",
      "Episode: 40, total numsteps: 20392, reward: -76.88, average reward: -99.48\n",
      "Episode: 41, total numsteps: 20491, reward: -101.91, average reward: -99.53\n",
      "Episode: 42, total numsteps: 20551, reward: -99.73, average reward: -99.54\n",
      "Episode: 43, total numsteps: 20602, reward: -103.78, average reward: -99.64\n",
      "Episode: 44, total numsteps: 20676, reward: -102.33, average reward: -99.7\n",
      "Episode: 45, total numsteps: 20755, reward: -99.96, average reward: -99.7\n",
      "Episode: 46, total numsteps: 20823, reward: -105.35, average reward: -99.82\n",
      "Episode: 47, total numsteps: 20885, reward: -101.01, average reward: -99.85\n",
      "Episode: 48, total numsteps: 20990, reward: -126.6, average reward: -100.39\n",
      "Episode: 49, total numsteps: 21140, reward: -101.75, average reward: -100.42\n",
      "Episode: 50, total numsteps: 21197, reward: -102.14, average reward: -100.45\n",
      "Episode: 51, total numsteps: 21248, reward: -102.18, average reward: -100.49\n",
      "Episode: 52, total numsteps: 21308, reward: -124.98, average reward: -100.95\n",
      "Episode: 53, total numsteps: 21408, reward: -129.56, average reward: -101.48\n",
      "Episode: 54, total numsteps: 21482, reward: -105.64, average reward: -101.55\n",
      "Episode: 55, total numsteps: 21537, reward: -101.03, average reward: -101.54\n",
      "Episode: 56, total numsteps: 21650, reward: -128.26, average reward: -102.01\n",
      "Episode: 57, total numsteps: 21698, reward: -102.82, average reward: -102.03\n",
      "Episode: 58, total numsteps: 21759, reward: -120.57, average reward: -102.34\n",
      "Episode: 59, total numsteps: 21828, reward: -97.04, average reward: -102.25\n",
      "Episode: 60, total numsteps: 21883, reward: -102.2, average reward: -102.25\n",
      "Episode: 61, total numsteps: 22048, reward: -99.9, average reward: -102.21\n",
      "Episode: 62, total numsteps: 22183, reward: -138.04, average reward: -102.78\n",
      "Episode: 63, total numsteps: 22264, reward: -98.1, average reward: -102.71\n",
      "Episode: 64, total numsteps: 22332, reward: -99.33, average reward: -102.66\n",
      "Episode: 65, total numsteps: 22427, reward: -122.02, average reward: -102.95\n",
      "Episode: 66, total numsteps: 22544, reward: -130.7, average reward: -103.37\n",
      "Episode: 67, total numsteps: 22614, reward: -103.46, average reward: -103.37\n",
      "Episode: 68, total numsteps: 22742, reward: -99.16, average reward: -103.31\n",
      "Episode: 69, total numsteps: 22823, reward: -125.87, average reward: -103.63\n",
      "Episode: 70, total numsteps: 22943, reward: -133.72, average reward: -104.05\n",
      "Episode: 71, total numsteps: 22996, reward: -100.25, average reward: -104.0\n",
      "Episode: 72, total numsteps: 23079, reward: -120.86, average reward: -104.23\n",
      "Episode: 73, total numsteps: 23191, reward: -131.49, average reward: -104.6\n",
      "Episode: 74, total numsteps: 23292, reward: -104.23, average reward: -104.59\n",
      "Episode: 75, total numsteps: 23363, reward: -97.87, average reward: -104.51\n",
      "Episode: 76, total numsteps: 23466, reward: -102.26, average reward: -104.48\n",
      "Episode: 77, total numsteps: 23541, reward: -100.96, average reward: -104.43\n",
      "Episode: 78, total numsteps: 23615, reward: -100.45, average reward: -104.38\n",
      "Episode: 79, total numsteps: 23692, reward: -121.65, average reward: -104.6\n",
      "Episode: 80, total numsteps: 23819, reward: -132.24, average reward: -104.94\n",
      "Episode: 81, total numsteps: 23918, reward: -98.65, average reward: -104.86\n",
      "Episode: 82, total numsteps: 24005, reward: -104.24, average reward: -104.85\n",
      "Episode: 83, total numsteps: 24103, reward: -102.72, average reward: -104.83\n",
      "Episode: 84, total numsteps: 24167, reward: -100.53, average reward: -104.78\n",
      "Episode: 85, total numsteps: 24400, reward: -136.61, average reward: -105.15\n",
      "Episode: 86, total numsteps: 24493, reward: -121.38, average reward: -105.33\n",
      "Episode: 87, total numsteps: 24563, reward: -102.18, average reward: -105.3\n",
      "Episode: 88, total numsteps: 24644, reward: -100.27, average reward: -105.24\n",
      "Episode: 89, total numsteps: 24765, reward: -126.39, average reward: -105.48\n",
      "Episode: 90, total numsteps: 24940, reward: -134.48, average reward: -105.8\n",
      "Episode: 91, total numsteps: 25090, reward: -102.13, average reward: -105.76\n",
      "Episode: 92, total numsteps: 25286, reward: -136.83, average reward: -106.09\n",
      "Episode: 93, total numsteps: 25346, reward: -100.77, average reward: -106.03\n",
      "Episode: 94, total numsteps: 25445, reward: -126.45, average reward: -106.25\n",
      "Episode: 95, total numsteps: 25544, reward: -123.91, average reward: -106.43\n",
      "Episode: 96, total numsteps: 25616, reward: -102.73, average reward: -106.39\n",
      "Episode: 97, total numsteps: 25706, reward: -125.15, average reward: -106.59\n",
      "Episode: 98, total numsteps: 25828, reward: -128.93, average reward: -106.81\n",
      "Episode: 99, total numsteps: 25973, reward: -124.25, average reward: -106.99\n",
      "Episode: 100, total numsteps: 26044, reward: -98.19, average reward: -107.29\n",
      "Episode: 101, total numsteps: 27644, reward: -87.44, average reward: -107.48\n",
      "Episode: 102, total numsteps: 29244, reward: -89.67, average reward: -107.27\n",
      "Episode: 103, total numsteps: 30844, reward: -88.73, average reward: -107.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 104, total numsteps: 32444, reward: -91.9, average reward: -106.76\n",
      "Episode: 105, total numsteps: 33644, reward: -182.82, average reward: -107.53\n",
      "Episode: 106, total numsteps: 34650, reward: -174.35, average reward: -108.02\n",
      "Episode: 107, total numsteps: 35826, reward: -180.86, average reward: -108.82\n",
      "Episode: 108, total numsteps: 35907, reward: -101.31, average reward: -108.8\n",
      "Episode: 109, total numsteps: 36914, reward: -174.96, average reward: -109.55\n",
      "Episode: 110, total numsteps: 37700, reward: -163.44, average reward: -110.17\n",
      "Episode: 111, total numsteps: 38361, reward: -157.82, average reward: -110.69\n",
      "Episode: 112, total numsteps: 39100, reward: -162.08, average reward: -111.27\n",
      "Episode: 113, total numsteps: 39823, reward: -162.1, average reward: -111.9\n",
      "Episode: 114, total numsteps: 41131, reward: -192.09, average reward: -113.12\n",
      "Episode: 115, total numsteps: 41812, reward: -158.02, average reward: -113.93\n",
      "Episode: 116, total numsteps: 41912, reward: -102.23, average reward: -114.28\n",
      "Episode: 117, total numsteps: 41996, reward: -103.55, average reward: -114.56\n",
      "Episode: 118, total numsteps: 42550, reward: -152.87, average reward: -115.34\n",
      "Episode: 119, total numsteps: 42599, reward: -106.81, average reward: -115.15\n",
      "Episode: 120, total numsteps: 42709, reward: -108.16, average reward: -115.16\n",
      "Episode: 121, total numsteps: 42768, reward: -106.48, average reward: -115.16\n",
      "Episode: 122, total numsteps: 42881, reward: -107.42, average reward: -115.12\n",
      "Episode: 123, total numsteps: 43582, reward: -158.77, average reward: -115.65\n",
      "Episode: 124, total numsteps: 44160, reward: -152.91, average reward: -116.36\n",
      "Episode: 125, total numsteps: 44789, reward: -155.17, average reward: -116.87\n",
      "Episode: 126, total numsteps: 45571, reward: -164.61, average reward: -117.46\n",
      "Episode: 127, total numsteps: 46530, reward: -172.78, average reward: -118.14\n",
      "Episode: 128, total numsteps: 47369, reward: -167.48, average reward: -118.73\n",
      "Episode: 129, total numsteps: 48114, reward: -161.11, average reward: -119.17\n",
      "Episode: 130, total numsteps: 48875, reward: -161.65, average reward: -119.7\n",
      "Episode: 131, total numsteps: 50027, reward: -182.48, average reward: -120.48\n",
      "Episode: 132, total numsteps: 51426, reward: -194.76, average reward: -121.42\n",
      "Episode: 133, total numsteps: 52541, reward: -181.57, average reward: -122.2\n",
      "Episode: 134, total numsteps: 53974, reward: -197.54, average reward: -123.17\n",
      "Episode: 135, total numsteps: 55082, reward: -180.12, average reward: -123.68\n",
      "Episode: 136, total numsteps: 56682, reward: -203.83, average reward: -124.6\n",
      "Episode: 137, total numsteps: 56771, reward: -97.93, average reward: -124.89\n",
      "Episode: 138, total numsteps: 58371, reward: -99.69, average reward: -124.82\n",
      "Episode: 139, total numsteps: 59971, reward: -98.45, average reward: -124.81\n",
      "Episode: 140, total numsteps: 61571, reward: -98.44, average reward: -125.03\n",
      "Episode: 141, total numsteps: 61638, reward: -101.79, average reward: -125.02\n",
      "Episode: 142, total numsteps: 61707, reward: -104.2, average reward: -125.07\n",
      "Episode: 143, total numsteps: 61776, reward: -100.08, average reward: -125.03\n",
      "Episode: 144, total numsteps: 63376, reward: -101.02, average reward: -125.02\n",
      "Episode: 145, total numsteps: 64976, reward: -95.07, average reward: -124.97\n",
      "Episode: 146, total numsteps: 66576, reward: -90.74, average reward: -124.82\n",
      "Episode: 147, total numsteps: 68176, reward: -95.25, average reward: -124.77\n",
      "Episode: 148, total numsteps: 68291, reward: -125.73, average reward: -124.76\n",
      "Episode: 149, total numsteps: 69891, reward: -89.48, average reward: -124.64\n",
      "Episode: 150, total numsteps: 71491, reward: -95.64, average reward: -124.57\n",
      "Episode: 151, total numsteps: 73091, reward: -92.83, average reward: -124.48\n",
      "Episode: 152, total numsteps: 74691, reward: -97.82, average reward: -124.21\n",
      "Episode: 153, total numsteps: 76291, reward: -89.25, average reward: -123.8\n",
      "Episode: 154, total numsteps: 77891, reward: -97.71, average reward: -123.72\n",
      "Episode: 155, total numsteps: 79491, reward: -89.55, average reward: -123.61\n",
      "Episode: 156, total numsteps: 81091, reward: -90.76, average reward: -123.23\n",
      "Episode: 157, total numsteps: 82691, reward: -91.67, average reward: -123.12\n",
      "Episode: 158, total numsteps: 82755, reward: -102.76, average reward: -122.94\n",
      "Episode: 159, total numsteps: 84355, reward: -87.46, average reward: -122.85\n",
      "Episode: 160, total numsteps: 85955, reward: -101.65, average reward: -122.84\n",
      "Episode: 161, total numsteps: 87555, reward: -96.34, average reward: -122.81\n",
      "Episode: 162, total numsteps: 87617, reward: -101.22, average reward: -122.44\n",
      "Episode: 163, total numsteps: 87715, reward: -99.79, average reward: -122.46\n",
      "Episode: 164, total numsteps: 89315, reward: -88.59, average reward: -122.35\n",
      "Episode: 165, total numsteps: 90915, reward: -93.55, average reward: -122.06\n",
      "Episode: 166, total numsteps: 90996, reward: -97.12, average reward: -121.73\n",
      "Episode: 167, total numsteps: 92596, reward: -97.41, average reward: -121.67\n",
      "Episode: 168, total numsteps: 94196, reward: -89.38, average reward: -121.57\n",
      "Episode: 169, total numsteps: 95796, reward: -90.89, average reward: -121.22\n",
      "Episode: 170, total numsteps: 97396, reward: -98.9, average reward: -120.87\n",
      "Episode: 171, total numsteps: 98996, reward: -87.17, average reward: -120.74\n",
      "Episode: 172, total numsteps: 100596, reward: -95.68, average reward: -120.49\n",
      "Episode: 173, total numsteps: 100666, reward: -97.49, average reward: -120.15\n",
      "Episode: 174, total numsteps: 102266, reward: -88.87, average reward: -119.99\n",
      "Episode: 175, total numsteps: 102352, reward: -98.27, average reward: -120.0\n",
      "Episode: 176, total numsteps: 103952, reward: -95.75, average reward: -119.93\n",
      "Episode: 177, total numsteps: 104035, reward: -96.96, average reward: -119.89\n",
      "Episode: 178, total numsteps: 104095, reward: -116.13, average reward: -120.05\n",
      "Episode: 179, total numsteps: 104190, reward: -103.56, average reward: -119.87\n",
      "Episode: 180, total numsteps: 104308, reward: -109.32, average reward: -119.64\n",
      "Episode: 181, total numsteps: 105908, reward: -96.82, average reward: -119.62\n",
      "Episode: 182, total numsteps: 107508, reward: -93.92, average reward: -119.52\n",
      "Episode: 183, total numsteps: 107583, reward: -100.32, average reward: -119.49\n",
      "Episode: 184, total numsteps: 109183, reward: -85.2, average reward: -119.34\n",
      "Episode: 185, total numsteps: 109286, reward: -100.68, average reward: -118.98\n",
      "Episode: 186, total numsteps: 110886, reward: -83.41, average reward: -118.6\n",
      "Episode: 187, total numsteps: 112486, reward: -79.84, average reward: -118.38\n",
      "Episode: 188, total numsteps: 112573, reward: -101.61, average reward: -118.39\n",
      "Episode: 189, total numsteps: 114173, reward: -76.48, average reward: -117.89\n",
      "Episode: 190, total numsteps: 115773, reward: -89.69, average reward: -117.45\n",
      "Episode: 191, total numsteps: 117373, reward: -84.53, average reward: -117.27\n",
      "Episode: 192, total numsteps: 118973, reward: -66.33, average reward: -116.56\n",
      "Episode: 193, total numsteps: 120573, reward: -75.19, average reward: -116.31\n",
      "Episode: 194, total numsteps: 122173, reward: -71.0, average reward: -115.75\n",
      "Episode: 195, total numsteps: 123773, reward: -80.73, average reward: -115.32\n",
      "Episode: 196, total numsteps: 125373, reward: -72.5, average reward: -115.02\n",
      "Episode: 197, total numsteps: 125480, reward: -108.32, average reward: -114.85\n",
      "Episode: 198, total numsteps: 125576, reward: -101.94, average reward: -114.58\n",
      "Episode: 199, total numsteps: 127176, reward: -73.3, average reward: -114.07\n",
      "Episode: 200, total numsteps: 127251, reward: -102.46, average reward: -114.12\n",
      "Episode: 201, total numsteps: 127323, reward: -99.81, average reward: -114.24\n",
      "Episode: 202, total numsteps: 128923, reward: -67.99, average reward: -114.02\n",
      "Episode: 203, total numsteps: 129032, reward: -100.05, average reward: -114.14\n",
      "Episode: 204, total numsteps: 129099, reward: -99.85, average reward: -114.21\n",
      "Episode: 205, total numsteps: 130699, reward: -73.71, average reward: -113.12\n",
      "Episode: 206, total numsteps: 130800, reward: -102.74, average reward: -112.41\n",
      "Episode: 207, total numsteps: 130887, reward: -96.66, average reward: -111.57\n",
      "Episode: 208, total numsteps: 130963, reward: -100.61, average reward: -111.56\n",
      "Episode: 209, total numsteps: 132563, reward: -77.36, average reward: -110.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 210, total numsteps: 134163, reward: -80.15, average reward: -109.75\n",
      "Episode: 211, total numsteps: 135763, reward: -65.17, average reward: -108.82\n",
      "Episode: 212, total numsteps: 137363, reward: -77.89, average reward: -107.98\n",
      "Episode: 213, total numsteps: 138963, reward: -80.15, average reward: -107.16\n",
      "Episode: 214, total numsteps: 139095, reward: -125.81, average reward: -106.5\n",
      "Episode: 215, total numsteps: 140695, reward: -70.98, average reward: -105.63\n",
      "Episode: 216, total numsteps: 142295, reward: -63.61, average reward: -105.24\n",
      "Episode: 217, total numsteps: 143895, reward: -80.54, average reward: -105.01\n",
      "Episode: 218, total numsteps: 145495, reward: -76.99, average reward: -104.25\n",
      "Episode: 219, total numsteps: 147095, reward: -87.05, average reward: -104.06\n",
      "Episode: 220, total numsteps: 148695, reward: -75.43, average reward: -103.73\n",
      "Episode: 221, total numsteps: 150295, reward: -92.93, average reward: -103.59\n",
      "Episode: 222, total numsteps: 151895, reward: -82.4, average reward: -103.34\n",
      "Episode: 223, total numsteps: 151961, reward: -98.36, average reward: -102.74\n",
      "Episode: 224, total numsteps: 153561, reward: -95.86, average reward: -102.17\n",
      "Episode: 225, total numsteps: 155161, reward: -82.35, average reward: -101.44\n",
      "Episode: 226, total numsteps: 156761, reward: -80.55, average reward: -100.6\n",
      "Episode: 227, total numsteps: 158361, reward: -83.48, average reward: -99.71\n",
      "Episode: 228, total numsteps: 159307, reward: -116.34, average reward: -99.2\n",
      "Episode: 229, total numsteps: 160907, reward: -85.84, average reward: -98.44\n",
      "Episode: 230, total numsteps: 162507, reward: -74.24, average reward: -97.57\n",
      "Episode: 231, total numsteps: 164107, reward: -85.75, average reward: -96.6\n",
      "Episode: 232, total numsteps: 165707, reward: -80.95, average reward: -95.46\n",
      "Episode: 233, total numsteps: 167307, reward: -79.34, average reward: -94.44\n",
      "Episode: 234, total numsteps: 168907, reward: -75.89, average reward: -93.22\n",
      "Episode: 235, total numsteps: 169007, reward: -97.95, average reward: -92.4\n",
      "Episode: 236, total numsteps: 170607, reward: -73.93, average reward: -91.1\n",
      "Episode: 237, total numsteps: 172207, reward: -90.24, average reward: -91.03\n",
      "Episode: 238, total numsteps: 173807, reward: -88.51, average reward: -90.91\n",
      "Episode: 239, total numsteps: 173863, reward: -113.94, average reward: -91.07\n",
      "Episode: 240, total numsteps: 175463, reward: -75.42, average reward: -90.84\n",
      "Episode: 241, total numsteps: 177063, reward: -83.47, average reward: -90.66\n",
      "Episode: 242, total numsteps: 178663, reward: -79.85, average reward: -90.41\n",
      "Episode: 243, total numsteps: 180263, reward: -72.89, average reward: -90.14\n",
      "Episode: 244, total numsteps: 181863, reward: -89.24, average reward: -90.02\n",
      "Episode: 245, total numsteps: 183463, reward: -78.06, average reward: -89.85\n",
      "Episode: 246, total numsteps: 185063, reward: -78.27, average reward: -89.73\n",
      "Episode: 247, total numsteps: 186663, reward: -87.65, average reward: -89.65\n",
      "Episode: 248, total numsteps: 188263, reward: -88.62, average reward: -89.28\n",
      "Episode: 249, total numsteps: 189863, reward: -72.46, average reward: -89.11\n",
      "Episode: 250, total numsteps: 191463, reward: -82.66, average reward: -88.98\n",
      "Episode: 251, total numsteps: 193063, reward: -82.47, average reward: -88.88\n",
      "Episode: 252, total numsteps: 194663, reward: -85.05, average reward: -88.75\n",
      "Episode: 253, total numsteps: 196263, reward: -91.61, average reward: -88.77\n",
      "Episode: 254, total numsteps: 197863, reward: -93.34, average reward: -88.73\n",
      "Episode: 255, total numsteps: 199463, reward: -83.74, average reward: -88.67\n",
      "Episode: 256, total numsteps: 201063, reward: -85.54, average reward: -88.62\n",
      "Episode: 257, total numsteps: 202663, reward: -79.97, average reward: -88.5\n",
      "Episode: 258, total numsteps: 204263, reward: -78.93, average reward: -88.26\n",
      "Episode: 259, total numsteps: 205863, reward: -83.45, average reward: -88.22\n",
      "Episode: 260, total numsteps: 207463, reward: -87.47, average reward: -88.08\n",
      "Episode: 261, total numsteps: 209063, reward: -84.1, average reward: -87.96\n",
      "Episode: 262, total numsteps: 210663, reward: -82.17, average reward: -87.77\n",
      "Episode: 263, total numsteps: 212263, reward: -73.85, average reward: -87.51\n",
      "Episode: 264, total numsteps: 213863, reward: -76.78, average reward: -87.39\n",
      "Episode: 265, total numsteps: 215463, reward: -72.3, average reward: -87.18\n",
      "Episode: 266, total numsteps: 217063, reward: -72.69, average reward: -86.93\n",
      "Episode: 267, total numsteps: 218663, reward: -71.48, average reward: -86.68\n",
      "Episode: 268, total numsteps: 220263, reward: -67.14, average reward: -86.45\n",
      "Episode: 269, total numsteps: 221863, reward: -93.22, average reward: -86.48\n",
      "Episode: 270, total numsteps: 223463, reward: -66.35, average reward: -86.15\n",
      "Episode: 271, total numsteps: 225063, reward: -74.23, average reward: -86.02\n",
      "Episode: 272, total numsteps: 226663, reward: -63.38, average reward: -85.7\n",
      "Episode: 273, total numsteps: 228263, reward: -82.36, average reward: -85.55\n",
      "Episode: 274, total numsteps: 229863, reward: -83.94, average reward: -85.5\n",
      "Episode: 275, total numsteps: 231463, reward: -84.25, average reward: -85.36\n",
      "Episode: 276, total numsteps: 233063, reward: -89.63, average reward: -85.3\n",
      "Episode: 277, total numsteps: 234663, reward: -69.88, average reward: -85.03\n",
      "Episode: 278, total numsteps: 236263, reward: -73.77, average reward: -84.6\n",
      "Episode: 279, total numsteps: 237863, reward: -81.32, average reward: -84.38\n",
      "Episode: 280, total numsteps: 239463, reward: -74.93, average reward: -84.04\n",
      "Episode: 281, total numsteps: 241063, reward: -85.66, average reward: -83.92\n",
      "Episode: 282, total numsteps: 242663, reward: -79.76, average reward: -83.78\n",
      "Episode: 283, total numsteps: 242724, reward: -112.57, average reward: -83.91\n",
      "Episode: 284, total numsteps: 244324, reward: -71.55, average reward: -83.77\n",
      "Episode: 285, total numsteps: 245924, reward: -59.51, average reward: -83.36\n",
      "Episode: 286, total numsteps: 247524, reward: -58.19, average reward: -83.1\n",
      "Episode: 287, total numsteps: 249124, reward: -57.58, average reward: -82.88\n",
      "Episode: 288, total numsteps: 250724, reward: -65.87, average reward: -82.52\n",
      "Episode: 289, total numsteps: 252324, reward: -79.04, average reward: -82.55\n",
      "Episode: 290, total numsteps: 253924, reward: -70.74, average reward: -82.36\n",
      "Episode: 291, total numsteps: 255524, reward: -56.83, average reward: -82.08\n",
      "Episode: 292, total numsteps: 255627, reward: -97.42, average reward: -82.39\n",
      "Episode: 293, total numsteps: 257227, reward: -59.22, average reward: -82.23\n",
      "Episode: 294, total numsteps: 258827, reward: -75.46, average reward: -82.28\n",
      "Episode: 295, total numsteps: 260427, reward: -49.73, average reward: -81.97\n",
      "Episode: 296, total numsteps: 262027, reward: -40.7, average reward: -81.65\n",
      "Episode: 297, total numsteps: 263302, reward: -140.92, average reward: -81.98\n",
      "Episode: 298, total numsteps: 264902, reward: -66.62, average reward: -81.62\n",
      "Episode: 299, total numsteps: 266502, reward: -47.13, average reward: -81.36\n",
      "Episode: 300, total numsteps: 268102, reward: -62.81, average reward: -80.97\n",
      "Episode: 301, total numsteps: 269702, reward: -51.37, average reward: -80.48\n",
      "Episode: 302, total numsteps: 271302, reward: -62.24, average reward: -80.42\n",
      "Episode: 303, total numsteps: 272902, reward: -75.18, average reward: -80.18\n",
      "Episode: 304, total numsteps: 274502, reward: -57.85, average reward: -79.76\n",
      "Episode: 305, total numsteps: 276102, reward: -61.42, average reward: -79.63\n",
      "Episode: 306, total numsteps: 277702, reward: -67.07, average reward: -79.28\n",
      "Episode: 307, total numsteps: 279302, reward: -76.16, average reward: -79.07\n",
      "Episode: 308, total numsteps: 280902, reward: -68.3, average reward: -78.75\n",
      "Episode: 309, total numsteps: 282502, reward: -55.8, average reward: -78.53\n",
      "Episode: 310, total numsteps: 284102, reward: -69.62, average reward: -78.43\n",
      "Episode: 311, total numsteps: 285702, reward: -53.42, average reward: -78.31\n",
      "Episode: 312, total numsteps: 287302, reward: -75.28, average reward: -78.28\n",
      "Episode: 313, total numsteps: 287392, reward: -94.97, average reward: -78.43\n",
      "Episode: 314, total numsteps: 288992, reward: -46.5, average reward: -77.64\n",
      "Episode: 315, total numsteps: 290592, reward: -52.64, average reward: -77.45\n",
      "Episode: 316, total numsteps: 292192, reward: -58.79, average reward: -77.41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 317, total numsteps: 293792, reward: -64.95, average reward: -77.25\n",
      "Episode: 318, total numsteps: 295392, reward: -75.83, average reward: -77.24\n",
      "Episode: 319, total numsteps: 296992, reward: -59.61, average reward: -76.96\n",
      "Episode: 320, total numsteps: 298592, reward: -60.05, average reward: -76.81\n",
      "Episode: 321, total numsteps: 298654, reward: -123.4, average reward: -77.12\n",
      "Episode: 322, total numsteps: 300254, reward: -55.69, average reward: -76.85\n",
      "Episode: 323, total numsteps: 301854, reward: -40.56, average reward: -76.27\n",
      "Episode: 324, total numsteps: 303454, reward: -47.23, average reward: -75.78\n",
      "Episode: 325, total numsteps: 305054, reward: -41.26, average reward: -75.37\n",
      "Episode: 326, total numsteps: 306654, reward: -40.16, average reward: -74.97\n",
      "Episode: 327, total numsteps: 308254, reward: -53.34, average reward: -74.67\n",
      "Episode: 328, total numsteps: 309854, reward: -42.28, average reward: -73.93\n",
      "Episode: 329, total numsteps: 311454, reward: -44.38, average reward: -73.51\n",
      "Episode: 330, total numsteps: 313054, reward: -36.09, average reward: -73.13\n",
      "Episode: 331, total numsteps: 314654, reward: -39.0, average reward: -72.66\n",
      "Episode: 332, total numsteps: 316254, reward: -65.68, average reward: -72.51\n",
      "Episode: 333, total numsteps: 317854, reward: -62.07, average reward: -72.34\n",
      "Episode: 334, total numsteps: 319454, reward: -47.89, average reward: -72.06\n",
      "Episode: 335, total numsteps: 321054, reward: -39.14, average reward: -71.47\n",
      "Episode: 336, total numsteps: 322654, reward: -42.91, average reward: -71.16\n",
      "Episode: 337, total numsteps: 324254, reward: -54.97, average reward: -70.81\n",
      "Episode: 338, total numsteps: 325854, reward: -43.1, average reward: -70.35\n",
      "Episode: 339, total numsteps: 327454, reward: -72.82, average reward: -69.94\n",
      "Episode: 340, total numsteps: 329054, reward: -54.99, average reward: -69.74\n",
      "Episode: 341, total numsteps: 330654, reward: -65.52, average reward: -69.56\n",
      "Episode: 342, total numsteps: 332254, reward: -68.59, average reward: -69.45\n",
      "Episode: 343, total numsteps: 332350, reward: -109.4, average reward: -69.81\n",
      "Episode: 344, total numsteps: 333950, reward: -56.15, average reward: -69.48\n",
      "Episode: 345, total numsteps: 335127, reward: -137.39, average reward: -70.07\n",
      "Episode: 346, total numsteps: 335608, reward: -104.7, average reward: -70.34\n",
      "Episode: 347, total numsteps: 337208, reward: -52.84, average reward: -69.99\n",
      "Episode: 348, total numsteps: 338808, reward: -62.53, average reward: -69.73\n",
      "Episode: 349, total numsteps: 338908, reward: -106.63, average reward: -70.07\n",
      "Episode: 350, total numsteps: 340508, reward: -63.1, average reward: -69.87\n",
      "Episode: 351, total numsteps: 340552, reward: -102.7, average reward: -70.08\n",
      "Episode: 352, total numsteps: 342152, reward: -60.4, average reward: -69.83\n",
      "Episode: 353, total numsteps: 343752, reward: -48.5, average reward: -69.4\n",
      "Episode: 354, total numsteps: 345352, reward: -60.04, average reward: -69.07\n",
      "Episode: 355, total numsteps: 346952, reward: -45.71, average reward: -68.69\n",
      "Episode: 356, total numsteps: 348552, reward: -54.08, average reward: -68.37\n",
      "Episode: 357, total numsteps: 350152, reward: -49.09, average reward: -68.06\n",
      "Episode: 358, total numsteps: 351752, reward: -45.52, average reward: -67.73\n",
      "Episode: 359, total numsteps: 353352, reward: -41.1, average reward: -67.3\n",
      "Episode: 360, total numsteps: 353557, reward: -94.98, average reward: -67.38\n",
      "Episode: 361, total numsteps: 355157, reward: -42.74, average reward: -66.97\n",
      "Episode: 362, total numsteps: 356757, reward: -45.94, average reward: -66.6\n",
      "Episode: 363, total numsteps: 358357, reward: -51.37, average reward: -66.38\n",
      "Episode: 364, total numsteps: 359957, reward: -37.01, average reward: -65.98\n",
      "Episode: 365, total numsteps: 360021, reward: -115.82, average reward: -66.42\n",
      "Episode: 366, total numsteps: 360170, reward: -123.52, average reward: -66.93\n",
      "Episode: 367, total numsteps: 361770, reward: -40.68, average reward: -66.62\n",
      "Episode: 368, total numsteps: 363370, reward: -46.97, average reward: -66.42\n",
      "Episode: 369, total numsteps: 363438, reward: -125.97, average reward: -66.74\n",
      "Episode: 370, total numsteps: 365038, reward: -29.66, average reward: -66.38\n",
      "Episode: 371, total numsteps: 366638, reward: -51.0, average reward: -66.14\n",
      "Episode: 372, total numsteps: 367303, reward: -98.47, average reward: -66.49\n",
      "Episode: 373, total numsteps: 368903, reward: -37.51, average reward: -66.05\n",
      "Episode: 374, total numsteps: 370503, reward: -39.59, average reward: -65.6\n",
      "Episode: 375, total numsteps: 372103, reward: -44.5, average reward: -65.21\n",
      "Episode: 376, total numsteps: 373703, reward: -32.56, average reward: -64.63\n",
      "Episode: 377, total numsteps: 375303, reward: -49.21, average reward: -64.43\n",
      "Episode: 378, total numsteps: 376903, reward: -41.76, average reward: -64.11\n",
      "Episode: 379, total numsteps: 378503, reward: -60.45, average reward: -63.9\n",
      "Episode: 380, total numsteps: 380103, reward: -69.54, average reward: -63.85\n",
      "Episode: 381, total numsteps: 381703, reward: -53.44, average reward: -63.52\n",
      "Episode: 382, total numsteps: 383303, reward: -55.59, average reward: -63.28\n",
      "Episode: 383, total numsteps: 384903, reward: -55.98, average reward: -62.72\n",
      "Episode: 384, total numsteps: 386503, reward: -54.97, average reward: -62.55\n",
      "Episode: 385, total numsteps: 388103, reward: -66.34, average reward: -62.62\n",
      "Episode: 386, total numsteps: 389703, reward: -67.87, average reward: -62.71\n",
      "Episode: 387, total numsteps: 391303, reward: -66.52, average reward: -62.8\n",
      "Episode: 388, total numsteps: 392903, reward: -43.79, average reward: -62.58\n",
      "Episode: 389, total numsteps: 394503, reward: -23.08, average reward: -62.02\n",
      "Episode: 390, total numsteps: 396103, reward: -62.63, average reward: -61.94\n",
      "Episode: 391, total numsteps: 397703, reward: -43.12, average reward: -61.81\n",
      "Episode: 392, total numsteps: 399303, reward: -28.32, average reward: -61.11\n",
      "Episode: 393, total numsteps: 400903, reward: -58.31, average reward: -61.11\n",
      "Episode: 394, total numsteps: 402503, reward: -54.15, average reward: -60.89\n",
      "Episode: 395, total numsteps: 404069, reward: -99.35, average reward: -61.39\n",
      "Episode: 396, total numsteps: 405669, reward: -49.64, average reward: -61.48\n",
      "Episode: 397, total numsteps: 407269, reward: -41.64, average reward: -60.49\n",
      "Episode: 398, total numsteps: 408869, reward: -47.01, average reward: -60.29\n",
      "Episode: 399, total numsteps: 410469, reward: -55.01, average reward: -60.37\n",
      "Episode: 400, total numsteps: 412069, reward: -37.71, average reward: -60.12\n",
      "Episode: 401, total numsteps: 413669, reward: -52.23, average reward: -60.13\n",
      "Episode: 402, total numsteps: 415269, reward: -55.6, average reward: -60.06\n",
      "Episode: 403, total numsteps: 416869, reward: -62.07, average reward: -59.93\n",
      "Episode: 404, total numsteps: 416909, reward: -107.28, average reward: -60.42\n",
      "Episode: 405, total numsteps: 418509, reward: -58.64, average reward: -60.39\n",
      "Episode: 406, total numsteps: 420109, reward: -55.09, average reward: -60.27\n",
      "Episode: 407, total numsteps: 421709, reward: -53.26, average reward: -60.05\n",
      "Episode: 408, total numsteps: 423309, reward: -35.62, average reward: -59.72\n",
      "Episode: 409, total numsteps: 423438, reward: -101.82, average reward: -60.18\n",
      "Episode: 410, total numsteps: 425038, reward: -36.83, average reward: -59.85\n",
      "Episode: 411, total numsteps: 426638, reward: -39.2, average reward: -59.71\n",
      "Episode: 412, total numsteps: 428238, reward: -43.23, average reward: -59.39\n",
      "Episode: 413, total numsteps: 429838, reward: -9.4, average reward: -58.53\n",
      "Episode: 414, total numsteps: 431438, reward: -18.8, average reward: -58.26\n",
      "Episode: 415, total numsteps: 432696, reward: -128.44, average reward: -59.01\n",
      "Episode: 416, total numsteps: 434296, reward: -32.68, average reward: -58.75\n",
      "Episode: 417, total numsteps: 435896, reward: -32.12, average reward: -58.42\n",
      "Episode: 418, total numsteps: 435960, reward: -115.92, average reward: -58.83\n",
      "Episode: 419, total numsteps: 437560, reward: -36.71, average reward: -58.6\n",
      "Episode: 420, total numsteps: 437656, reward: -119.27, average reward: -59.19\n",
      "Episode: 421, total numsteps: 439256, reward: -45.48, average reward: -58.41\n",
      "Episode: 422, total numsteps: 440856, reward: -33.79, average reward: -58.19\n",
      "Episode: 423, total numsteps: 442456, reward: -50.92, average reward: -58.29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 424, total numsteps: 444056, reward: -27.3, average reward: -58.09\n",
      "Episode: 425, total numsteps: 445656, reward: -59.61, average reward: -58.28\n",
      "Episode: 426, total numsteps: 447256, reward: -75.33, average reward: -58.63\n",
      "Episode: 427, total numsteps: 448856, reward: -49.63, average reward: -58.59\n",
      "Episode: 428, total numsteps: 450456, reward: -45.38, average reward: -58.62\n",
      "Episode: 429, total numsteps: 452056, reward: -37.65, average reward: -58.56\n",
      "Episode: 430, total numsteps: 453656, reward: -59.46, average reward: -58.79\n",
      "Episode: 431, total numsteps: 455256, reward: -38.15, average reward: -58.78\n",
      "Episode: 432, total numsteps: 456856, reward: -74.44, average reward: -58.87\n",
      "Episode: 433, total numsteps: 458456, reward: -54.74, average reward: -58.8\n",
      "Episode: 434, total numsteps: 460056, reward: -49.83, average reward: -58.82\n",
      "Episode: 435, total numsteps: 461656, reward: -51.38, average reward: -58.94\n",
      "Episode: 436, total numsteps: 463256, reward: -53.1, average reward: -59.04\n",
      "Episode: 437, total numsteps: 464856, reward: -40.02, average reward: -58.89\n",
      "Episode: 438, total numsteps: 466456, reward: -29.78, average reward: -58.76\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import math\n",
    "import gym\n",
    "import numpy as np\n",
    "import itertools\n",
    "import torch\n",
    "from PIL import Image\n",
    "from SAC.sac import SAC\n",
    "from tensorboardX import SummaryWriter\n",
    "from SAC.normalized_actions import NormalizedActions\n",
    "from SAC.replay_memory import ReplayMemory\n",
    "\n",
    "'''\n",
    "parser = argparse.ArgumentParser(description='PyTorch REINFORCE example')\n",
    "parser.add_argument('--env-name', default=\"BipedalWalker-v2\",\n",
    "                    help='name of the environment to run')\n",
    "parser.add_argument('--policy', default=\"Gaussian\",\n",
    "                    help='algorithm to use: Gaussian | Deterministic')\n",
    "parser.add_argument('--eval', type=bool, default=False,\n",
    "                    help='Evaluate a policy (default:False)')\n",
    "parser.add_argument('--gamma', type=float, default=0.99, metavar='G',\n",
    "                    help='discount factor for reward (default: 0.99)')\n",
    "parser.add_argument('--tau', type=float, default=0.005, metavar='G',\n",
    "                    help='target smoothing coefficient(τ) (default: 0.005)')\n",
    "parser.add_argument('--lr', type=float, default=0.0003, metavar='G',\n",
    "                    help='learning rate (default: 0.0003)')\n",
    "parser.add_argument('--alpha', type=float, default=0.2, metavar='G',\n",
    "                    help='Temperature parameter α determines the relative importance of the entropy term against the reward (default: 0.2)')\n",
    "parser.add_argument('--seed', type=int, default=543, metavar='N',\n",
    "                    help='random seed (default: 543)')\n",
    "parser.add_argument('--batch_size', type=int, default=256, metavar='N',\n",
    "                    help='batch size (default: 256)')\n",
    "parser.add_argument('--num_steps', type=int, default=1000000, metavar='N',\n",
    "                    help='maximum number of steps (default: 1000000)')\n",
    "parser.add_argument('--hidden_size', type=int, default=256, metavar='N',\n",
    "                    help='hidden size (default: 256)')\n",
    "parser.add_argument('--updates_per_step', type=int, default=1, metavar='N',\n",
    "                    help='model updates per simulator step (default: 1)')\n",
    "parser.add_argument('--target_update_interval', type=int, default=1, metavar='N',\n",
    "                    help='Value target update per no. of updates per step (default: 1)')\n",
    "parser.add_argument('--replay_size', type=int, default=1000000, metavar='N',\n",
    "                    help='size of replay buffer (default: 10000000)')\n",
    "args = parser.parse_args()\n",
    "'''\n",
    "\n",
    "args = {\n",
    "    \"env_name\": \"BipedalWalker-v2\",\n",
    "    \"policy\": \"Gaussian\",\n",
    "    \"eval\": False,\n",
    "    \"gamma\": 0.99,\n",
    "    \"tau\": 0.001,\n",
    "    \"lr\": 0.0001,\n",
    "    \"alpha\": 0.2,\n",
    "    \"seed\": 123,\n",
    "    \"batch_size\": 256,\n",
    "    \"num_steps\": 5000000,\n",
    "    \"steps_in_episode\": 2000,\n",
    "    \"hidden_size\": 256,\n",
    "    \"updates_per_step\": 1,\n",
    "    \"target_update_interval\": 2,\n",
    "    \"replay_size\": 20000\n",
    "}    \n",
    "\n",
    "\n",
    "# Environment\n",
    "env = NormalizedActions(gym.make(args['env_name']))\n",
    "env.seed(args['seed'])\n",
    "torch.manual_seed(args['seed'])\n",
    "np.random.seed(args['seed'])\n",
    "\n",
    "# Agent\n",
    "agent = SAC(env.observation_space.shape[0], env.action_space, args)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Memory\n",
    "memory = ReplayMemory(args['replay_size'])\n",
    "\n",
    "# Training Loop\n",
    "rewards = []\n",
    "total_numsteps = 0\n",
    "updates = 0\n",
    "\n",
    "for i_episode in itertools.count():\n",
    "    state = env.reset()\n",
    "\n",
    "    episode_reward = 0\n",
    "    for t in range(args['steps_in_episode']):\n",
    "        action = agent.select_action(state)  # Sample action from policy\n",
    "        next_state, reward, done, _ = env.step(action)  # Step\n",
    "        mask = not done  # 1 for not done and 0 for done\n",
    "        memory.push(state, action, reward, next_state, mask)  # Append transition to memory\n",
    "        if len(memory) > args['batch_size']:\n",
    "            for i in range(args['updates_per_step']): # Number of updates per step in environment\n",
    "                # Sample a batch from memory\n",
    "                state_batch, action_batch, reward_batch, next_state_batch, mask_batch = memory.sample(args['batch_size'])\n",
    "                # Update parameters of all the networks\n",
    "                value_loss, critic_1_loss, critic_2_loss, policy_loss = agent.update_parameters(state_batch, action_batch, \n",
    "                                                                                                reward_batch, next_state_batch, \n",
    "                                                                                                mask_batch, updates)\n",
    "\n",
    "                writer.add_scalar('loss/value', value_loss, updates)\n",
    "                writer.add_scalar('loss/critic_1', critic_1_loss, updates)\n",
    "                writer.add_scalar('loss/critic_2', critic_2_loss, updates)\n",
    "                writer.add_scalar('loss/policy', policy_loss, updates)\n",
    "                updates += 1\n",
    "\n",
    "        state = next_state\n",
    "        total_numsteps += 1\n",
    "        episode_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if total_numsteps > args['num_steps']:\n",
    "        break\n",
    "\n",
    "    writer.add_scalar('reward/train', episode_reward, i_episode)\n",
    "    rewards.append(episode_reward)\n",
    "    print(\"Episode: {}, total numsteps: {}, reward: {}, average reward: {}\".format(i_episode, total_numsteps, np.round(rewards[-1],2),\n",
    "                                                                                np.round(np.mean(rewards[-100:]),2)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(agent):   \n",
    "    random_seed = 0\n",
    "    episodes = 3\n",
    "    max_timesteps = 2000\n",
    "    render = True\n",
    "    save_gif = True\n",
    "     \n",
    "    for i_episode in range(1, episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        for t in range(10000):  # Don't infinite loop while learning\n",
    "            action = agent.select_action(state)  # Sample action from policy\n",
    "            next_state, reward, done, _ = env.step(action)  # Step\n",
    "           \n",
    "            if render:\n",
    "                env.render()  \n",
    "                if save_gif:\n",
    "                    dirname = './gif/sac/{}'.format(i_episode)\n",
    "                    if not os.path.isdir(dirname):\n",
    "                        os.mkdir(dirname)\n",
    "                    img = env.render(mode = 'rgb_array')\n",
    "                    img = Image.fromarray(img)\n",
    "                    img.save('./gif/sac/{}/{}.jpg'.format(i_episode,t))\n",
    "\n",
    "            state = next_state            \n",
    "            episode_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break    \n",
    "   \n",
    "            \n",
    "        print('Episode: {}\\tReward: {}'.format(i_episode, int(episode_reward)))\n",
    "        running_reward = 0\n",
    "        env.close()        \n",
    "                \n",
    "test(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
