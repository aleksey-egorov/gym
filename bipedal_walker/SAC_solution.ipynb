{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: <class 'SAC.normalized_actions.NormalizedActions'> doesn't implement 'action' method. Maybe it implements deprecated '_action' method.\u001b[0m\n",
      "Episode: 0, total numsteps: 170, reward: -133.75, average reward: -133.75\n",
      "Episode: 1, total numsteps: 1770, reward: -79.8, average reward: -106.78\n",
      "Episode: 2, total numsteps: 1866, reward: -102.0, average reward: -105.18\n",
      "Episode: 3, total numsteps: 3466, reward: -95.41, average reward: -102.74\n",
      "Episode: 4, total numsteps: 5066, reward: -67.44, average reward: -95.68\n",
      "Episode: 5, total numsteps: 5125, reward: -101.64, average reward: -96.67\n",
      "Episode: 6, total numsteps: 6725, reward: -74.66, average reward: -93.53\n",
      "Episode: 7, total numsteps: 6783, reward: -109.92, average reward: -95.58\n",
      "Episode: 8, total numsteps: 8383, reward: -76.81, average reward: -93.49\n",
      "Episode: 9, total numsteps: 8443, reward: -99.79, average reward: -94.12\n",
      "Episode: 10, total numsteps: 8520, reward: -101.03, average reward: -94.75\n",
      "Episode: 11, total numsteps: 8666, reward: -98.33, average reward: -95.05\n",
      "Episode: 12, total numsteps: 8726, reward: -100.95, average reward: -95.5\n",
      "Episode: 13, total numsteps: 8791, reward: -109.17, average reward: -96.48\n",
      "Episode: 14, total numsteps: 10391, reward: -94.3, average reward: -96.33\n",
      "Episode: 15, total numsteps: 10465, reward: -99.82, average reward: -96.55\n",
      "Episode: 16, total numsteps: 10507, reward: -104.22, average reward: -97.0\n",
      "Episode: 17, total numsteps: 10591, reward: -105.07, average reward: -97.45\n",
      "Episode: 18, total numsteps: 10657, reward: -99.79, average reward: -97.57\n",
      "Episode: 19, total numsteps: 10713, reward: -109.29, average reward: -98.16\n",
      "Episode: 20, total numsteps: 10797, reward: -104.27, average reward: -98.45\n",
      "Episode: 21, total numsteps: 10863, reward: -103.53, average reward: -98.68\n",
      "Episode: 22, total numsteps: 12463, reward: -88.27, average reward: -98.23\n",
      "Episode: 23, total numsteps: 12545, reward: -100.62, average reward: -98.33\n",
      "Episode: 24, total numsteps: 12617, reward: -101.97, average reward: -98.47\n",
      "Episode: 25, total numsteps: 12701, reward: -104.51, average reward: -98.71\n",
      "Episode: 26, total numsteps: 12809, reward: -101.04, average reward: -98.79\n",
      "Episode: 27, total numsteps: 12906, reward: -106.12, average reward: -99.05\n",
      "Episode: 28, total numsteps: 13040, reward: -126.24, average reward: -99.99\n",
      "Episode: 29, total numsteps: 14640, reward: -97.18, average reward: -99.9\n",
      "Episode: 30, total numsteps: 16240, reward: -89.82, average reward: -99.57\n",
      "Episode: 31, total numsteps: 16312, reward: -103.18, average reward: -99.69\n",
      "Episode: 32, total numsteps: 16353, reward: -103.1, average reward: -99.79\n",
      "Episode: 33, total numsteps: 16423, reward: -108.74, average reward: -100.05\n",
      "Episode: 34, total numsteps: 16522, reward: -101.62, average reward: -100.1\n",
      "Episode: 35, total numsteps: 18122, reward: -69.7, average reward: -99.25\n",
      "Episode: 36, total numsteps: 18201, reward: -104.97, average reward: -99.41\n",
      "Episode: 37, total numsteps: 18274, reward: -106.18, average reward: -99.59\n",
      "Episode: 38, total numsteps: 18342, reward: -107.5, average reward: -99.79\n",
      "Episode: 39, total numsteps: 18400, reward: -105.48, average reward: -99.93\n",
      "Episode: 40, total numsteps: 18469, reward: -101.02, average reward: -99.96\n",
      "Episode: 41, total numsteps: 18555, reward: -99.87, average reward: -99.96\n",
      "Episode: 42, total numsteps: 18614, reward: -104.19, average reward: -100.05\n",
      "Episode: 43, total numsteps: 18690, reward: -117.52, average reward: -100.45\n",
      "Episode: 44, total numsteps: 18741, reward: -105.83, average reward: -100.57\n",
      "Episode: 45, total numsteps: 18807, reward: -106.14, average reward: -100.69\n",
      "Episode: 46, total numsteps: 20407, reward: -91.64, average reward: -100.5\n",
      "Episode: 47, total numsteps: 20493, reward: -104.62, average reward: -100.58\n",
      "Episode: 48, total numsteps: 20727, reward: -101.5, average reward: -100.6\n",
      "Episode: 49, total numsteps: 20788, reward: -101.3, average reward: -100.62\n",
      "Episode: 50, total numsteps: 20907, reward: -114.04, average reward: -100.88\n",
      "Episode: 51, total numsteps: 20965, reward: -103.82, average reward: -100.94\n",
      "Episode: 52, total numsteps: 21034, reward: -112.87, average reward: -101.16\n",
      "Episode: 53, total numsteps: 22634, reward: -79.59, average reward: -100.76\n",
      "Episode: 54, total numsteps: 22690, reward: -105.89, average reward: -100.86\n",
      "Episode: 55, total numsteps: 22775, reward: -103.53, average reward: -100.9\n",
      "Episode: 56, total numsteps: 24375, reward: -86.98, average reward: -100.66\n",
      "Episode: 57, total numsteps: 25975, reward: -86.59, average reward: -100.42\n",
      "Episode: 58, total numsteps: 27575, reward: -86.53, average reward: -100.18\n",
      "Episode: 59, total numsteps: 27673, reward: -113.76, average reward: -100.41\n",
      "Episode: 60, total numsteps: 29273, reward: -84.69, average reward: -100.15\n",
      "Episode: 61, total numsteps: 29376, reward: -100.53, average reward: -100.16\n",
      "Episode: 62, total numsteps: 30976, reward: -79.79, average reward: -99.83\n",
      "Episode: 63, total numsteps: 31055, reward: -114.18, average reward: -100.06\n",
      "Episode: 64, total numsteps: 31114, reward: -99.01, average reward: -100.04\n",
      "Episode: 65, total numsteps: 32714, reward: -84.78, average reward: -99.81\n",
      "Episode: 66, total numsteps: 34314, reward: -81.13, average reward: -99.53\n",
      "Episode: 67, total numsteps: 35914, reward: -80.75, average reward: -99.25\n",
      "Episode: 68, total numsteps: 37514, reward: -87.67, average reward: -99.09\n",
      "Episode: 69, total numsteps: 37570, reward: -113.64, average reward: -99.29\n",
      "Episode: 70, total numsteps: 37657, reward: -117.78, average reward: -99.55\n",
      "Episode: 71, total numsteps: 39257, reward: -75.42, average reward: -99.22\n",
      "Episode: 72, total numsteps: 39363, reward: -105.1, average reward: -99.3\n",
      "Episode: 73, total numsteps: 40963, reward: -72.68, average reward: -98.94\n",
      "Episode: 74, total numsteps: 42563, reward: -82.12, average reward: -98.72\n",
      "Episode: 75, total numsteps: 44163, reward: -90.05, average reward: -98.6\n",
      "Episode: 76, total numsteps: 44280, reward: -97.46, average reward: -98.59\n",
      "Episode: 77, total numsteps: 44361, reward: -100.4, average reward: -98.61\n",
      "Episode: 78, total numsteps: 44455, reward: -122.88, average reward: -98.92\n",
      "Episode: 79, total numsteps: 44550, reward: -125.12, average reward: -99.25\n",
      "Episode: 80, total numsteps: 46150, reward: -78.27, average reward: -98.99\n",
      "Episode: 81, total numsteps: 47750, reward: -76.55, average reward: -98.71\n",
      "Episode: 82, total numsteps: 47841, reward: -99.56, average reward: -98.72\n",
      "Episode: 83, total numsteps: 47906, reward: -102.68, average reward: -98.77\n",
      "Episode: 84, total numsteps: 47974, reward: -99.77, average reward: -98.78\n",
      "Episode: 85, total numsteps: 49574, reward: -69.65, average reward: -98.44\n",
      "Episode: 86, total numsteps: 51174, reward: -77.96, average reward: -98.21\n",
      "Episode: 87, total numsteps: 52774, reward: -81.51, average reward: -98.02\n",
      "Episode: 88, total numsteps: 52902, reward: -123.24, average reward: -98.3\n",
      "Episode: 89, total numsteps: 53088, reward: -100.37, average reward: -98.32\n",
      "Episode: 90, total numsteps: 54688, reward: -83.02, average reward: -98.16\n",
      "Episode: 91, total numsteps: 54760, reward: -118.95, average reward: -98.38\n",
      "Episode: 92, total numsteps: 54895, reward: -101.99, average reward: -98.42\n",
      "Episode: 93, total numsteps: 56495, reward: -78.15, average reward: -98.21\n",
      "Episode: 94, total numsteps: 58095, reward: -88.09, average reward: -98.1\n",
      "Episode: 95, total numsteps: 58179, reward: -116.64, average reward: -98.29\n",
      "Episode: 96, total numsteps: 58293, reward: -130.81, average reward: -98.63\n",
      "Episode: 97, total numsteps: 58409, reward: -125.42, average reward: -98.9\n",
      "Episode: 98, total numsteps: 58514, reward: -131.37, average reward: -99.23\n",
      "Episode: 99, total numsteps: 60114, reward: -75.79, average reward: -98.99\n",
      "Episode: 100, total numsteps: 60197, reward: -121.29, average reward: -98.87\n",
      "Episode: 101, total numsteps: 61797, reward: -74.8, average reward: -98.82\n",
      "Episode: 102, total numsteps: 63397, reward: -78.08, average reward: -98.58\n",
      "Episode: 103, total numsteps: 64997, reward: -76.11, average reward: -98.39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 104, total numsteps: 66597, reward: -75.24, average reward: -98.47\n",
      "Episode: 105, total numsteps: 68197, reward: -75.19, average reward: -98.2\n",
      "Episode: 106, total numsteps: 69797, reward: -70.08, average reward: -98.16\n",
      "Episode: 107, total numsteps: 71397, reward: -75.22, average reward: -97.81\n",
      "Episode: 108, total numsteps: 72997, reward: -81.01, average reward: -97.85\n",
      "Episode: 109, total numsteps: 74597, reward: -72.35, average reward: -97.58\n",
      "Episode: 110, total numsteps: 76197, reward: -69.8, average reward: -97.26\n",
      "Episode: 111, total numsteps: 76377, reward: -113.06, average reward: -97.41\n",
      "Episode: 112, total numsteps: 77977, reward: -73.68, average reward: -97.14\n",
      "Episode: 113, total numsteps: 79577, reward: -76.27, average reward: -96.81\n",
      "Episode: 114, total numsteps: 81177, reward: -75.87, average reward: -96.62\n",
      "Episode: 115, total numsteps: 81302, reward: -106.36, average reward: -96.69\n",
      "Episode: 116, total numsteps: 82902, reward: -75.53, average reward: -96.4\n",
      "Episode: 117, total numsteps: 84502, reward: -74.85, average reward: -96.1\n",
      "Episode: 118, total numsteps: 86102, reward: -78.42, average reward: -95.89\n",
      "Episode: 119, total numsteps: 87702, reward: -79.93, average reward: -95.59\n",
      "Episode: 120, total numsteps: 89302, reward: -84.36, average reward: -95.39\n",
      "Episode: 121, total numsteps: 90902, reward: -78.7, average reward: -95.15\n",
      "Episode: 122, total numsteps: 92502, reward: -72.91, average reward: -94.99\n",
      "Episode: 123, total numsteps: 94102, reward: -80.39, average reward: -94.79\n",
      "Episode: 124, total numsteps: 95702, reward: -69.01, average reward: -94.46\n",
      "Episode: 125, total numsteps: 97302, reward: -75.9, average reward: -94.17\n",
      "Episode: 126, total numsteps: 98902, reward: -76.67, average reward: -93.93\n",
      "Episode: 127, total numsteps: 100502, reward: -82.11, average reward: -93.69\n",
      "Episode: 128, total numsteps: 102102, reward: -76.88, average reward: -93.2\n",
      "Episode: 129, total numsteps: 103702, reward: -80.26, average reward: -93.03\n",
      "Episode: 130, total numsteps: 105302, reward: -85.5, average reward: -92.98\n",
      "Episode: 131, total numsteps: 106902, reward: -76.25, average reward: -92.72\n",
      "Episode: 132, total numsteps: 108502, reward: -78.38, average reward: -92.47\n",
      "Episode: 133, total numsteps: 110102, reward: -83.58, average reward: -92.22\n",
      "Episode: 134, total numsteps: 111702, reward: -76.58, average reward: -91.97\n",
      "Episode: 135, total numsteps: 113302, reward: -75.36, average reward: -92.02\n",
      "Episode: 136, total numsteps: 114902, reward: -81.68, average reward: -91.79\n",
      "Episode: 137, total numsteps: 116502, reward: -79.99, average reward: -91.53\n",
      "Episode: 138, total numsteps: 118102, reward: -80.32, average reward: -91.26\n",
      "Episode: 139, total numsteps: 119702, reward: -76.1, average reward: -90.96\n",
      "Episode: 140, total numsteps: 121302, reward: -76.2, average reward: -90.71\n",
      "Episode: 141, total numsteps: 122902, reward: -76.64, average reward: -90.48\n",
      "Episode: 142, total numsteps: 124502, reward: -74.84, average reward: -90.19\n",
      "Episode: 143, total numsteps: 126102, reward: -72.15, average reward: -89.73\n",
      "Episode: 144, total numsteps: 127702, reward: -79.0, average reward: -89.47\n",
      "Episode: 145, total numsteps: 129302, reward: -76.53, average reward: -89.17\n",
      "Episode: 146, total numsteps: 130902, reward: -87.12, average reward: -89.13\n",
      "Episode: 147, total numsteps: 132502, reward: -74.39, average reward: -88.82\n",
      "Episode: 148, total numsteps: 134102, reward: -75.54, average reward: -88.56\n",
      "Episode: 149, total numsteps: 135702, reward: -79.62, average reward: -88.35\n",
      "Episode: 150, total numsteps: 137302, reward: -71.83, average reward: -87.92\n",
      "Episode: 151, total numsteps: 138902, reward: -78.38, average reward: -87.67\n",
      "Episode: 152, total numsteps: 140502, reward: -76.54, average reward: -87.31\n",
      "Episode: 153, total numsteps: 142102, reward: -75.94, average reward: -87.27\n",
      "Episode: 154, total numsteps: 143702, reward: -77.55, average reward: -86.99\n",
      "Episode: 155, total numsteps: 145302, reward: -77.59, average reward: -86.73\n",
      "Episode: 156, total numsteps: 146902, reward: -73.43, average reward: -86.59\n",
      "Episode: 157, total numsteps: 148502, reward: -79.6, average reward: -86.52\n",
      "Episode: 158, total numsteps: 150102, reward: -76.77, average reward: -86.42\n",
      "Episode: 159, total numsteps: 151702, reward: -74.57, average reward: -86.03\n",
      "Episode: 160, total numsteps: 153302, reward: -73.45, average reward: -85.92\n",
      "Episode: 161, total numsteps: 154902, reward: -74.32, average reward: -85.66\n",
      "Episode: 162, total numsteps: 156502, reward: -75.69, average reward: -85.62\n",
      "Episode: 163, total numsteps: 158102, reward: -65.32, average reward: -85.13\n",
      "Episode: 164, total numsteps: 159702, reward: -76.26, average reward: -84.9\n",
      "Episode: 165, total numsteps: 161302, reward: -77.11, average reward: -84.82\n",
      "Episode: 166, total numsteps: 162902, reward: -81.15, average reward: -84.82\n",
      "Episode: 167, total numsteps: 164502, reward: -73.98, average reward: -84.76\n",
      "Episode: 168, total numsteps: 166102, reward: -73.02, average reward: -84.61\n",
      "Episode: 169, total numsteps: 167702, reward: -77.23, average reward: -84.25\n",
      "Episode: 170, total numsteps: 169302, reward: -72.95, average reward: -83.8\n",
      "Episode: 171, total numsteps: 170902, reward: -70.55, average reward: -83.75\n",
      "Episode: 172, total numsteps: 172502, reward: -76.44, average reward: -83.46\n",
      "Episode: 173, total numsteps: 174102, reward: -69.51, average reward: -83.43\n",
      "Episode: 174, total numsteps: 175702, reward: -73.68, average reward: -83.35\n",
      "Episode: 175, total numsteps: 177302, reward: -68.65, average reward: -83.13\n",
      "Episode: 176, total numsteps: 178902, reward: -74.04, average reward: -82.9\n",
      "Episode: 177, total numsteps: 180502, reward: -71.29, average reward: -82.61\n",
      "Episode: 178, total numsteps: 182102, reward: -70.35, average reward: -82.08\n",
      "Episode: 179, total numsteps: 183702, reward: -77.92, average reward: -81.61\n",
      "Episode: 180, total numsteps: 185302, reward: -78.61, average reward: -81.61\n",
      "Episode: 181, total numsteps: 186902, reward: -68.02, average reward: -81.53\n",
      "Episode: 182, total numsteps: 188502, reward: -72.76, average reward: -81.26\n",
      "Episode: 183, total numsteps: 190102, reward: -70.14, average reward: -80.93\n",
      "Episode: 184, total numsteps: 191702, reward: -77.29, average reward: -80.71\n",
      "Episode: 185, total numsteps: 193302, reward: -75.79, average reward: -80.77\n",
      "Episode: 186, total numsteps: 194902, reward: -75.4, average reward: -80.75\n",
      "Episode: 187, total numsteps: 196502, reward: -73.04, average reward: -80.66\n",
      "Episode: 188, total numsteps: 198102, reward: -69.3, average reward: -80.12\n",
      "Episode: 189, total numsteps: 199702, reward: -75.64, average reward: -79.87\n",
      "Episode: 190, total numsteps: 201302, reward: -74.62, average reward: -79.79\n",
      "Episode: 191, total numsteps: 202902, reward: -70.94, average reward: -79.31\n",
      "Episode: 192, total numsteps: 204502, reward: -76.39, average reward: -79.05\n",
      "Episode: 193, total numsteps: 206102, reward: -73.17, average reward: -79.0\n",
      "Episode: 194, total numsteps: 207702, reward: -75.64, average reward: -78.88\n",
      "Episode: 195, total numsteps: 209302, reward: -74.84, average reward: -78.46\n",
      "Episode: 196, total numsteps: 210902, reward: -75.13, average reward: -77.9\n",
      "Episode: 197, total numsteps: 212502, reward: -77.49, average reward: -77.43\n",
      "Episode: 198, total numsteps: 214102, reward: -74.93, average reward: -76.86\n",
      "Episode: 199, total numsteps: 215702, reward: -75.45, average reward: -76.86\n",
      "Episode: 200, total numsteps: 217302, reward: -77.99, average reward: -76.42\n",
      "Episode: 201, total numsteps: 218902, reward: -82.72, average reward: -76.5\n",
      "Episode: 202, total numsteps: 220502, reward: -75.91, average reward: -76.48\n",
      "Episode: 203, total numsteps: 222102, reward: -76.96, average reward: -76.49\n",
      "Episode: 204, total numsteps: 223702, reward: -74.89, average reward: -76.49\n",
      "Episode: 205, total numsteps: 225302, reward: -77.69, average reward: -76.51\n",
      "Episode: 206, total numsteps: 226902, reward: -76.73, average reward: -76.58\n",
      "Episode: 207, total numsteps: 228502, reward: -75.37, average reward: -76.58\n",
      "Episode: 208, total numsteps: 230102, reward: -75.64, average reward: -76.53\n",
      "Episode: 209, total numsteps: 231702, reward: -70.03, average reward: -76.5\n",
      "Episode: 210, total numsteps: 233302, reward: -71.38, average reward: -76.52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 211, total numsteps: 234902, reward: -73.59, average reward: -76.12\n",
      "Episode: 212, total numsteps: 236502, reward: -75.52, average reward: -76.14\n",
      "Episode: 213, total numsteps: 238102, reward: -75.7, average reward: -76.14\n",
      "Episode: 214, total numsteps: 239702, reward: -72.65, average reward: -76.11\n",
      "Episode: 215, total numsteps: 241302, reward: -75.78, average reward: -75.8\n",
      "Episode: 216, total numsteps: 242902, reward: -74.17, average reward: -75.79\n",
      "Episode: 217, total numsteps: 244502, reward: -71.92, average reward: -75.76\n",
      "Episode: 218, total numsteps: 246102, reward: -75.47, average reward: -75.73\n",
      "Episode: 219, total numsteps: 247702, reward: -73.68, average reward: -75.66\n",
      "Episode: 220, total numsteps: 249302, reward: -78.85, average reward: -75.61\n",
      "Episode: 221, total numsteps: 250902, reward: -62.13, average reward: -75.44\n",
      "Episode: 222, total numsteps: 252502, reward: -68.01, average reward: -75.39\n",
      "Episode: 223, total numsteps: 254102, reward: -69.21, average reward: -75.28\n",
      "Episode: 224, total numsteps: 255702, reward: -77.21, average reward: -75.36\n",
      "Episode: 225, total numsteps: 257302, reward: -70.97, average reward: -75.32\n",
      "Episode: 226, total numsteps: 258902, reward: -72.85, average reward: -75.28\n",
      "Episode: 227, total numsteps: 260502, reward: -76.91, average reward: -75.23\n",
      "Episode: 228, total numsteps: 262102, reward: -74.99, average reward: -75.21\n",
      "Episode: 229, total numsteps: 263702, reward: -74.98, average reward: -75.15\n",
      "Episode: 230, total numsteps: 265302, reward: -75.52, average reward: -75.05\n",
      "Episode: 231, total numsteps: 266902, reward: -75.29, average reward: -75.04\n",
      "Episode: 232, total numsteps: 268502, reward: -72.91, average reward: -74.99\n",
      "Episode: 233, total numsteps: 270102, reward: -75.04, average reward: -74.9\n",
      "Episode: 234, total numsteps: 271702, reward: -74.15, average reward: -74.88\n",
      "Episode: 235, total numsteps: 273302, reward: -78.24, average reward: -74.91\n",
      "Episode: 236, total numsteps: 274902, reward: -64.46, average reward: -74.74\n",
      "Episode: 237, total numsteps: 276502, reward: -70.79, average reward: -74.64\n",
      "Episode: 238, total numsteps: 278102, reward: -73.89, average reward: -74.58\n",
      "Episode: 239, total numsteps: 279702, reward: -73.01, average reward: -74.55\n",
      "Episode: 240, total numsteps: 281302, reward: -67.11, average reward: -74.46\n",
      "Episode: 241, total numsteps: 282902, reward: -75.64, average reward: -74.45\n",
      "Episode: 242, total numsteps: 284502, reward: -74.46, average reward: -74.44\n",
      "Episode: 243, total numsteps: 286102, reward: -71.93, average reward: -74.44\n",
      "Episode: 244, total numsteps: 287702, reward: -72.27, average reward: -74.37\n",
      "Episode: 245, total numsteps: 289302, reward: -71.78, average reward: -74.33\n",
      "Episode: 246, total numsteps: 290902, reward: -75.04, average reward: -74.21\n",
      "Episode: 247, total numsteps: 292502, reward: -70.76, average reward: -74.17\n",
      "Episode: 248, total numsteps: 294102, reward: -70.01, average reward: -74.11\n",
      "Episode: 249, total numsteps: 295702, reward: -73.19, average reward: -74.05\n",
      "Episode: 250, total numsteps: 297302, reward: -72.38, average reward: -74.06\n",
      "Episode: 251, total numsteps: 298902, reward: -74.6, average reward: -74.02\n",
      "Episode: 252, total numsteps: 300502, reward: -71.08, average reward: -73.96\n",
      "Episode: 253, total numsteps: 302102, reward: -73.75, average reward: -73.94\n",
      "Episode: 254, total numsteps: 303702, reward: -73.82, average reward: -73.9\n",
      "Episode: 255, total numsteps: 305302, reward: -69.78, average reward: -73.83\n",
      "Episode: 256, total numsteps: 306902, reward: -76.52, average reward: -73.86\n",
      "Episode: 257, total numsteps: 308502, reward: -74.42, average reward: -73.81\n",
      "Episode: 258, total numsteps: 310102, reward: -71.75, average reward: -73.76\n",
      "Episode: 259, total numsteps: 311702, reward: -67.7, average reward: -73.69\n",
      "Episode: 260, total numsteps: 313302, reward: -77.66, average reward: -73.73\n",
      "Episode: 261, total numsteps: 314902, reward: -71.93, average reward: -73.7\n",
      "Episode: 262, total numsteps: 316502, reward: -72.5, average reward: -73.67\n",
      "Episode: 263, total numsteps: 318102, reward: -70.3, average reward: -73.72\n",
      "Episode: 264, total numsteps: 319702, reward: -73.04, average reward: -73.69\n",
      "Episode: 265, total numsteps: 321302, reward: -72.99, average reward: -73.65\n",
      "Episode: 266, total numsteps: 322902, reward: -73.15, average reward: -73.57\n",
      "Episode: 267, total numsteps: 324502, reward: -74.84, average reward: -73.58\n",
      "Episode: 268, total numsteps: 326102, reward: -72.2, average reward: -73.57\n",
      "Episode: 269, total numsteps: 327702, reward: -74.25, average reward: -73.54\n",
      "Episode: 270, total numsteps: 329302, reward: -74.84, average reward: -73.56\n",
      "Episode: 271, total numsteps: 330902, reward: -70.2, average reward: -73.56\n",
      "Episode: 272, total numsteps: 332502, reward: -76.11, average reward: -73.55\n",
      "Episode: 273, total numsteps: 334102, reward: -72.92, average reward: -73.59\n",
      "Episode: 274, total numsteps: 335702, reward: -73.98, average reward: -73.59\n",
      "Episode: 275, total numsteps: 337302, reward: -70.11, average reward: -73.6\n",
      "Episode: 276, total numsteps: 338902, reward: -73.63, average reward: -73.6\n",
      "Episode: 277, total numsteps: 340502, reward: -74.42, average reward: -73.63\n",
      "Episode: 278, total numsteps: 342102, reward: -69.06, average reward: -73.62\n",
      "Episode: 279, total numsteps: 343702, reward: -75.59, average reward: -73.59\n",
      "Episode: 280, total numsteps: 345302, reward: -71.8, average reward: -73.53\n",
      "Episode: 281, total numsteps: 346902, reward: -66.98, average reward: -73.52\n",
      "Episode: 282, total numsteps: 348502, reward: -72.92, average reward: -73.52\n",
      "Episode: 283, total numsteps: 350102, reward: -78.08, average reward: -73.6\n",
      "Episode: 284, total numsteps: 351702, reward: -74.29, average reward: -73.57\n",
      "Episode: 285, total numsteps: 353302, reward: -74.99, average reward: -73.56\n",
      "Episode: 286, total numsteps: 354902, reward: -76.52, average reward: -73.57\n",
      "Episode: 287, total numsteps: 356502, reward: -78.18, average reward: -73.62\n",
      "Episode: 288, total numsteps: 358102, reward: -73.21, average reward: -73.66\n",
      "Episode: 289, total numsteps: 359702, reward: -75.05, average reward: -73.66\n",
      "Episode: 290, total numsteps: 361302, reward: -74.36, average reward: -73.65\n",
      "Episode: 291, total numsteps: 362902, reward: -74.59, average reward: -73.69\n",
      "Episode: 292, total numsteps: 364502, reward: -75.49, average reward: -73.68\n",
      "Episode: 293, total numsteps: 366102, reward: -75.1, average reward: -73.7\n",
      "Episode: 294, total numsteps: 367702, reward: -73.91, average reward: -73.68\n",
      "Episode: 295, total numsteps: 369302, reward: -71.39, average reward: -73.65\n",
      "Episode: 296, total numsteps: 370902, reward: -72.74, average reward: -73.62\n",
      "Episode: 297, total numsteps: 372502, reward: -76.69, average reward: -73.62\n",
      "Episode: 298, total numsteps: 374102, reward: -71.84, average reward: -73.58\n",
      "Episode: 299, total numsteps: 375702, reward: -71.6, average reward: -73.55\n",
      "Episode: 300, total numsteps: 377302, reward: -71.34, average reward: -73.48\n",
      "Episode: 301, total numsteps: 378902, reward: -71.38, average reward: -73.37\n",
      "Episode: 302, total numsteps: 380502, reward: -74.21, average reward: -73.35\n",
      "Episode: 303, total numsteps: 382102, reward: -77.2, average reward: -73.35\n",
      "Episode: 304, total numsteps: 383702, reward: -79.06, average reward: -73.39\n",
      "Episode: 305, total numsteps: 385302, reward: -74.6, average reward: -73.36\n",
      "Episode: 306, total numsteps: 386902, reward: -75.86, average reward: -73.35\n",
      "Episode: 307, total numsteps: 388502, reward: -73.55, average reward: -73.34\n",
      "Episode: 308, total numsteps: 390102, reward: -72.1, average reward: -73.3\n",
      "Episode: 309, total numsteps: 391702, reward: -72.79, average reward: -73.33\n",
      "Episode: 310, total numsteps: 393302, reward: -71.57, average reward: -73.33\n",
      "Episode: 311, total numsteps: 394902, reward: -72.42, average reward: -73.32\n",
      "Episode: 312, total numsteps: 396502, reward: -73.09, average reward: -73.29\n",
      "Episode: 313, total numsteps: 398102, reward: -67.25, average reward: -73.21\n",
      "Episode: 314, total numsteps: 399702, reward: -74.74, average reward: -73.23\n",
      "Episode: 315, total numsteps: 401302, reward: -77.09, average reward: -73.24\n",
      "Episode: 316, total numsteps: 402902, reward: -75.29, average reward: -73.25\n",
      "Episode: 317, total numsteps: 404502, reward: -65.34, average reward: -73.19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 318, total numsteps: 406102, reward: -72.31, average reward: -73.16\n",
      "Episode: 319, total numsteps: 407702, reward: -73.26, average reward: -73.15\n",
      "Episode: 320, total numsteps: 409302, reward: -75.1, average reward: -73.12\n",
      "Episode: 321, total numsteps: 410902, reward: -77.17, average reward: -73.27\n",
      "Episode: 322, total numsteps: 412502, reward: -74.62, average reward: -73.33\n",
      "Episode: 323, total numsteps: 414102, reward: -72.63, average reward: -73.37\n",
      "Episode: 324, total numsteps: 415702, reward: -73.85, average reward: -73.33\n",
      "Episode: 325, total numsteps: 417302, reward: -77.01, average reward: -73.39\n",
      "Episode: 326, total numsteps: 418902, reward: -79.34, average reward: -73.46\n",
      "Episode: 327, total numsteps: 420502, reward: -70.43, average reward: -73.39\n",
      "Episode: 328, total numsteps: 422102, reward: -77.8, average reward: -73.42\n",
      "Episode: 329, total numsteps: 423702, reward: -80.58, average reward: -73.48\n",
      "Episode: 330, total numsteps: 425302, reward: -74.46, average reward: -73.47\n",
      "Episode: 331, total numsteps: 426902, reward: -71.21, average reward: -73.43\n",
      "Episode: 332, total numsteps: 428502, reward: -70.21, average reward: -73.4\n",
      "Episode: 333, total numsteps: 430102, reward: -72.14, average reward: -73.37\n",
      "Episode: 334, total numsteps: 431702, reward: -75.82, average reward: -73.39\n",
      "Episode: 335, total numsteps: 433302, reward: -74.19, average reward: -73.35\n",
      "Episode: 336, total numsteps: 434902, reward: -72.2, average reward: -73.42\n",
      "Episode: 337, total numsteps: 436502, reward: -74.88, average reward: -73.46\n",
      "Episode: 338, total numsteps: 438102, reward: -75.58, average reward: -73.48\n",
      "Episode: 339, total numsteps: 439702, reward: -72.99, average reward: -73.48\n",
      "Episode: 340, total numsteps: 441302, reward: -71.52, average reward: -73.53\n",
      "Episode: 341, total numsteps: 442902, reward: -76.42, average reward: -73.53\n",
      "Episode: 342, total numsteps: 444502, reward: -76.32, average reward: -73.55\n",
      "Episode: 343, total numsteps: 446102, reward: -70.47, average reward: -73.54\n",
      "Episode: 344, total numsteps: 447702, reward: -69.9, average reward: -73.51\n",
      "Episode: 345, total numsteps: 449302, reward: -73.78, average reward: -73.53\n",
      "Episode: 346, total numsteps: 450902, reward: -71.12, average reward: -73.49\n",
      "Episode: 347, total numsteps: 452502, reward: -74.4, average reward: -73.53\n",
      "Episode: 348, total numsteps: 454102, reward: -77.21, average reward: -73.6\n",
      "Episode: 349, total numsteps: 455702, reward: -76.07, average reward: -73.63\n",
      "Episode: 350, total numsteps: 457302, reward: -72.74, average reward: -73.63\n",
      "Episode: 351, total numsteps: 458902, reward: -72.71, average reward: -73.62\n",
      "Episode: 352, total numsteps: 460502, reward: -74.74, average reward: -73.65\n",
      "Episode: 353, total numsteps: 462102, reward: -71.93, average reward: -73.63\n",
      "Episode: 354, total numsteps: 463702, reward: -72.72, average reward: -73.62\n",
      "Episode: 355, total numsteps: 465302, reward: -71.06, average reward: -73.64\n",
      "Episode: 356, total numsteps: 466902, reward: -75.49, average reward: -73.63\n",
      "Episode: 357, total numsteps: 468502, reward: -73.82, average reward: -73.62\n",
      "Episode: 358, total numsteps: 470102, reward: -74.1, average reward: -73.64\n",
      "Episode: 359, total numsteps: 471702, reward: -76.52, average reward: -73.73\n",
      "Episode: 360, total numsteps: 473302, reward: -74.35, average reward: -73.7\n",
      "Episode: 361, total numsteps: 474902, reward: -76.36, average reward: -73.74\n",
      "Episode: 362, total numsteps: 476502, reward: -74.1, average reward: -73.76\n",
      "Episode: 363, total numsteps: 478102, reward: -74.68, average reward: -73.8\n",
      "Episode: 364, total numsteps: 479702, reward: -77.31, average reward: -73.85\n",
      "Episode: 365, total numsteps: 481302, reward: -77.82, average reward: -73.89\n",
      "Episode: 366, total numsteps: 482902, reward: -81.17, average reward: -73.97\n",
      "Episode: 367, total numsteps: 484502, reward: -76.15, average reward: -73.99\n",
      "Episode: 368, total numsteps: 486102, reward: -79.99, average reward: -74.06\n",
      "Episode: 369, total numsteps: 487702, reward: -71.89, average reward: -74.04\n",
      "Episode: 370, total numsteps: 489302, reward: -73.43, average reward: -74.03\n",
      "Episode: 371, total numsteps: 490902, reward: -74.79, average reward: -74.07\n",
      "Episode: 372, total numsteps: 492502, reward: -72.58, average reward: -74.04\n",
      "Episode: 373, total numsteps: 494102, reward: -76.27, average reward: -74.07\n",
      "Episode: 374, total numsteps: 495702, reward: -71.66, average reward: -74.05\n",
      "Episode: 375, total numsteps: 497302, reward: -73.83, average reward: -74.08\n",
      "Episode: 376, total numsteps: 498902, reward: -78.64, average reward: -74.14\n",
      "Episode: 377, total numsteps: 500502, reward: -74.01, average reward: -74.13\n",
      "Episode: 378, total numsteps: 502102, reward: -75.79, average reward: -74.2\n",
      "Episode: 379, total numsteps: 503702, reward: -73.73, average reward: -74.18\n",
      "Episode: 380, total numsteps: 505302, reward: -79.4, average reward: -74.26\n",
      "Episode: 381, total numsteps: 506902, reward: -73.73, average reward: -74.32\n",
      "Episode: 382, total numsteps: 508502, reward: -77.35, average reward: -74.37\n",
      "Episode: 383, total numsteps: 510102, reward: -71.13, average reward: -74.3\n",
      "Episode: 384, total numsteps: 511702, reward: -74.39, average reward: -74.3\n",
      "Episode: 385, total numsteps: 513302, reward: -75.65, average reward: -74.31\n",
      "Episode: 386, total numsteps: 514902, reward: -72.61, average reward: -74.27\n",
      "Episode: 387, total numsteps: 516502, reward: -71.95, average reward: -74.2\n",
      "Episode: 388, total numsteps: 518102, reward: -73.48, average reward: -74.21\n",
      "Episode: 389, total numsteps: 519702, reward: -76.27, average reward: -74.22\n",
      "Episode: 390, total numsteps: 521302, reward: -70.24, average reward: -74.18\n",
      "Episode: 391, total numsteps: 522902, reward: -67.09, average reward: -74.1\n",
      "Episode: 392, total numsteps: 524502, reward: -73.43, average reward: -74.08\n",
      "Episode: 393, total numsteps: 526102, reward: -69.42, average reward: -74.03\n",
      "Episode: 394, total numsteps: 527702, reward: -75.12, average reward: -74.04\n",
      "Episode: 395, total numsteps: 529302, reward: -70.44, average reward: -74.03\n",
      "Episode: 396, total numsteps: 530902, reward: -72.32, average reward: -74.02\n",
      "Episode: 397, total numsteps: 532502, reward: -73.5, average reward: -73.99\n",
      "Episode: 398, total numsteps: 534102, reward: -77.53, average reward: -74.05\n",
      "Episode: 399, total numsteps: 535702, reward: -75.82, average reward: -74.09\n",
      "Episode: 400, total numsteps: 537302, reward: -73.64, average reward: -74.11\n",
      "Episode: 401, total numsteps: 538902, reward: -70.48, average reward: -74.11\n",
      "Episode: 402, total numsteps: 540502, reward: -76.82, average reward: -74.13\n",
      "Episode: 403, total numsteps: 542102, reward: -71.68, average reward: -74.08\n",
      "Episode: 404, total numsteps: 543702, reward: -74.52, average reward: -74.03\n",
      "Episode: 405, total numsteps: 545302, reward: -75.51, average reward: -74.04\n",
      "Episode: 406, total numsteps: 546902, reward: -72.34, average reward: -74.0\n",
      "Episode: 407, total numsteps: 548502, reward: -76.82, average reward: -74.04\n",
      "Episode: 408, total numsteps: 550102, reward: -74.82, average reward: -74.06\n",
      "Episode: 409, total numsteps: 551702, reward: -75.29, average reward: -74.09\n",
      "Episode: 410, total numsteps: 553302, reward: -73.93, average reward: -74.11\n",
      "Episode: 411, total numsteps: 554902, reward: -72.26, average reward: -74.11\n",
      "Episode: 412, total numsteps: 556502, reward: -79.3, average reward: -74.17\n",
      "Episode: 413, total numsteps: 558102, reward: -76.96, average reward: -74.27\n",
      "Episode: 414, total numsteps: 559702, reward: -78.3, average reward: -74.31\n",
      "Episode: 415, total numsteps: 561302, reward: -74.25, average reward: -74.28\n",
      "Episode: 416, total numsteps: 562902, reward: -72.67, average reward: -74.25\n",
      "Episode: 417, total numsteps: 564502, reward: -76.77, average reward: -74.37\n",
      "Episode: 418, total numsteps: 566102, reward: -77.63, average reward: -74.42\n",
      "Episode: 419, total numsteps: 567702, reward: -73.93, average reward: -74.43\n",
      "Episode: 420, total numsteps: 569302, reward: -74.25, average reward: -74.42\n",
      "Episode: 421, total numsteps: 570902, reward: -73.75, average reward: -74.38\n",
      "Episode: 422, total numsteps: 572502, reward: -76.18, average reward: -74.4\n",
      "Episode: 423, total numsteps: 574102, reward: -76.72, average reward: -74.44\n",
      "Episode: 424, total numsteps: 575702, reward: -70.28, average reward: -74.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 425, total numsteps: 577302, reward: -68.86, average reward: -74.32\n",
      "Episode: 426, total numsteps: 578902, reward: -72.47, average reward: -74.25\n",
      "Episode: 427, total numsteps: 580502, reward: -69.14, average reward: -74.24\n",
      "Episode: 428, total numsteps: 582102, reward: -73.7, average reward: -74.2\n",
      "Episode: 429, total numsteps: 583702, reward: -74.47, average reward: -74.14\n",
      "Episode: 430, total numsteps: 585302, reward: -71.63, average reward: -74.11\n",
      "Episode: 431, total numsteps: 586902, reward: -68.49, average reward: -74.08\n",
      "Episode: 432, total numsteps: 586963, reward: -111.85, average reward: -74.5\n",
      "Episode: 433, total numsteps: 588563, reward: -72.03, average reward: -74.5\n",
      "Episode: 434, total numsteps: 590163, reward: -72.95, average reward: -74.47\n",
      "Episode: 435, total numsteps: 591763, reward: -81.04, average reward: -74.54\n",
      "Episode: 436, total numsteps: 593363, reward: -75.15, average reward: -74.57\n",
      "Episode: 437, total numsteps: 594963, reward: -77.5, average reward: -74.59\n",
      "Episode: 438, total numsteps: 596563, reward: -76.41, average reward: -74.6\n",
      "Episode: 439, total numsteps: 598163, reward: -78.35, average reward: -74.66\n",
      "Episode: 440, total numsteps: 599763, reward: -74.21, average reward: -74.68\n",
      "Episode: 441, total numsteps: 601363, reward: -71.5, average reward: -74.63\n",
      "Episode: 442, total numsteps: 602963, reward: -80.12, average reward: -74.67\n",
      "Episode: 443, total numsteps: 604563, reward: -75.07, average reward: -74.72\n",
      "Episode: 444, total numsteps: 606163, reward: -69.85, average reward: -74.72\n",
      "Episode: 445, total numsteps: 607763, reward: -72.52, average reward: -74.7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5a3378e2afe2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 value_loss, critic_1_loss, critic_2_loss, policy_loss = agent.update_parameters(state_batch, action_batch, \n\u001b[1;32m     99\u001b[0m                                                                                                 \u001b[0mreward_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                                                                                                 mask_batch, updates)\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss/value'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/gym/bipedal_walker/SAC/sac.py\u001b[0m in \u001b[0;36mupdate_parameters\u001b[0;34m(self, state_batch, action_batch, reward_batch, next_state_batch, mask_batch, updates)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mq2_value_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Gaussian\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                     \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import math\n",
    "import gym\n",
    "import numpy as np\n",
    "import itertools\n",
    "import torch\n",
    "from PIL import Image\n",
    "from SAC.sac import SAC\n",
    "from tensorboardX import SummaryWriter\n",
    "from SAC.normalized_actions import NormalizedActions\n",
    "from SAC.replay_memory import ReplayMemory\n",
    "\n",
    "'''\n",
    "parser = argparse.ArgumentParser(description='PyTorch REINFORCE example')\n",
    "parser.add_argument('--env-name', default=\"BipedalWalker-v2\",\n",
    "                    help='name of the environment to run')\n",
    "parser.add_argument('--policy', default=\"Gaussian\",\n",
    "                    help='algorithm to use: Gaussian | Deterministic')\n",
    "parser.add_argument('--eval', type=bool, default=False,\n",
    "                    help='Evaluate a policy (default:False)')\n",
    "parser.add_argument('--gamma', type=float, default=0.99, metavar='G',\n",
    "                    help='discount factor for reward (default: 0.99)')\n",
    "parser.add_argument('--tau', type=float, default=0.005, metavar='G',\n",
    "                    help='target smoothing coefficient(τ) (default: 0.005)')\n",
    "parser.add_argument('--lr', type=float, default=0.0003, metavar='G',\n",
    "                    help='learning rate (default: 0.0003)')\n",
    "parser.add_argument('--alpha', type=float, default=0.2, metavar='G',\n",
    "                    help='Temperature parameter α determines the relative importance of the entropy term against the reward (default: 0.2)')\n",
    "parser.add_argument('--seed', type=int, default=543, metavar='N',\n",
    "                    help='random seed (default: 543)')\n",
    "parser.add_argument('--batch_size', type=int, default=256, metavar='N',\n",
    "                    help='batch size (default: 256)')\n",
    "parser.add_argument('--num_steps', type=int, default=1000000, metavar='N',\n",
    "                    help='maximum number of steps (default: 1000000)')\n",
    "parser.add_argument('--hidden_size', type=int, default=256, metavar='N',\n",
    "                    help='hidden size (default: 256)')\n",
    "parser.add_argument('--updates_per_step', type=int, default=1, metavar='N',\n",
    "                    help='model updates per simulator step (default: 1)')\n",
    "parser.add_argument('--target_update_interval', type=int, default=1, metavar='N',\n",
    "                    help='Value target update per no. of updates per step (default: 1)')\n",
    "parser.add_argument('--replay_size', type=int, default=1000000, metavar='N',\n",
    "                    help='size of replay buffer (default: 10000000)')\n",
    "args = parser.parse_args()\n",
    "'''\n",
    "\n",
    "args = {\n",
    "    \"env_name\": \"BipedalWalker-v2\",\n",
    "    \"policy\": \"Gaussian\",\n",
    "    \"eval\": False,\n",
    "    \"gamma\": 0.99,\n",
    "    \"tau\": 0.001,\n",
    "    \"lr\": 0.001,\n",
    "    \"alpha\": 0.2,\n",
    "    \"seed\": 543,\n",
    "    \"batch_size\": 256,\n",
    "    \"num_steps\": 5000000,\n",
    "    \"hidden_size\": 256,\n",
    "    \"updates_per_step\": 1,\n",
    "    \"target_update_interval\": 1,\n",
    "    \"replay_size\": 1000000\n",
    "}    \n",
    "\n",
    "\n",
    "# Environment\n",
    "env = NormalizedActions(gym.make(args['env_name']))\n",
    "env.seed(args['seed'])\n",
    "torch.manual_seed(args['seed'])\n",
    "np.random.seed(args['seed'])\n",
    "\n",
    "# Agent\n",
    "agent = SAC(env.observation_space.shape[0], env.action_space, args)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Memory\n",
    "memory = ReplayMemory(args['replay_size'])\n",
    "\n",
    "# Training Loop\n",
    "rewards = []\n",
    "total_numsteps = 0\n",
    "updates = 0\n",
    "\n",
    "for i_episode in itertools.count():\n",
    "    state = env.reset()\n",
    "\n",
    "    episode_reward = 0\n",
    "    for t in range(10000):\n",
    "        action = agent.select_action(state)  # Sample action from policy\n",
    "        next_state, reward, done, _ = env.step(action)  # Step\n",
    "        mask = not done  # 1 for not done and 0 for done\n",
    "        memory.push(state, action, reward, next_state, mask)  # Append transition to memory\n",
    "        if len(memory) > args['batch_size']:\n",
    "            for i in range(args['updates_per_step']): # Number of updates per step in environment\n",
    "                # Sample a batch from memory\n",
    "                state_batch, action_batch, reward_batch, next_state_batch, mask_batch = memory.sample(args['batch_size'])\n",
    "                # Update parameters of all the networks\n",
    "                value_loss, critic_1_loss, critic_2_loss, policy_loss = agent.update_parameters(state_batch, action_batch, \n",
    "                                                                                                reward_batch, next_state_batch, \n",
    "                                                                                                mask_batch, updates)\n",
    "\n",
    "                writer.add_scalar('loss/value', value_loss, updates)\n",
    "                writer.add_scalar('loss/critic_1', critic_1_loss, updates)\n",
    "                writer.add_scalar('loss/critic_2', critic_2_loss, updates)\n",
    "                writer.add_scalar('loss/policy', policy_loss, updates)\n",
    "                updates += 1\n",
    "\n",
    "        state = next_state\n",
    "        total_numsteps += 1\n",
    "        episode_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if total_numsteps > args['num_steps']:\n",
    "        break\n",
    "\n",
    "    writer.add_scalar('reward/train', episode_reward, i_episode)\n",
    "    rewards.append(episode_reward)\n",
    "    print(\"Episode: {}, total numsteps: {}, reward: {}, average reward: {}\".format(i_episode, total_numsteps, np.round(rewards[-1],2),\n",
    "                                                                                np.round(np.mean(rewards[-100:]),2)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(agent):   \n",
    "    random_seed = 0\n",
    "    episodes = 3\n",
    "    max_timesteps = 2000\n",
    "    render = True\n",
    "    save_gif = True\n",
    "     \n",
    "    for i_episode in range(1, episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        for t in range(10000):  # Don't infinite loop while learning\n",
    "            action = agent.select_action(state)  # Sample action from policy\n",
    "            next_state, reward, done, _ = env.step(action)  # Step\n",
    "           \n",
    "            if render:\n",
    "                env.render()  \n",
    "                if save_gif:\n",
    "                    dirname = './gif/sac/{}'.format(i_episode)\n",
    "                    if not os.path.isdir(dirname):\n",
    "                        os.mkdir(dirname)\n",
    "                    img = env.render(mode = 'rgb_array')\n",
    "                    img = Image.fromarray(img)\n",
    "                    img.save('./gif/sac/{}/{}.jpg'.format(i_episode,t))\n",
    "\n",
    "            state = next_state            \n",
    "            episode_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break    \n",
    "   \n",
    "            \n",
    "        print('Episode: {}\\tReward: {}'.format(i_episode, int(episode_reward)))\n",
    "        running_reward = 0\n",
    "        env.close()        \n",
    "                \n",
    "test(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
