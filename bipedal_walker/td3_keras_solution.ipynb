{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "action_space=Box(4,)\n",
      "obs_space=Box(24,)\n",
      "threshold=300 \n",
      "\n",
      "Episode 0\tLast length:   686\t Reward: -157.88\t Avg Reward: -157.88\t Noise: 1.00\n",
      "Episode 1\tLast length:   119\t Reward: -110.27\t Avg Reward: -134.08\t Noise: 1.00\n",
      "Episode 2\tLast length:    78\t Reward: -115.11\t Avg Reward: -127.75\t Noise: 1.00\n",
      "Episode 3\tLast length:    46\t Reward: -113.45\t Avg Reward: -124.18\t Noise: 1.00\n",
      "Episode 4\tLast length:    52\t Reward: -111.54\t Avg Reward: -121.65\t Noise: 1.00\n",
      "Episode 5\tLast length:    39\t Reward: -108.78\t Avg Reward: -119.51\t Noise: 0.99\n",
      "Episode 6\tLast length:    99\t Reward: -117.96\t Avg Reward: -119.28\t Noise: 0.99\n",
      "Episode 7\tLast length:    48\t Reward: -109.73\t Avg Reward: -118.09\t Noise: 0.99\n",
      "Episode 8\tLast length:   202\t Reward: -116.69\t Avg Reward: -117.94\t Noise: 0.99\n",
      "Episode 9\tLast length:    88\t Reward: -108.11\t Avg Reward: -116.95\t Noise: 0.99\n",
      "Episode 10\tLast length:    65\t Reward: -113.05\t Avg Reward: -116.60\t Noise: 0.99\n",
      "Episode 11\tLast length:    88\t Reward: -120.10\t Avg Reward: -116.89\t Noise: 0.99\n",
      "Episode 12\tLast length:    71\t Reward: -106.45\t Avg Reward: -116.09\t Noise: 0.99\n",
      "Episode 13\tLast length:    44\t Reward: -112.16\t Avg Reward: -115.81\t Noise: 0.99\n",
      "Episode 14\tLast length:    62\t Reward: -114.37\t Avg Reward: -115.71\t Noise: 0.99\n",
      "Episode 15\tLast length:    36\t Reward: -111.17\t Avg Reward: -115.43\t Noise: 0.98\n",
      "Episode 16\tLast length:   107\t Reward: -104.98\t Avg Reward: -114.81\t Noise: 0.98\n",
      "Episode 17\tLast length:    70\t Reward: -114.84\t Avg Reward: -114.81\t Noise: 0.98\n",
      "Episode 18\tLast length:    68\t Reward: -115.26\t Avg Reward: -114.84\t Noise: 0.98\n",
      "Episode 19\tLast length:    58\t Reward: -116.14\t Avg Reward: -114.90\t Noise: 0.98\n",
      "Episode 20\tLast length:    53\t Reward: -110.32\t Avg Reward: -114.68\t Noise: 0.98\n",
      "Episode 21\tLast length:   117\t Reward: -128.48\t Avg Reward: -115.31\t Noise: 0.98\n",
      "Episode 22\tLast length:   886\t Reward: -183.17\t Avg Reward: -118.26\t Noise: 0.98\n",
      "Episode 23\tLast length:    40\t Reward: -113.81\t Avg Reward: -118.08\t Noise: 0.98\n",
      "Episode 24\tLast length:    45\t Reward: -118.53\t Avg Reward: -118.09\t Noise: 0.98\n",
      "Episode 25\tLast length:    39\t Reward: -109.21\t Avg Reward: -117.75\t Noise: 0.97\n",
      "Episode 26\tLast length:    48\t Reward: -108.21\t Avg Reward: -117.40\t Noise: 0.97\n",
      "Episode 27\tLast length:    56\t Reward: -101.88\t Avg Reward: -116.84\t Noise: 0.97\n",
      "Episode 28\tLast length:    84\t Reward: -105.43\t Avg Reward: -116.45\t Noise: 0.97\n",
      "Episode 29\tLast length:    52\t Reward: -101.28\t Avg Reward: -115.95\t Noise: 0.97\n",
      "Episode 30\tLast length:    84\t Reward: -104.91\t Avg Reward: -115.59\t Noise: 0.97\n",
      "Episode 31\tLast length:    42\t Reward: -106.05\t Avg Reward: -115.29\t Noise: 0.97\n",
      "Episode 32\tLast length:    55\t Reward: -100.73\t Avg Reward: -114.85\t Noise: 0.97\n",
      "Episode 33\tLast length:    50\t Reward: -103.49\t Avg Reward: -114.52\t Noise: 0.97\n",
      "Episode 34\tLast length:    43\t Reward: -104.49\t Avg Reward: -114.23\t Noise: 0.97\n",
      "Episode 35\tLast length:    77\t Reward: -104.47\t Avg Reward: -113.96\t Noise: 0.96\n",
      "Episode 36\tLast length:    57\t Reward:  -99.82\t Avg Reward: -113.58\t Noise: 0.96\n",
      "Episode 37\tLast length:    57\t Reward: -100.89\t Avg Reward: -113.24\t Noise: 0.96\n",
      "Episode 38\tLast length:    62\t Reward: -102.93\t Avg Reward: -112.98\t Noise: 0.96\n",
      "Episode 39\tLast length:    51\t Reward: -108.97\t Avg Reward: -112.88\t Noise: 0.96\n",
      "Episode 40\tLast length:    56\t Reward: -104.43\t Avg Reward: -112.67\t Noise: 0.96\n",
      "Episode 41\tLast length:    45\t Reward: -103.95\t Avg Reward: -112.46\t Noise: 0.96\n",
      "Episode 42\tLast length:    42\t Reward: -105.62\t Avg Reward: -112.31\t Noise: 0.96\n",
      "Episode 43\tLast length:    62\t Reward: -115.53\t Avg Reward: -112.38\t Noise: 0.96\n",
      "Episode 44\tLast length:  1599\t Reward: -130.29\t Avg Reward: -112.78\t Noise: 0.96\n",
      "Episode 45\tLast length:  1599\t Reward:  -95.50\t Avg Reward: -112.40\t Noise: 0.95\n",
      "Episode 46\tLast length:  1599\t Reward: -100.85\t Avg Reward: -112.16\t Noise: 0.95\n",
      "Episode 47\tLast length:  1599\t Reward: -111.44\t Avg Reward: -112.14\t Noise: 0.95\n",
      "Episode 48\tLast length:    43\t Reward: -114.27\t Avg Reward: -112.18\t Noise: 0.95\n",
      "Episode 49\tLast length:  1599\t Reward: -115.69\t Avg Reward: -112.25\t Noise: 0.95\n",
      "Episode 50\tLast length:  1599\t Reward: -117.15\t Avg Reward: -112.35\t Noise: 0.95\n",
      "Episode 51\tLast length:  1599\t Reward: -125.88\t Avg Reward: -112.61\t Noise: 0.95\n",
      "Episode 52\tLast length:   968\t Reward: -196.83\t Avg Reward: -114.20\t Noise: 0.95\n",
      "Episode 53\tLast length:   288\t Reward: -144.05\t Avg Reward: -114.75\t Noise: 0.95\n",
      "Episode 54\tLast length:  1599\t Reward: -101.59\t Avg Reward: -114.51\t Noise: 0.95\n",
      "Episode 55\tLast length:  1599\t Reward: -114.59\t Avg Reward: -114.51\t Noise: 0.94\n",
      "Episode 56\tLast length:  1599\t Reward: -134.03\t Avg Reward: -114.86\t Noise: 0.94\n",
      "Episode 57\tLast length:  1599\t Reward: -140.28\t Avg Reward: -115.29\t Noise: 0.94\n",
      "Episode 58\tLast length:  1599\t Reward: -139.56\t Avg Reward: -115.71\t Noise: 0.94\n",
      "Episode 59\tLast length:  1599\t Reward: -146.53\t Avg Reward: -116.22\t Noise: 0.94\n",
      "Episode 60\tLast length:  1599\t Reward: -148.17\t Avg Reward: -116.74\t Noise: 0.94\n",
      "Episode 61\tLast length:  1599\t Reward: -151.92\t Avg Reward: -117.31\t Noise: 0.94\n",
      "Episode 62\tLast length:  1599\t Reward: -154.78\t Avg Reward: -117.91\t Noise: 0.94\n",
      "Episode 63\tLast length:  1599\t Reward: -149.70\t Avg Reward: -118.40\t Noise: 0.94\n",
      "Episode 64\tLast length:  1599\t Reward: -145.51\t Avg Reward: -118.82\t Noise: 0.94\n",
      "Episode 65\tLast length:  1599\t Reward: -145.63\t Avg Reward: -119.23\t Noise: 0.94\n",
      "Episode 66\tLast length:  1599\t Reward: -164.76\t Avg Reward: -119.91\t Noise: 0.93\n",
      "Episode 67\tLast length:  1599\t Reward: -147.48\t Avg Reward: -120.31\t Noise: 0.93\n",
      "Episode 68\tLast length:  1599\t Reward: -147.62\t Avg Reward: -120.71\t Noise: 0.93\n",
      "Episode 69\tLast length:  1599\t Reward: -148.17\t Avg Reward: -121.10\t Noise: 0.93\n",
      "Episode 70\tLast length:  1599\t Reward: -155.12\t Avg Reward: -121.58\t Noise: 0.93\n",
      "Episode 71\tLast length:  1599\t Reward: -147.94\t Avg Reward: -121.94\t Noise: 0.93\n",
      "Episode 72\tLast length:  1599\t Reward: -145.82\t Avg Reward: -122.27\t Noise: 0.93\n",
      "Episode 73\tLast length:  1599\t Reward: -148.60\t Avg Reward: -122.63\t Noise: 0.93\n",
      "Episode 74\tLast length:  1599\t Reward: -152.22\t Avg Reward: -123.02\t Noise: 0.93\n",
      "Episode 75\tLast length:  1599\t Reward: -155.69\t Avg Reward: -123.45\t Noise: 0.93\n",
      "Episode 76\tLast length:  1599\t Reward: -151.98\t Avg Reward: -123.82\t Noise: 0.92\n",
      "Episode 77\tLast length:  1599\t Reward: -154.44\t Avg Reward: -124.21\t Noise: 0.92\n",
      "Episode 78\tLast length:  1599\t Reward: -145.66\t Avg Reward: -124.49\t Noise: 0.92\n",
      "Episode 79\tLast length:  1599\t Reward: -147.46\t Avg Reward: -124.77\t Noise: 0.92\n",
      "Episode 80\tLast length:  1599\t Reward: -150.17\t Avg Reward: -125.09\t Noise: 0.92\n",
      "Episode 81\tLast length:  1599\t Reward: -154.81\t Avg Reward: -125.45\t Noise: 0.92\n",
      "Episode 82\tLast length:  1599\t Reward: -151.39\t Avg Reward: -125.76\t Noise: 0.92\n",
      "Episode 83\tLast length:  1599\t Reward: -154.20\t Avg Reward: -126.10\t Noise: 0.92\n",
      "Episode 84\tLast length:  1599\t Reward: -147.74\t Avg Reward: -126.35\t Noise: 0.92\n",
      "Episode 85\tLast length:  1599\t Reward: -145.88\t Avg Reward: -126.58\t Noise: 0.92\n",
      "Episode 86\tLast length:    45\t Reward: -123.56\t Avg Reward: -126.55\t Noise: 0.91\n",
      "Episode 87\tLast length:  1599\t Reward: -148.67\t Avg Reward: -126.80\t Noise: 0.91\n",
      "Episode 88\tLast length:  1599\t Reward: -148.20\t Avg Reward: -127.04\t Noise: 0.91\n",
      "Episode 89\tLast length:  1599\t Reward: -150.16\t Avg Reward: -127.30\t Noise: 0.91\n",
      "Episode 90\tLast length:    52\t Reward: -123.42\t Avg Reward: -127.25\t Noise: 0.91\n",
      "Episode 91\tLast length:  1599\t Reward: -144.17\t Avg Reward: -127.44\t Noise: 0.91\n",
      "Episode 92\tLast length:  1599\t Reward: -149.26\t Avg Reward: -127.67\t Noise: 0.91\n",
      "Episode 93\tLast length:  1599\t Reward: -150.23\t Avg Reward: -127.91\t Noise: 0.91\n",
      "Episode 94\tLast length:  1599\t Reward: -148.67\t Avg Reward: -128.13\t Noise: 0.91\n",
      "Episode 95\tLast length:  1599\t Reward: -149.07\t Avg Reward: -128.35\t Noise: 0.91\n",
      "Episode 96\tLast length:  1599\t Reward: -149.04\t Avg Reward: -128.56\t Noise: 0.90\n",
      "Episode 97\tLast length:   105\t Reward: -127.23\t Avg Reward: -128.55\t Noise: 0.90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 98\tLast length:   129\t Reward: -129.83\t Avg Reward: -128.56\t Noise: 0.90\n",
      "Episode 99\tLast length:  1599\t Reward: -150.79\t Avg Reward: -128.78\t Noise: 0.90\n",
      "Episode 100\tLast length:  1599\t Reward: -145.95\t Avg Reward: -128.66\t Noise: 0.90\n",
      "Episode 101\tLast length:  1599\t Reward: -148.81\t Avg Reward: -129.05\t Noise: 0.90\n",
      "Episode 102\tLast length:  1599\t Reward: -149.37\t Avg Reward: -129.39\t Noise: 0.90\n",
      "Episode 103\tLast length:    41\t Reward: -112.33\t Avg Reward: -129.38\t Noise: 0.90\n",
      "Episode 104\tLast length:  1599\t Reward: -150.69\t Avg Reward: -129.77\t Noise: 0.90\n",
      "Episode 105\tLast length:  1599\t Reward: -154.84\t Avg Reward: -130.23\t Noise: 0.90\n",
      "Episode 106\tLast length:  1599\t Reward: -142.08\t Avg Reward: -130.47\t Noise: 0.89\n",
      "Episode 107\tLast length:  1599\t Reward: -150.30\t Avg Reward: -130.88\t Noise: 0.89\n",
      "Episode 108\tLast length:  1599\t Reward: -151.19\t Avg Reward: -131.22\t Noise: 0.89\n",
      "Episode 109\tLast length:  1599\t Reward: -149.02\t Avg Reward: -131.63\t Noise: 0.89\n",
      "Episode 110\tLast length:  1599\t Reward: -149.53\t Avg Reward: -132.00\t Noise: 0.89\n",
      "Episode 111\tLast length:  1599\t Reward: -151.85\t Avg Reward: -132.32\t Noise: 0.89\n",
      "Episode 112\tLast length:  1599\t Reward: -150.99\t Avg Reward: -132.76\t Noise: 0.89\n",
      "Episode 113\tLast length:  1599\t Reward: -146.72\t Avg Reward: -133.11\t Noise: 0.89\n",
      "Episode 114\tLast length:  1599\t Reward: -149.95\t Avg Reward: -133.46\t Noise: 0.89\n",
      "Episode 115\tLast length:  1599\t Reward: -149.07\t Avg Reward: -133.84\t Noise: 0.89\n",
      "Episode 116\tLast length:  1599\t Reward: -151.07\t Avg Reward: -134.30\t Noise: 0.88\n",
      "Episode 117\tLast length:  1599\t Reward: -151.32\t Avg Reward: -134.67\t Noise: 0.88\n",
      "Episode 118\tLast length:  1599\t Reward: -150.91\t Avg Reward: -135.02\t Noise: 0.88\n",
      "Episode 119\tLast length:  1599\t Reward: -148.68\t Avg Reward: -135.35\t Noise: 0.88\n",
      "Episode 120\tLast length:  1599\t Reward: -150.84\t Avg Reward: -135.75\t Noise: 0.88\n",
      "Episode 121\tLast length:  1599\t Reward: -156.31\t Avg Reward: -136.03\t Noise: 0.88\n",
      "Episode 122\tLast length:  1599\t Reward: -150.54\t Avg Reward: -135.71\t Noise: 0.88\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-23b2864d8bd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"obs_space={}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"threshold={} \\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-23b2864d8bd3>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(agent)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'render'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mrunning_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/reinforcement/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/reinforcement/lib/python3.6/site-packages/gym/envs/box2d/bipedal_walker.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_polyline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/reinforcement/lib/python3.6/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monetime_geoms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_rgb_array\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/reinforcement/lib/python3.6/site-packages/pyglet/window/xlib/__init__.py\u001b[0m in \u001b[0;36mflip\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0;31m# TODO canvas.flip?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sync_resize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/reinforcement/lib/python3.6/site-packages/pyglet/gl/xlib.py\u001b[0m in \u001b[0;36mflip\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vsync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_vsync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m         \u001b[0mglx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglXSwapBuffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_display\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglx_window\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/reinforcement/lib/python3.6/site-packages/pyglet/gl/xlib.py\u001b[0m in \u001b[0;36m_wait_vsync\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_uint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0mglxext_arb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglXGetVideoSyncSGI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m             \u001b[0mglxext_arb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglXWaitVideoSyncSGI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "\n",
    "from TD3_keras.td3 import TD3\n",
    "\n",
    "args = {\n",
    "    'render': True,\n",
    "    'log_interval': 1\n",
    "}\n",
    "\n",
    "env = gym.make('BipedalWalker-v2')\n",
    "episodes = 100000\n",
    "reward_history = []\n",
    "\n",
    "task = {\n",
    "        'state_size': 24,\n",
    "        'action_size': 4,\n",
    "        'action_high': 1,\n",
    "        'action_low': -1\n",
    "    }\n",
    "agent = TD3(task)    \n",
    "\n",
    "\n",
    "def main(agent):   \n",
    "    \n",
    "    \n",
    "    for i_episode in range(episodes):\n",
    "        running_reward = 0        \n",
    "        state = env.reset()\n",
    "        for t in range(10000):  # Don't infinite loop while learning\n",
    "            action, noise_coeff = agent.act(state, i_episode)                \n",
    "            state, reward, done, _ = env.step(action)  \n",
    "            agent.step(action, reward, state, done, t)\n",
    "            if args['render']:\n",
    "                env.render()                   \n",
    "            running_reward += reward            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        reward_history.append(running_reward)\n",
    "        \n",
    "        if i_episode % args['log_interval'] == 0:\n",
    "            avg_reward = np.mean(reward_history[-100:])            \n",
    "            print('Episode {}\\tLast length: {:5d}\\t Reward: {:7.2f}\\t Avg Reward: {:7.2f}\\t Noise: {:.2f}'.format(\n",
    "                i_episode, t, running_reward, avg_reward, noise_coeff))\n",
    "        if avg_reward > env.spec.reward_threshold and i_episode > 100:\n",
    "            print(\"Solved! Average 100-episode reward is now {}!\".format(avg_reward))\n",
    "            break\n",
    "            \n",
    "print(\"action_space={}\".format(env.action_space))\n",
    "print(\"obs_space={}\".format(env.observation_space))\n",
    "print(\"threshold={} \\n\".format(env.spec.reward_threshold))\n",
    "main(agent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def test(agent):   \n",
    "    random_seed = 0\n",
    "    episodes = 3\n",
    "    max_timesteps = 2000\n",
    "    render = True\n",
    "    save_gif = True\n",
    "     \n",
    "    for i_episode in range(1, episodes):\n",
    "        running_reward = 0        \n",
    "        state = env.reset()\n",
    "        for t in range(10000):  # Don't infinite loop while learning\n",
    "            action, noise_coeff = agent.act(state, i_episode)                \n",
    "            state, reward, done, _ = env.step(action)  \n",
    "            agent.step(action, reward, state, done, t)\n",
    "            if args['render']:\n",
    "                env.render()  \n",
    "                if save_gif:\n",
    "                    dirname = './gif/td3_keras/{}'.format(i_episode)\n",
    "                    if not os.path.isdir(dirname):\n",
    "                        os.mkdir(dirname)\n",
    "                    img = env.render(mode = 'rgb_array')\n",
    "                    img = Image.fromarray(img)\n",
    "                    img.save('./gif/td3_keras/{}/{}.jpg'.format(i_episode,t))\n",
    "            running_reward += reward            \n",
    "            if done:\n",
    "                break    \n",
    "   \n",
    "            \n",
    "        print('Episode: {}\\tReward: {}'.format(i_episode, int(running_reward)))\n",
    "        running_reward = 0\n",
    "        env.close()        \n",
    "                \n",
    "test(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
