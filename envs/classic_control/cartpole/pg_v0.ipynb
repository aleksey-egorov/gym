{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "from gym import wrappers\n",
    "from PIL import Image\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "import ptan\n",
    "\n",
    "from PG.pg import PG\n",
    "from PG.utils import mkdir\n",
    "from PG.buffer import MeanBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v0'\n",
    "lr_base = 0.001\n",
    "lr_decay = 0.0001\n",
    "\n",
    "random_seed = 42\n",
    "gamma = 0.99                # discount for future rewards\n",
    "batch_size = 32         # num of transitions sampled from replay buffer\n",
    "\n",
    "max_episodes = 100000         # max num of episodes\n",
    "max_timesteps = 3000        # max timesteps in one episode\n",
    "log_interval = 50           # print avg reward after interval\n",
    "\n",
    "entropy_beta = 0.01\n",
    "bellman_steps = 10\n",
    "baseline_steps = 50000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = [\n",
    "     {'dim': [None, 128], 'dropout': False, 'activation': 'relu'},    \n",
    "     {'dim': [128, None], 'dropout': False, 'activation': False},    \n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PG_Trainer():\n",
    "    \n",
    "    def __init__(self, env_name, config, random_seed=42, lr_base=0.001, lr_decay=0.00005, \n",
    "                 gamma=0.99, batch_size=32, \n",
    "                 max_episodes=100000, max_timesteps=3000, \n",
    "                 log_interval=5, threshold=None, lr_minimum=1e-10, \n",
    "                 entropy_beta=0.01, bellman_steps=10, baseline_steps=50000, log_dir='./log/'):\n",
    "                \n",
    "        self.algorithm_name = 'pg'\n",
    "        self.env_name = env_name\n",
    "        self.env = gym.make(env_name)\n",
    "        self.log_dir = os.path.join(log_dir, self.algorithm_name)\n",
    "        self.writer = SummaryWriter(log_dir=self.log_dir, comment=\"-cartpole-pg-v0\")\n",
    "               \n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.n    \n",
    "        self.should_record = False\n",
    "        if not threshold == None:\n",
    "            self.threshold = threshold\n",
    "        else:    \n",
    "            self.threshold = self.env.spec.reward_threshold\n",
    "              \n",
    "        self.config = config\n",
    "        self.config[0]['dim'][0] = self.state_dim\n",
    "        self.config[-1]['dim'][1] = self.action_dim      \n",
    "        \n",
    "        \n",
    "        self.random_seed = random_seed\n",
    "        self.lr_base = lr_base\n",
    "        self.lr_decay = lr_decay\n",
    "        self.lr_minimum = lr_minimum        \n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size   \n",
    "        \n",
    "        self.entropy_beta = entropy_beta\n",
    "        self.bellman_steps = bellman_steps\n",
    "        self.baseline_steps = baseline_steps        \n",
    "        \n",
    "        self.max_episodes = max_episodes\n",
    "        self.max_timesteps = max_timesteps\n",
    "        self.log_interval = log_interval\n",
    "               \n",
    "        prdir = mkdir('.', 'preTrained')\n",
    "        self.directory = mkdir(prdir, self.algorithm_name)\n",
    "        self.filename = \"{}_{}_{}\".format(self.algorithm_name, self.env_name, self.random_seed)\n",
    "                       \n",
    "        self.policy = PG(self.env, self.config, self.gamma, self.bellman_steps)   \n",
    "        \n",
    "        # The experience source interacts with the environment and returns (s,a,r,s') transitions\n",
    "        self.exp_source = ptan.experience.ExperienceSourceFirstLast(self.env, self.policy.ptan_agent,\n",
    "                                                                    gamma=self.gamma,\n",
    "                                                                    steps_count=self.bellman_steps)\n",
    "        \n",
    "        self.baseline_buffer = MeanBuffer(self.baseline_steps)\n",
    "      \n",
    "        \n",
    "        self.reward_history = []\n",
    "        self.make_plots = False       \n",
    "        \n",
    "        if self.random_seed:\n",
    "            print(\"Random Seed: {}\".format(self.random_seed))\n",
    "            self.env.seed(self.random_seed)\n",
    "            torch.manual_seed(self.random_seed)\n",
    "            np.random.seed(self.random_seed)\n",
    "            \n",
    "    def train(self):\n",
    "        \n",
    "        start_time = time.time()        \n",
    "        print(\"action_space={}\".format(self.env.action_space))\n",
    "        print(\"obs_space={}\".format(self.env.observation_space))\n",
    "        print(\"threshold={} \\n\".format(self.threshold))        \n",
    "\n",
    "        # loading models\n",
    "        self.policy.load(self.directory, self.filename)                   \n",
    "        \n",
    "        print(\"\\nTraining started ... \")\n",
    "        avg_loss = 0\n",
    "        total_rewards = []\n",
    "        step_rewards = []        \n",
    "        step_idx = 0\n",
    "        episode = 0\n",
    "              \n",
    "        batch_states, batch_actions, batch_scales = [], [], []\n",
    "        learning_rate = self.lr_base\n",
    "        self.policy.set_optimizers(lr=learning_rate)\n",
    "\n",
    "        # each iteration runs one action in the environment and returns a (s,a,r,s') transition\n",
    "        for step_idx, exp in enumerate(self.exp_source):\n",
    "            self.baseline_buffer.add(exp.reward)\n",
    "            baseline = self.baseline_buffer.mean()\n",
    "            self.writer.add_scalar(\"baseline\", baseline, step_idx)\n",
    "            \n",
    "            batch_states.append(exp.state)\n",
    "            batch_actions.append(int(exp.action))\n",
    "            batch_scales.append(exp.reward - baseline)\n",
    "\n",
    "            # handle when an episode is completed\n",
    "            episode_rewards = self.exp_source.pop_total_rewards()\n",
    "            if episode_rewards:\n",
    "                episode += 1\n",
    "                reward = episode_rewards[0]\n",
    "                total_rewards.append(reward)\n",
    "                avg_reward = float(np.mean(total_rewards[-100:]))\n",
    "                \n",
    "                if len(self.policy.loss_list) > 0:               \n",
    "                    avg_loss = np.mean(self.policy.loss_list[-100:])     \n",
    "                \n",
    "                # Print avg reward every log interval:\n",
    "                if episode % self.log_interval == 0:            \n",
    "                    self.policy.save(self.directory, self.filename)\n",
    "                    print(\"Ep:{:5d}  Rew:{:8.2f}  Avg Rew:{:8.2f}  LR:{:8.8f}  Loss: {:8.6f}\".format(\n",
    "                        episode, reward, avg_reward, learning_rate, avg_loss))\n",
    "                \n",
    "               \n",
    "                \n",
    "                learning_rate = max(self.lr_base / (1.0 + episode * self.lr_decay), self.lr_minimum) \n",
    "                                              \n",
    "                self.writer.add_scalar(\"reward\", reward, step_idx)\n",
    "                self.writer.add_scalar(\"reward_100\", avg_reward, step_idx)\n",
    "                self.writer.add_scalar(\"episodes\", episode, step_idx)\n",
    "                \n",
    "                # if avg reward > threshold then save and stop traning:\n",
    "                if avg_reward >= self.threshold and episode > 100: \n",
    "                    print(\"Ep:{:5d}  Rew:{:8.2f}  Avg Rew:{:8.2f}  LR:{:8.8f}  Loss: {:8.6f}\".format(\n",
    "                        episode, reward, avg_reward, learning_rate, avg_loss))\n",
    "                    print(\"########## Solved! ###########\")\n",
    "                    name = self.filename + '_solved'\n",
    "                    self.policy.save(self.directory, name)                   \n",
    "                    self.env.close()  \n",
    "                    training_time = time.time() - start_time\n",
    "                    print(\"Training time: {:6.2f} sec\".format(training_time))\n",
    "                    break    \n",
    "\n",
    "            if len(batch_states) < self.batch_size:\n",
    "                continue\n",
    "            \n",
    "            scalars = self.policy.update(batch_states, batch_actions, batch_scales, self.batch_size, self.entropy_beta)        \n",
    "            entropy_v, entropy_loss_v, loss_policy_v, loss_v, grad_means, grad_count, grad_max = scalars        \n",
    "                           \n",
    "            self.writer.add_scalar(\"baseline\", baseline, step_idx)\n",
    "            self.writer.add_scalar(\"entropy\", entropy_v.item(), step_idx)\n",
    "            self.writer.add_scalar(\"batch_scales\", np.mean(batch_scales), step_idx)\n",
    "            self.writer.add_scalar(\"loss_entropy\", entropy_loss_v.item(), step_idx)\n",
    "            self.writer.add_scalar(\"loss_policy\", loss_policy_v.item(), step_idx)\n",
    "            self.writer.add_scalar(\"loss_total\", loss_v.item(), step_idx)\n",
    "            self.writer.add_scalar(\"grad_l2\", grad_means / grad_count, step_idx)\n",
    "            self.writer.add_scalar(\"grad_max\", grad_max, step_idx)\n",
    "\n",
    "            batch_states.clear()\n",
    "            batch_actions.clear()\n",
    "            batch_scales.clear()\n",
    "            \n",
    "        self.writer.export_scalars_to_json(os.path.join(self.log_dir, \"all_scalars.json\"))    \n",
    "        self.writer.close()    \n",
    "\n",
    "            \n",
    "                                \n",
    "    def test(self, episodes=3, render=True, save_gif=True):              \n",
    "\n",
    "        gifdir = mkdir('.','gif')\n",
    "        algdir = mkdir(gifdir, self.algorithm_name)\n",
    "        \n",
    "        t = 0\n",
    "        for episode in range(1, episodes+1):\n",
    "            ep_reward = 0.0            \n",
    "            epdir = mkdir(algdir, str(episode))\n",
    "        \n",
    "            for step_idx, exp in enumerate(self.exp_source):\n",
    "                self.baseline_buffer.add(exp.reward)\n",
    "                baseline = self.baseline_buffer.mean()\n",
    "                #writer.add_scalar(\"baseline\", baseline, step_idx)\n",
    "\n",
    "                if save_gif:                                       \n",
    "                    img = self.env.render(mode = 'rgb_array')\n",
    "                    img = Image.fromarray(img)\n",
    "                    img.save('{}/{}.jpg'.format(epdir, t))\n",
    "                t+= 1\n",
    "                    \n",
    "                # handle when an episode is completed\n",
    "                episode_rewards = self.exp_source.pop_total_rewards()\n",
    "                if episode_rewards:\n",
    "                    ep_reward = episode_rewards[0]\n",
    "                    t = 0\n",
    "                    break\n",
    "        \n",
    "            print('Test episode: {}\\tReward: {:4.2f}'.format(episode, ep_reward))           \n",
    "            self.env.close()        \n",
    "                   \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NETWORK: Sequential(\n",
      "  (0): Linear(in_features=4, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=2, bias=True)\n",
      ") Device: cuda:0\n",
      "Random Seed: 42\n",
      "action_space=Discrete(2)\n",
      "obs_space=Box(4,)\n",
      "threshold=195.0 \n",
      "\n",
      "DIR=./preTrained/pg NAME=pg_CartPole-v0_42\n",
      "No models to load\n",
      "\n",
      "Training started ... \n",
      "Ep:   50  Rew:   10.00  Avg Rew:   23.24  LR:0.00099512  Loss: -0.106356\n",
      "Ep:  100  Rew:   30.00  Avg Rew:   29.69  LR:0.00099020  Loss: 0.099704\n",
      "Ep:  150  Rew:   61.00  Avg Rew:   42.19  LR:0.00098532  Loss: 0.266764\n",
      "Ep:  200  Rew:  139.00  Avg Rew:   62.00  LR:0.00098049  Loss: 0.256482\n",
      "Ep:  250  Rew:   95.00  Avg Rew:  106.94  LR:0.00097570  Loss: 0.223954\n",
      "Ep:  300  Rew:  155.00  Avg Rew:  152.33  LR:0.00097097  Loss: 0.165062\n",
      "Ep:  350  Rew:  110.00  Avg Rew:  168.97  LR:0.00096628  Loss: 0.119042\n",
      "Ep:  400  Rew:  176.00  Avg Rew:  171.86  LR:0.00096163  Loss: 0.063070\n",
      "Ep:  450  Rew:  193.00  Avg Rew:  175.72  LR:0.00095703  Loss: 0.048297\n",
      "Ep:  500  Rew:  200.00  Avg Rew:  186.35  LR:0.00095247  Loss: 0.008365\n",
      "Ep:  548  Rew:  200.00  Avg Rew:  195.22  LR:0.00094805  Loss: 0.009231\n",
      "########## Solved! ###########\n",
      "Training time:  97.59 sec\n"
     ]
    }
   ],
   "source": [
    "agent = PG_Trainer(env_name, config, random_seed=random_seed, lr_base=lr_base, lr_decay=lr_decay, \n",
    "                   gamma=gamma, batch_size=batch_size,\n",
    "                   max_episodes=max_episodes, max_timesteps=max_timesteps, \n",
    "                   log_interval=log_interval, entropy_beta=entropy_beta, \n",
    "                   bellman_steps=bellman_steps, baseline_steps=baseline_steps\n",
    "                   )\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test episode: 1\tReward: 200.00\n",
      "Test episode: 2\tReward: 200.00\n",
      "Test episode: 3\tReward: 200.00\n"
     ]
    }
   ],
   "source": [
    "agent.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
