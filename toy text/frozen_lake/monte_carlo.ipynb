{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "episodes = 5000000\n",
    "reward_history = []\n",
    "EPSILON = 0.3\n",
    "GAMMA = 0.9\n",
    "render = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_dict(d):\n",
    "    # returns the argmax (key) and max (value) from a dictionary   \n",
    "    max_key = None\n",
    "    max_val = float('-inf')\n",
    "    for k, v in d.items():\n",
    "        if v > max_val:\n",
    "            max_val = v\n",
    "            max_key = k\n",
    "    return max_key, max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(values, n=100) :\n",
    "    ret = np.cumsum(values, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_action(a, eps=0.1):\n",
    "    p = np.random.random()\n",
    "    if p < (1 - eps):\n",
    "        return a\n",
    "    else:\n",
    "        return env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_returns(states_actions_rewards):\n",
    "    # calculate the returns by working backwards from the terminal state\n",
    "    G = 0\n",
    "    states_actions_returns = []\n",
    "    first = True\n",
    "    for s, a, r in reversed(states_actions_rewards):\n",
    "        # a terminal state has a value of 0 by definition\n",
    "        # this is the first state we encounter in the reversed list\n",
    "        # we'll ignore its return (G) since it doesn't correspond to any move\n",
    "        if first:\n",
    "            first = False\n",
    "        else:\n",
    "            states_actions_returns.append((s, a, G))\n",
    "        G = r + GAMMA*G\n",
    "    states_actions_returns.reverse() # back to the original order of states visited\n",
    "\n",
    "    return states_actions_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_space=Discrete(4)\n",
      "obs_space=Discrete(16)\n",
      "threshold=0.78 \n",
      "\n",
      "Initial policy values: {0: 0, 1: 3, 2: 1, 3: 0, 4: 3, 5: 3, 6: 3, 7: 3, 8: 1, 9: 3, 10: 1, 11: 2, 12: 0, 13: 3, 14: 2, 15: 0} \n",
      "\n",
      "\n",
      "Episode: 0   Avg reward: 0.000  Avg Delta: 0.00000000  Epsilon: 0.300\n",
      "Episode: 5000   Avg reward: 0.072  Avg Delta: 0.00100847  Epsilon: 0.295\n",
      "Episode: 10000   Avg reward: 0.056  Avg Delta: 0.00029844  Epsilon: 0.290\n",
      "Episode: 15000   Avg reward: 0.060  Avg Delta: 0.00022527  Epsilon: 0.285\n",
      "Episode: 20000   Avg reward: 0.078  Avg Delta: 0.00016754  Epsilon: 0.280\n",
      "Episode: 25000   Avg reward: 0.084  Avg Delta: 0.00014455  Epsilon: 0.275\n",
      "Episode: 30000   Avg reward: 0.100  Avg Delta: 0.00011969  Epsilon: 0.270\n",
      "Episode: 35000   Avg reward: 0.098  Avg Delta: 0.00010169  Epsilon: 0.265\n",
      "Episode: 40000   Avg reward: 0.156  Avg Delta: 0.00012559  Epsilon: 0.260\n",
      "Episode: 45000   Avg reward: 0.148  Avg Delta: 0.00009544  Epsilon: 0.255\n",
      "Episode: 50000   Avg reward: 0.164  Avg Delta: 0.00009022  Epsilon: 0.250\n",
      "Episode: 55000   Avg reward: 0.152  Avg Delta: 0.00007615  Epsilon: 0.245\n",
      "Episode: 60000   Avg reward: 0.148  Avg Delta: 0.00006682  Epsilon: 0.240\n",
      "Episode: 65000   Avg reward: 0.166  Avg Delta: 0.00005817  Epsilon: 0.235\n",
      "Episode: 70000   Avg reward: 0.190  Avg Delta: 0.00005787  Epsilon: 0.230\n",
      "Episode: 75000   Avg reward: 0.178  Avg Delta: 0.00004811  Epsilon: 0.225\n",
      "Episode: 80000   Avg reward: 0.194  Avg Delta: 0.00004350  Epsilon: 0.220\n",
      "Episode: 85000   Avg reward: 0.164  Avg Delta: 0.00003684  Epsilon: 0.215\n",
      "Episode: 90000   Avg reward: 0.206  Avg Delta: 0.00004338  Epsilon: 0.210\n",
      "Episode: 95000   Avg reward: 0.178  Avg Delta: 0.00003447  Epsilon: 0.205\n",
      "Episode: 100000   Avg reward: 0.220  Avg Delta: 0.00003918  Epsilon: 0.200\n",
      "Episode: 105000   Avg reward: 0.204  Avg Delta: 0.00003324  Epsilon: 0.195\n",
      "Episode: 110000   Avg reward: 0.202  Avg Delta: 0.00002990  Epsilon: 0.190\n",
      "Episode: 115000   Avg reward: 0.220  Avg Delta: 0.00002945  Epsilon: 0.185\n",
      "Episode: 120000   Avg reward: 0.200  Avg Delta: 0.00002780  Epsilon: 0.180\n",
      "Episode: 125000   Avg reward: 0.252  Avg Delta: 0.00003015  Epsilon: 0.175\n",
      "Episode: 130000   Avg reward: 0.230  Avg Delta: 0.00003001  Epsilon: 0.170\n",
      "Episode: 135000   Avg reward: 0.222  Avg Delta: 0.00002543  Epsilon: 0.165\n",
      "Episode: 140000   Avg reward: 0.238  Avg Delta: 0.00002495  Epsilon: 0.160\n",
      "Episode: 145000   Avg reward: 0.250  Avg Delta: 0.00002430  Epsilon: 0.155\n",
      "Episode: 150000   Avg reward: 0.252  Avg Delta: 0.00002393  Epsilon: 0.150\n",
      "Episode: 155000   Avg reward: 0.220  Avg Delta: 0.00002058  Epsilon: 0.145\n",
      "Episode: 160000   Avg reward: 0.246  Avg Delta: 0.00002268  Epsilon: 0.140\n",
      "Episode: 165000   Avg reward: 0.252  Avg Delta: 0.00002211  Epsilon: 0.135\n",
      "Episode: 170000   Avg reward: 0.242  Avg Delta: 0.00001992  Epsilon: 0.130\n",
      "Episode: 175000   Avg reward: 0.258  Avg Delta: 0.00001788  Epsilon: 0.125\n",
      "Episode: 180000   Avg reward: 0.276  Avg Delta: 0.00001877  Epsilon: 0.120\n",
      "Episode: 185000   Avg reward: 0.328  Avg Delta: 0.00001933  Epsilon: 0.115\n",
      "Episode: 190000   Avg reward: 0.282  Avg Delta: 0.00001789  Epsilon: 0.110\n",
      "Episode: 195000   Avg reward: 0.286  Avg Delta: 0.00001761  Epsilon: 0.105\n",
      "Episode: 200000   Avg reward: 0.284  Avg Delta: 0.00001573  Epsilon: 0.100\n",
      "Episode: 205000   Avg reward: 0.306  Avg Delta: 0.00001662  Epsilon: 0.095\n",
      "Episode: 210000   Avg reward: 0.384  Avg Delta: 0.00001487  Epsilon: 0.090\n",
      "Episode: 215000   Avg reward: 0.342  Avg Delta: 0.00001488  Epsilon: 0.085\n",
      "Episode: 220000   Avg reward: 0.358  Avg Delta: 0.00001353  Epsilon: 0.080\n",
      "Episode: 225000   Avg reward: 0.340  Avg Delta: 0.00001340  Epsilon: 0.075\n",
      "Episode: 230000   Avg reward: 0.328  Avg Delta: 0.00001212  Epsilon: 0.070\n",
      "Episode: 235000   Avg reward: 0.332  Avg Delta: 0.00001345  Epsilon: 0.065\n",
      "Episode: 240000   Avg reward: 0.424  Avg Delta: 0.00001307  Epsilon: 0.060\n",
      "Episode: 245000   Avg reward: 0.380  Avg Delta: 0.00001172  Epsilon: 0.055\n",
      "Episode: 250000   Avg reward: 0.372  Avg Delta: 0.00000932  Epsilon: 0.050\n",
      "Episode: 255000   Avg reward: 0.438  Avg Delta: 0.00000947  Epsilon: 0.045\n",
      "Episode: 260000   Avg reward: 0.396  Avg Delta: 0.00000903  Epsilon: 0.040\n",
      "Episode: 265000   Avg reward: 0.408  Avg Delta: 0.00000748  Epsilon: 0.035\n",
      "Episode: 270000   Avg reward: 0.432  Avg Delta: 0.00000888  Epsilon: 0.030\n",
      "Episode: 275000   Avg reward: 0.424  Avg Delta: 0.00000845  Epsilon: 0.025\n",
      "Episode: 280000   Avg reward: 0.416  Avg Delta: 0.00000653  Epsilon: 0.020\n",
      "Episode: 285000   Avg reward: 0.448  Avg Delta: 0.00000664  Epsilon: 0.015\n",
      "Episode: 290000   Avg reward: 0.482  Avg Delta: 0.00000570  Epsilon: 0.010\n",
      "Episode: 295000   Avg reward: 0.472  Avg Delta: 0.00000397  Epsilon: 0.005\n",
      "Episode: 300000   Avg reward: 0.498  Avg Delta: 0.00000348  Epsilon: 0.000\n",
      "Episode: 305000   Avg reward: 0.524  Avg Delta: 0.00000327  Epsilon: 0.000\n",
      "Episode: 310000   Avg reward: 0.510  Avg Delta: 0.00000328  Epsilon: 0.000\n",
      "Episode: 315000   Avg reward: 0.456  Avg Delta: 0.00000301  Epsilon: 0.000\n",
      "Episode: 320000   Avg reward: 0.506  Avg Delta: 0.00000303  Epsilon: 0.000\n",
      "Episode: 325000   Avg reward: 0.546  Avg Delta: 0.00000306  Epsilon: 0.000\n",
      "Episode: 330000   Avg reward: 0.514  Avg Delta: 0.00000287  Epsilon: 0.000\n",
      "Episode: 335000   Avg reward: 0.480  Avg Delta: 0.00000267  Epsilon: 0.000\n",
      "Episode: 340000   Avg reward: 0.524  Avg Delta: 0.00000284  Epsilon: 0.000\n",
      "Episode: 345000   Avg reward: 0.510  Avg Delta: 0.00000272  Epsilon: 0.000\n",
      "Episode: 350000   Avg reward: 0.520  Avg Delta: 0.00000265  Epsilon: 0.000\n",
      "Episode: 355000   Avg reward: 0.528  Avg Delta: 0.00000266  Epsilon: 0.000\n",
      "Episode: 360000   Avg reward: 0.482  Avg Delta: 0.00000247  Epsilon: 0.000\n",
      "Episode: 365000   Avg reward: 0.516  Avg Delta: 0.00000250  Epsilon: 0.000\n",
      "Episode: 370000   Avg reward: 0.526  Avg Delta: 0.00000239  Epsilon: 0.000\n",
      "Episode: 375000   Avg reward: 0.492  Avg Delta: 0.00000225  Epsilon: 0.000\n",
      "Episode: 380000   Avg reward: 0.518  Avg Delta: 0.00000224  Epsilon: 0.000\n",
      "Episode: 385000   Avg reward: 0.498  Avg Delta: 0.00000232  Epsilon: 0.000\n",
      "Episode: 390000   Avg reward: 0.506  Avg Delta: 0.00000223  Epsilon: 0.000\n",
      "Episode: 395000   Avg reward: 0.528  Avg Delta: 0.00000220  Epsilon: 0.000\n",
      "Episode: 400000   Avg reward: 0.472  Avg Delta: 0.00000206  Epsilon: 0.000\n",
      "Episode: 405000   Avg reward: 0.494  Avg Delta: 0.00000208  Epsilon: 0.000\n",
      "Episode: 410000   Avg reward: 0.518  Avg Delta: 0.00000203  Epsilon: 0.000\n",
      "Episode: 415000   Avg reward: 0.494  Avg Delta: 0.00000207  Epsilon: 0.000\n",
      "Episode: 420000   Avg reward: 0.536  Avg Delta: 0.00000208  Epsilon: 0.000\n",
      "Episode: 425000   Avg reward: 0.538  Avg Delta: 0.00000207  Epsilon: 0.000\n",
      "Episode: 430000   Avg reward: 0.520  Avg Delta: 0.00000194  Epsilon: 0.000\n",
      "Episode: 435000   Avg reward: 0.476  Avg Delta: 0.00000185  Epsilon: 0.000\n",
      "Episode: 440000   Avg reward: 0.492  Avg Delta: 0.00000187  Epsilon: 0.000\n",
      "Episode: 445000   Avg reward: 0.512  Avg Delta: 0.00000186  Epsilon: 0.000\n",
      "Episode: 450000   Avg reward: 0.516  Avg Delta: 0.00000186  Epsilon: 0.000\n",
      "Episode: 455000   Avg reward: 0.522  Avg Delta: 0.00000188  Epsilon: 0.000\n",
      "Episode: 460000   Avg reward: 0.540  Avg Delta: 0.00000186  Epsilon: 0.000\n",
      "Episode: 465000   Avg reward: 0.468  Avg Delta: 0.00000172  Epsilon: 0.000\n",
      "Episode: 470000   Avg reward: 0.506  Avg Delta: 0.00000173  Epsilon: 0.000\n",
      "Episode: 475000   Avg reward: 0.498  Avg Delta: 0.00000167  Epsilon: 0.000\n",
      "Episode: 480000   Avg reward: 0.494  Avg Delta: 0.00000170  Epsilon: 0.000\n",
      "Episode: 485000   Avg reward: 0.498  Avg Delta: 0.00000170  Epsilon: 0.000\n",
      "Episode: 490000   Avg reward: 0.522  Avg Delta: 0.00000159  Epsilon: 0.000\n",
      "Episode: 495000   Avg reward: 0.492  Avg Delta: 0.00000162  Epsilon: 0.000\n",
      "Episode: 500000   Avg reward: 0.546  Avg Delta: 0.00000168  Epsilon: 0.000\n",
      "Episode: 505000   Avg reward: 0.520  Avg Delta: 0.00000153  Epsilon: 0.000\n",
      "Episode: 510000   Avg reward: 0.502  Avg Delta: 0.00000154  Epsilon: 0.000\n",
      "Episode: 515000   Avg reward: 0.542  Avg Delta: 0.00000152  Epsilon: 0.000\n",
      "Episode: 520000   Avg reward: 0.510  Avg Delta: 0.00000154  Epsilon: 0.000\n",
      "Episode: 525000   Avg reward: 0.526  Avg Delta: 0.00000149  Epsilon: 0.000\n",
      "Episode: 530000   Avg reward: 0.506  Avg Delta: 0.00000145  Epsilon: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 535000   Avg reward: 0.510  Avg Delta: 0.00000152  Epsilon: 0.000\n",
      "Episode: 540000   Avg reward: 0.538  Avg Delta: 0.00000145  Epsilon: 0.000\n",
      "Episode: 545000   Avg reward: 0.532  Avg Delta: 0.00000147  Epsilon: 0.000\n",
      "Episode: 550000   Avg reward: 0.504  Avg Delta: 0.00000139  Epsilon: 0.000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7264a1d95f71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"obs_space={}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"threshold={} \\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Policy values: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-7264a1d95f71>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mold_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0;31m# the new Q[s][a] is the sample mean of all our returns for that (state, action)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m                 \u001b[0mbiggest_change\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbiggest_change\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_q\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mseen_state_action_pairs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/reinforcement/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   3116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3117\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 3118\u001b[0;31m                           out=out, **kwargs)\n\u001b[0m\u001b[1;32m   3119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/reinforcement/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mis_float16_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/reinforcement/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masanyarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m     \"\"\"\n\u001b[0;32m--> 591\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    eps = EPSILON\n",
    "    R = {}\n",
    "    Q = {}\n",
    "    P = {}\n",
    "    deltas = []\n",
    "    rewards_list = []\n",
    "    \n",
    "    for s in range(env.observation_space.n):\n",
    "        P[s] = env.action_space.sample()\n",
    "        Q[s] = {}\n",
    "        for a in range(env.action_space.n):\n",
    "            Q[s][a] = 0\n",
    "            R[(s,a)] = []\n",
    "            \n",
    "    print(\"Initial policy values: {} \\n\\n\".format(P))         \n",
    "        \n",
    "    for i_episode in range(episodes):            \n",
    "        \n",
    "        # Play episode\n",
    "        state = env.reset()        \n",
    "        reward_per_episode = 0  \n",
    "        states_actions_returns = []\n",
    "        action = epsilon_action(P[state], eps)\n",
    "        states_actions_rewards = [(state, action, 0)]\n",
    "        for t in range(10000):  # Don't infinite loop while learning                           \n",
    "            state, reward, done, _ = env.step(action) \n",
    "            reward_per_episode += reward\n",
    "            if render:\n",
    "                env.render()                           \n",
    "            if done:\n",
    "                states_actions_rewards.append((state, None, reward))\n",
    "                break\n",
    "            else:\n",
    "                action = epsilon_action(P[state], eps)\n",
    "                states_actions_rewards.append((state, action, reward))    \n",
    "        rewards_list.append(reward_per_episode)        \n",
    "        states_actions_returns = calculate_returns(states_actions_rewards)        \n",
    "        \n",
    "        # calculate Q(s,a)\n",
    "        biggest_change = 0\n",
    "        seen_state_action_pairs = set()\n",
    "        for s, a, r in states_actions_returns:\n",
    "            # check if we have already seen s\n",
    "            # first-visit Monte Carlo optimization\n",
    "            sa = (s, a)\n",
    "            if sa not in seen_state_action_pairs:\n",
    "                R[sa].append(r)\n",
    "                old_q = Q[s][a]\n",
    "                # the new Q[s][a] is the sample mean of all our returns for that (state, action)\n",
    "                Q[s][a] = np.mean(R[sa])\n",
    "                biggest_change = max(biggest_change, np.abs(old_q - Q[s][a]))\n",
    "                seen_state_action_pairs.add(sa)\n",
    "        deltas.append(biggest_change)\n",
    "        \n",
    "        # calculate new policy pi(s) = argmax[a]{ Q(s,a) }\n",
    "        for s in P.keys():\n",
    "            a, _ = max_dict(Q[s])\n",
    "            P[s] = a\n",
    "            \n",
    "        avg_reward = np.mean(rewards_list[-500:])     \n",
    "        if i_episode % 5000 == 0:            \n",
    "            avg_delta = np.mean(deltas[-500:])  \n",
    "            print (\"Episode: {}   Avg reward: {:3.3f}  Avg Delta: {:8.8f}  Epsilon: {:3.3f}\".format(\n",
    "                i_episode, avg_reward, avg_delta, eps))\n",
    "            eps = max(eps - 0.005, 0.0)\n",
    "            \n",
    "        if len(rewards_list) > 10000:\n",
    "            rewards_list.pop(0)\n",
    "            \n",
    "        if len(deltas) > 10000:\n",
    "            deltas.pop(0)\n",
    "          \n",
    "        if avg_reward >= env.spec.reward_threshold: \n",
    "            print(\"########## Solved! ###########\")\n",
    "            break\n",
    "            \n",
    "  \n",
    "    # calculate values for each state (just to print and compare)\n",
    "    # V(s) = max[a]{ Q(s,a) }\n",
    "    V = {}\n",
    "    for s in P.keys():\n",
    "        V[s] = max_dict(Q[s])[1]\n",
    "        \n",
    "    print(\"\\nV values: {}\".format(V))    \n",
    "   \n",
    "    return P\n",
    "            \n",
    "print(\"action_space={}\".format(env.action_space))\n",
    "print(\"obs_space={}\".format(env.observation_space))\n",
    "print(\"threshold={} \\n\".format(env.spec.reward_threshold))\n",
    "policy = main()\n",
    "\n",
    "print(\"Policy values: {}\".format(policy)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(policy):\n",
    "    print (policy)\n",
    "    state = env.reset()        \n",
    "    action = epsilon_action(policy[state], 0)\n",
    "    for t in range(10000):                        \n",
    "        state, reward, done, _ = env.step(action) \n",
    "        env.render()                           \n",
    "        if done:\n",
    "            print(\"Reward: {}\".format(reward))\n",
    "            break\n",
    "\n",
    "test(policy)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
