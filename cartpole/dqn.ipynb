{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from gym import wrappers\n",
    "from PIL import Image\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "\n",
    "from DQN.dqn import DQN\n",
    "from DQN.utils import ReplayBuffer, mkdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'Acrobot-v1'\n",
    "lr_base = 0.001\n",
    "lr_decay = 0.0001\n",
    "epsilon_base = 0.5 \n",
    "epsilon_decay = 0.002\n",
    "\n",
    "random_seed = 42\n",
    "gamma = 0.99                # discount for future rewards\n",
    "batch_size = 256         # num of transitions sampled from replay buffer\n",
    "polyak = 0.999               # target policy update parameter (1-tau)\n",
    "max_episodes = 100000         # max num of episodes\n",
    "max_timesteps = 3000        # max timesteps in one episode\n",
    "max_buffer_length = 5000000\n",
    "log_interval = 10           # print avg reward after interval\n",
    "threshold = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''conv_config = [\n",
    "        {'dim': [None, 64], 'dropout': False, 'activation': 'relu'},\n",
    "        {'dim': [64, 64], 'dropout': False, 'activation': 'relu'},\n",
    "        {'dim': [64, None], 'dropout': False, 'activation': 'sigmoid'}\n",
    "] '''\n",
    "    \n",
    "fc_config = [\n",
    "        {'dim': [None, 64], 'dropout': False, 'activation': 'relu'},\n",
    "        {'dim': [64, None], 'dropout': False, 'activation': False}\n",
    "    ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNTrainer():\n",
    "    \n",
    "    def __init__(self, env_name, fc_config, random_seed=42, lr_base=0.001, lr_decay=0.00005, \n",
    "                 epsilon_base=0.3, epsilon_decay=0.0001, gamma=0.99, batch_size=1024, \n",
    "                 max_episodes=100000, max_timesteps=3000, max_buffer_length=5000000, \n",
    "                 log_interval=5, threshold=None, lr_minimum=1e-10, epsilon_minimum=1e-10,\n",
    "                 record_videos=True, record_interval=100):        \n",
    "        \n",
    "        self.algorithm_name = 'dqn'\n",
    "        self.env_name = env_name\n",
    "        self.env = gym.make(env_name)\n",
    "        self.record_videos = record_videos\n",
    "        self.record_interval = record_interval        \n",
    "        if self.record_videos == True:\n",
    "            videos_dir = mkdir('.', 'videos')\n",
    "            monitor_dir = mkdir(videos_dir, self.algorithm_name)\n",
    "            should_record = lambda i: self.should_record\n",
    "            self.env = wrappers.Monitor(self.env, monitor_dir, video_callable=should_record, force=True)            \n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.n    \n",
    "        self.should_record = False\n",
    "        if not threshold == None:\n",
    "            self.threshold = threshold\n",
    "        else:    \n",
    "            self.threshold = self.env.spec.reward_threshold\n",
    "              \n",
    "        self.fc_config = fc_config\n",
    "        self.fc_config[0]['dim'][0] = self.state_dim\n",
    "        self.fc_config[-1]['dim'][1] = self.action_dim      \n",
    "        \n",
    "        self.random_seed = random_seed\n",
    "        self.lr_base = lr_base\n",
    "        self.lr_decay = lr_decay\n",
    "        self.lr_minimum = lr_minimum  \n",
    "        self.epsilon_base = epsilon_base\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_minimum = epsilon_minimum\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size              \n",
    "        self.max_episodes = max_episodes\n",
    "        self.max_timesteps = max_timesteps\n",
    "        self.max_buffer_length = max_buffer_length\n",
    "        self.log_interval = log_interval\n",
    "        \n",
    "        prdir = mkdir('.', 'preTrained')\n",
    "        self.directory = mkdir(prdir, self.algorithm_name)\n",
    "        self.filename = \"{}_{}_{}\".format(self.algorithm_name, self.env_name, self.random_seed)\n",
    "        \n",
    "        self.policy = DQN(self.env, fc_config)   \n",
    "        self.replay_buffer = ReplayBuffer(size=self.max_buffer_length)\n",
    "        \n",
    "        self.reward_history = []\n",
    "        self.make_plots = False       \n",
    "        \n",
    "        if self.random_seed:\n",
    "            print(\"Random Seed: {}\".format(self.random_seed))\n",
    "            self.env.seed(self.random_seed)\n",
    "            torch.manual_seed(self.random_seed)\n",
    "            np.random.seed(self.random_seed)\n",
    "            \n",
    "    def train(self):\n",
    "        \n",
    "        start_time = time.time()        \n",
    "        print(\"action_space={}\".format(self.env.action_space))\n",
    "        print(\"obs_space={}\".format(self.env.observation_space))\n",
    "        print(\"threshold={} \\n\".format(self.threshold))        \n",
    "\n",
    "        # loading models\n",
    "        self.policy.load(self.directory, self.filename)\n",
    "                \n",
    "        # logging variables:        \n",
    "        log_f = open(\"train_{}.txt\".format(self.algorithm_name), \"w+\")\n",
    "        \n",
    "        print(\"Training started ... \")\n",
    "                \n",
    "        # training procedure:        \n",
    "        for episode in range(self.max_episodes):\n",
    "            \n",
    "            # Only record video during evaluation, every n steps\n",
    "            if episode % self.record_interval == 0:\n",
    "                self.should_record = True\n",
    "            \n",
    "            ep_reward = 0.0        \n",
    "            state = self.env.reset()\n",
    "                       \n",
    "            # calculate params\n",
    "            epsilon = max(self.epsilon_base / (1.0 + episode * self.epsilon_decay), self.epsilon_minimum)\n",
    "            learning_rate = max(self.lr_base / (1.0 + episode * self.lr_decay), self.lr_minimum)      \n",
    "            self.policy.set_optimizers(lr=learning_rate)\n",
    "           \n",
    "            for t in range(self.max_timesteps):\n",
    "                \n",
    "                action = self.policy.select_action(state, epsilon)                \n",
    "                next_state, reward, done, _ = self.env.step(action)          \n",
    "                self.replay_buffer.add(state, action, reward, next_state, float(done))\n",
    "                \n",
    "                # Updating policy\n",
    "                self.policy.update(self.replay_buffer, t, self.batch_size, self.gamma)\n",
    "                \n",
    "                state = next_state               \n",
    "                ep_reward += reward            \n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "           \n",
    "            self.reward_history.append(ep_reward)\n",
    "            avg_reward = np.mean(self.reward_history[-100:]) \n",
    "                       \n",
    "            # logging updates:        \n",
    "            log_f.write('{},{}\\n'.format(episode, ep_reward))\n",
    "            log_f.flush()\n",
    "           \n",
    "            if len(self.policy.Q_loss_list) > 0:               \n",
    "                avg_Q_loss = np.mean(self.policy.Q_loss_list[-100:])     \n",
    "            \n",
    "            # Truncate training history if we don't plan to plot it later\n",
    "            if not self.make_plots:\n",
    "                self.policy.truncate_loss_lists() \n",
    "                if len(self.reward_history) > 100:\n",
    "                    self.reward_history.pop(0)    \n",
    "         \n",
    "            # Print avg reward every log interval:\n",
    "            if episode % self.log_interval == 0:            \n",
    "                self.policy.save(self.directory, self.filename)\n",
    "                print(\"Ep:{:5d}  Rew:{:8.2f}  Avg Rew:{:8.2f}  LR:{:8.8f}  Bf:{:2.0f}  EPS:{:0.4f}  Loss: {:5.3f}\".format(\n",
    "                    episode, ep_reward, avg_reward, learning_rate, self.replay_buffer.get_fill(), \n",
    "                    epsilon, avg_Q_loss))\n",
    "                        \n",
    "            self.should_record = False\n",
    "            \n",
    "            # if avg reward > threshold then save and stop traning:\n",
    "            if avg_reward >= self.threshold and episode > 100: \n",
    "                print(\"Ep:{:5d}  Rew:{:8.2f}  Avg Rew:{:8.2f}  LR:{:8.8f}  Bf:{:2.0f}  EPS:{:0.4f}  Loss: {:5.3f}\".format(\n",
    "                    episode, ep_reward, avg_reward, learning_rate, self.replay_buffer.get_fill(), \n",
    "                    epsilon, avg_Q_loss))\n",
    "                print(\"########## Solved! ###########\")\n",
    "                name = self.filename + '_solved'\n",
    "                self.policy.save(self.directory, name)\n",
    "                log_f.close()\n",
    "                self.env.close()  \n",
    "                training_time = time.time() - start_time\n",
    "                print(\"Training time: {:6.2f} sec\".format(training_time))\n",
    "                break    \n",
    "                                \n",
    "    def test(self, episodes=3, render=True, save_gif=True):              \n",
    "\n",
    "        gifdir = mkdir('.','gif')\n",
    "        algdir = mkdir(gifdir, self.algorithm_name)\n",
    "        \n",
    "        for episode in range(1, episodes+1):\n",
    "            ep_reward = 0.0\n",
    "            state = self.env.reset()    \n",
    "            epdir = mkdir(algdir, str(episode))\n",
    "                       \n",
    "            for t in range(self.max_timesteps):\n",
    "                action = self.policy.select_action(state, 0)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                self.replay_buffer.add((state, action, reward, next_state, float(done)))\n",
    "                state = next_state               \n",
    "                ep_reward += reward                                  \n",
    "                \n",
    "                if save_gif:                                       \n",
    "                    img = self.env.render(mode = 'rgb_array')\n",
    "                    img = Image.fromarray(img)\n",
    "                    img.save('{}/{}.jpg'.format(epdir, t))\n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "            print('Test episode: {}\\tReward: {:4.2f}'.format(episode, ep_reward))           \n",
    "            self.env.close()        \n",
    "                   \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NETWORK=Sequential(\n",
      "  (0): Linear(in_features=6, out_features=64, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=64, out_features=3, bias=True)\n",
      ")\n",
      "NETWORK=Sequential(\n",
      "  (0): Linear(in_features=6, out_features=64, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=64, out_features=3, bias=True)\n",
      ")\n",
      "Random Seed: 42\n",
      "Training started ... \n",
      "\n",
      "action_space=Discrete(3)\n",
      "obs_space=Box(6,)\n",
      "threshold=-100\n",
      "DIR=./preTrained/dqn NAME=dqn_Acrobot-v1_42\n",
      "Models loaded\n",
      "Ep:    0  Rew: -376.00  Avg Rew: -376.00  LR:0.00100000  Bf: 0  EPS:0.5000  Loss: 0.003\n",
      "Ep:   10  Rew: -261.00  Avg Rew: -451.64  LR:0.00099900  Bf: 0  EPS:0.4902  Loss: 0.207\n",
      "Ep:   20  Rew: -500.00  Avg Rew: -431.81  LR:0.00099800  Bf: 0  EPS:0.4808  Loss: 0.567\n",
      "Ep:   30  Rew: -385.00  Avg Rew: -402.39  LR:0.00099701  Bf: 0  EPS:0.4717  Loss: 0.928\n",
      "Ep:   40  Rew: -235.00  Avg Rew: -372.00  LR:0.00099602  Bf: 0  EPS:0.4630  Loss: 1.224\n",
      "Ep:   50  Rew: -148.00  Avg Rew: -334.90  LR:0.00099502  Bf: 0  EPS:0.4545  Loss: 1.856\n",
      "Ep:   60  Rew: -138.00  Avg Rew: -304.97  LR:0.00099404  Bf: 0  EPS:0.4464  Loss: 2.072\n",
      "Ep:   70  Rew: -110.00  Avg Rew: -284.45  LR:0.00099305  Bf: 0  EPS:0.4386  Loss: 2.663\n",
      "Ep:   80  Rew: -169.00  Avg Rew: -269.26  LR:0.00099206  Bf: 0  EPS:0.4310  Loss: 2.705\n",
      "Ep:   90  Rew: -168.00  Avg Rew: -255.43  LR:0.00099108  Bf: 0  EPS:0.4237  Loss: 2.756\n",
      "Ep:  100  Rew: -118.00  Avg Rew: -242.06  LR:0.00099010  Bf: 0  EPS:0.4167  Loss: 2.973\n",
      "Ep:  110  Rew: -114.00  Avg Rew: -209.57  LR:0.00098912  Bf: 1  EPS:0.4098  Loss: 2.566\n",
      "Ep:  120  Rew: -150.00  Avg Rew: -183.04  LR:0.00098814  Bf: 1  EPS:0.4032  Loss: 2.803\n",
      "Ep:  130  Rew: -153.00  Avg Rew: -163.54  LR:0.00098717  Bf: 1  EPS:0.3968  Loss: 2.832\n",
      "Ep:  140  Rew: -121.00  Avg Rew: -149.41  LR:0.00098619  Bf: 1  EPS:0.3906  Loss: 2.474\n",
      "Ep:  150  Rew: -101.00  Avg Rew: -142.68  LR:0.00098522  Bf: 1  EPS:0.3846  Loss: 2.152\n",
      "Ep:  160  Rew: -113.00  Avg Rew: -139.27  LR:0.00098425  Bf: 1  EPS:0.3788  Loss: 2.190\n",
      "Ep:  170  Rew: -126.00  Avg Rew: -136.29  LR:0.00098328  Bf: 1  EPS:0.3731  Loss: 2.076\n",
      "Ep:  180  Rew: -102.00  Avg Rew: -132.43  LR:0.00098232  Bf: 1  EPS:0.3676  Loss: 2.072\n",
      "Ep:  190  Rew: -137.00  Avg Rew: -131.25  LR:0.00098135  Bf: 1  EPS:0.3623  Loss: 2.210\n",
      "Ep:  200  Rew: -133.00  Avg Rew: -133.58  LR:0.00098039  Bf: 1  EPS:0.3571  Loss: 1.793\n",
      "Ep:  210  Rew: -124.00  Avg Rew: -132.73  LR:0.00097943  Bf: 1  EPS:0.3521  Loss: 2.106\n",
      "Ep:  220  Rew: -141.00  Avg Rew: -130.33  LR:0.00097847  Bf: 1  EPS:0.3472  Loss: 1.850\n",
      "Ep:  230  Rew: -139.00  Avg Rew: -127.15  LR:0.00097752  Bf: 1  EPS:0.3425  Loss: 1.975\n",
      "Ep:  240  Rew: -117.00  Avg Rew: -125.51  LR:0.00097656  Bf: 1  EPS:0.3378  Loss: 1.936\n",
      "Ep:  250  Rew: -109.00  Avg Rew: -125.95  LR:0.00097561  Bf: 1  EPS:0.3333  Loss: 2.484\n",
      "Ep:  260  Rew: -351.00  Avg Rew: -129.01  LR:0.00097466  Bf: 1  EPS:0.3289  Loss: 1.781\n",
      "Ep:  270  Rew: -118.00  Avg Rew: -127.63  LR:0.00097371  Bf: 1  EPS:0.3247  Loss: 1.985\n",
      "Ep:  280  Rew: -117.00  Avg Rew: -130.01  LR:0.00097276  Bf: 1  EPS:0.3205  Loss: 2.151\n",
      "Ep:  290  Rew: -135.00  Avg Rew: -130.97  LR:0.00097182  Bf: 1  EPS:0.3165  Loss: 2.230\n",
      "Ep:  300  Rew:  -93.00  Avg Rew: -126.73  LR:0.00097087  Bf: 1  EPS:0.3125  Loss: 2.183\n",
      "Ep:  310  Rew: -118.00  Avg Rew: -125.86  LR:0.00096993  Bf: 1  EPS:0.3086  Loss: 2.282\n",
      "Ep:  320  Rew: -141.00  Avg Rew: -124.75  LR:0.00096899  Bf: 1  EPS:0.3049  Loss: 2.389\n",
      "Ep:  330  Rew: -117.00  Avg Rew: -125.48  LR:0.00096805  Bf: 1  EPS:0.3012  Loss: 2.273\n",
      "Ep:  340  Rew:  -82.00  Avg Rew: -124.79  LR:0.00096712  Bf: 1  EPS:0.2976  Loss: 2.126\n",
      "Ep:  350  Rew: -126.00  Avg Rew: -125.99  LR:0.00096618  Bf: 1  EPS:0.2941  Loss: 2.100\n",
      "Ep:  360  Rew: -135.00  Avg Rew: -122.56  LR:0.00096525  Bf: 1  EPS:0.2907  Loss: 2.349\n",
      "Ep:  370  Rew: -115.00  Avg Rew: -122.52  LR:0.00096432  Bf: 1  EPS:0.2874  Loss: 2.027\n",
      "Ep:  380  Rew:  -98.00  Avg Rew: -118.95  LR:0.00096339  Bf: 1  EPS:0.2841  Loss: 2.005\n",
      "Ep:  390  Rew:  -93.00  Avg Rew: -115.46  LR:0.00096246  Bf: 1  EPS:0.2809  Loss: 2.025\n",
      "Ep:  400  Rew: -110.00  Avg Rew: -115.19  LR:0.00096154  Bf: 1  EPS:0.2778  Loss: 2.338\n",
      "Ep:  410  Rew: -107.00  Avg Rew: -121.20  LR:0.00096061  Bf: 1  EPS:0.2747  Loss: 2.143\n",
      "Ep:  420  Rew: -114.00  Avg Rew: -122.15  LR:0.00095969  Bf: 1  EPS:0.2717  Loss: 2.649\n"
     ]
    }
   ],
   "source": [
    "agent = DQNTrainer(env_name, fc_config, random_seed=random_seed, lr_base=lr_base, lr_decay=lr_decay, \n",
    "                   epsilon_base=epsilon_base, epsilon_decay=epsilon_decay, gamma=gamma, batch_size=batch_size,\n",
    "                   max_episodes=max_episodes, max_timesteps=max_timesteps, \n",
    "                   max_buffer_length=max_buffer_length, log_interval=log_interval, threshold=threshold)\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "agent.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
