{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "action_space=Discrete(2)\n",
      "obs_space=Box(4,)\n",
      "threshold=475.0 \n",
      "\n",
      "Episode 0\tLast length:    10\t Reward:   11.00\t Avg Reward:    0.06\n",
      "Episode 200\tLast length:   105\t Reward:  106.00\t Avg Reward:   20.95\n",
      "Episode 400\tLast length:    10\t Reward:   11.00\t Avg Reward:   18.91\n",
      "Episode 600\tLast length:    22\t Reward:   23.00\t Avg Reward:   18.83\n",
      "Episode 800\tLast length:    38\t Reward:   39.00\t Avg Reward:   18.82\n",
      "Episode 1000\tLast length:    20\t Reward:   21.00\t Avg Reward:   17.79\n",
      "Episode 1200\tLast length:    10\t Reward:   11.00\t Avg Reward:   19.32\n",
      "Episode 1400\tLast length:    12\t Reward:   13.00\t Avg Reward:   17.75\n",
      "Episode 1600\tLast length:    15\t Reward:   16.00\t Avg Reward:   20.15\n",
      "Episode 1800\tLast length:    19\t Reward:   20.00\t Avg Reward:   18.80\n",
      "Episode 2000\tLast length:     7\t Reward:    8.00\t Avg Reward:   18.57\n",
      "Episode 2200\tLast length:    34\t Reward:   35.00\t Avg Reward:   18.61\n",
      "Episode 2400\tLast length:    15\t Reward:   16.00\t Avg Reward:   18.91\n",
      "Episode 2600\tLast length:    10\t Reward:   11.00\t Avg Reward:   18.67\n",
      "Episode 2800\tLast length:    27\t Reward:   28.00\t Avg Reward:   18.65\n",
      "Episode 3000\tLast length:    23\t Reward:   24.00\t Avg Reward:   18.74\n",
      "Episode 3200\tLast length:    13\t Reward:   14.00\t Avg Reward:   19.92\n",
      "Episode 3400\tLast length:    18\t Reward:   19.00\t Avg Reward:   17.63\n",
      "Episode 3600\tLast length:    11\t Reward:   12.00\t Avg Reward:   18.28\n",
      "Episode 3800\tLast length:    11\t Reward:   12.00\t Avg Reward:   18.26\n",
      "Episode 4000\tLast length:    50\t Reward:   51.00\t Avg Reward:   18.89\n",
      "Episode 4200\tLast length:    12\t Reward:   13.00\t Avg Reward:   17.73\n",
      "Episode 4400\tLast length:     8\t Reward:    9.00\t Avg Reward:   19.50\n",
      "Episode 4600\tLast length:    20\t Reward:   21.00\t Avg Reward:   18.51\n",
      "Episode 4800\tLast length:    18\t Reward:   19.00\t Avg Reward:   18.92\n",
      "Episode 5000\tLast length:     9\t Reward:   10.00\t Avg Reward:   18.43\n",
      "Episode 5200\tLast length:    15\t Reward:   16.00\t Avg Reward:   19.43\n",
      "Episode 5400\tLast length:    12\t Reward:   13.00\t Avg Reward:   18.27\n",
      "Episode 5600\tLast length:     9\t Reward:   10.00\t Avg Reward:   18.71\n",
      "Episode 5800\tLast length:    24\t Reward:   25.00\t Avg Reward:   18.41\n",
      "Episode 6000\tLast length:    15\t Reward:   16.00\t Avg Reward:   18.82\n",
      "Episode 6200\tLast length:    10\t Reward:   11.00\t Avg Reward:   20.20\n",
      "Episode 6400\tLast length:    22\t Reward:   23.00\t Avg Reward:   17.87\n",
      "Episode 6600\tLast length:    15\t Reward:   16.00\t Avg Reward:   19.64\n",
      "Episode 6800\tLast length:    32\t Reward:   33.00\t Avg Reward:   20.14\n",
      "Episode 7000\tLast length:    15\t Reward:   16.00\t Avg Reward:   17.87\n",
      "Episode 7200\tLast length:    14\t Reward:   15.00\t Avg Reward:   18.74\n",
      "Episode 7400\tLast length:    13\t Reward:   14.00\t Avg Reward:   18.55\n",
      "Episode 7600\tLast length:     9\t Reward:   10.00\t Avg Reward:   19.62\n",
      "Episode 7800\tLast length:    32\t Reward:   33.00\t Avg Reward:   19.09\n",
      "Episode 8000\tLast length:    19\t Reward:   20.00\t Avg Reward:   18.34\n",
      "Episode 8200\tLast length:    12\t Reward:   13.00\t Avg Reward:   17.26\n",
      "Episode 8400\tLast length:     8\t Reward:    9.00\t Avg Reward:   19.72\n",
      "Episode 8600\tLast length:    12\t Reward:   13.00\t Avg Reward:   19.99\n",
      "Episode 8800\tLast length:    35\t Reward:   36.00\t Avg Reward:   19.11\n",
      "Episode 9000\tLast length:    21\t Reward:   22.00\t Avg Reward:   18.34\n",
      "Episode 9200\tLast length:    37\t Reward:   38.00\t Avg Reward:   17.82\n",
      "Episode 9400\tLast length:    13\t Reward:   14.00\t Avg Reward:   19.93\n",
      "Episode 9600\tLast length:    13\t Reward:   14.00\t Avg Reward:   17.32\n",
      "Episode 9800\tLast length:    22\t Reward:   23.00\t Avg Reward:   18.75\n",
      "Episode 10000\tLast length:    12\t Reward:   13.00\t Avg Reward:   18.34\n",
      "Solved! Running reward is now 500.0 and the last episode runs to 499 time steps!\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "\n",
    "from DDPG.ddpg import DDPG\n",
    "\n",
    "args = {\n",
    "    'gamma': 0.99,\n",
    "    'render': True,\n",
    "    'log_interval': 200\n",
    "}\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "episodes = 100000\n",
    "print(\"action_space={}\".format(env.action_space))\n",
    "print(\"obs_space={}\".format(env.observation_space))\n",
    "\n",
    "\n",
    "def main():   \n",
    "    task = {\n",
    "        'state_size': 4,\n",
    "        'action_size': 1,\n",
    "        'action_high': 1,\n",
    "        'action_low': 0\n",
    "    }\n",
    "    agent = DDPG(task)\n",
    "    sum_reward = 0\n",
    "    for i_episode in range(episodes):\n",
    "        running_reward = 0        \n",
    "        state = env.reset()\n",
    "        for t in range(10000):  # Don't infinite loop while learning\n",
    "            action = agent.act(state, i_episode)            \n",
    "            state, reward, done, _ = env.step(action)            \n",
    "            if args['render']:\n",
    "                env.render()                   \n",
    "            running_reward += reward            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        sum_reward += running_reward\n",
    "        \n",
    "        if i_episode % args['log_interval'] == 0:\n",
    "            avg_reward = sum_reward / args['log_interval']\n",
    "            sum_reward = 0\n",
    "            print('Episode {}\\tLast length: {:5d}\\t Reward: {:7.2f}\\t Avg Reward: {:7.2f}'.format(\n",
    "                i_episode, t, running_reward, avg_reward))\n",
    "        if running_reward > env.spec.reward_threshold:\n",
    "            print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "            break\n",
    "\n",
    "print(\"threshold={} \\n\".format(env.spec.reward_threshold))\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
