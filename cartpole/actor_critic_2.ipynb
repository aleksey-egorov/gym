{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "action_space=Discrete(2)\n",
      "obs_space=Box(4,)\n",
      "threshold=475.0 \n",
      "\n",
      "Episode 0   Last length:    11   Reward:   12.00   Avg Reward:   12.00   Noise: 1.00\n",
      "Episode 50   Last length:    19   Reward:   20.00   Avg Reward:   16.18   Noise: 0.99\n",
      "Episode 100   Last length:    10   Reward:   11.00   Avg Reward:   15.63   Noise: 0.99\n",
      "Episode 150   Last length:    20   Reward:   21.00   Avg Reward:   13.72   Noise: 0.98\n",
      "Episode 200   Last length:     8   Reward:    9.00   Avg Reward:   11.57   Noise: 0.98\n",
      "Episode 250   Last length:     9   Reward:   10.00   Avg Reward:   10.18   Noise: 0.97\n",
      "Episode 300   Last length:    14   Reward:   15.00   Avg Reward:   10.07   Noise: 0.97\n",
      "Episode 350   Last length:     8   Reward:    9.00   Avg Reward:   12.38   Noise: 0.96\n",
      "Episode 400   Last length:    34   Reward:   35.00   Avg Reward:   16.52   Noise: 0.96\n",
      "Episode 450   Last length:    48   Reward:   49.00   Avg Reward:   23.88   Noise: 0.95\n",
      "Episode 500   Last length:    73   Reward:   74.00   Avg Reward:   35.81   Noise: 0.95\n",
      "Episode 550   Last length:    33   Reward:   34.00   Avg Reward:   55.97   Noise: 0.94\n",
      "Episode 600   Last length:    58   Reward:   59.00   Avg Reward:   65.26   Noise: 0.94\n",
      "Episode 650   Last length:    11   Reward:   12.00   Avg Reward:   71.00   Noise: 0.94\n",
      "Episode 700   Last length:    71   Reward:   72.00   Avg Reward:   68.29   Noise: 0.93\n",
      "Episode 750   Last length:    12   Reward:   13.00   Avg Reward:   48.73   Noise: 0.93\n",
      "Episode 800   Last length:    17   Reward:   18.00   Avg Reward:   37.20   Noise: 0.92\n",
      "Episode 850   Last length:   138   Reward:  139.00   Avg Reward:   38.39   Noise: 0.92\n",
      "Episode 900   Last length:    72   Reward:   73.00   Avg Reward:   49.72   Noise: 0.91\n",
      "Episode 950   Last length:    14   Reward:   15.00   Avg Reward:   45.48   Noise: 0.91\n",
      "Episode 1000   Last length:    14   Reward:   15.00   Avg Reward:   25.51   Noise: 0.90\n",
      "Episode 1050   Last length:    15   Reward:   16.00   Avg Reward:   15.30   Noise: 0.90\n",
      "Episode 1100   Last length:    16   Reward:   17.00   Avg Reward:   15.71   Noise: 0.89\n",
      "Episode 1150   Last length:    13   Reward:   14.00   Avg Reward:   14.57   Noise: 0.89\n",
      "Episode 1200   Last length:     8   Reward:    9.00   Avg Reward:   12.97   Noise: 0.88\n",
      "Episode 1250   Last length:    13   Reward:   14.00   Avg Reward:   12.82   Noise: 0.88\n",
      "Episode 1300   Last length:    19   Reward:   20.00   Avg Reward:   15.29   Noise: 0.87\n",
      "Episode 1350   Last length:    16   Reward:   17.00   Avg Reward:   18.29   Noise: 0.86\n",
      "Episode 1400   Last length:     8   Reward:    9.00   Avg Reward:   15.88   Noise: 0.86\n",
      "Episode 1450   Last length:    13   Reward:   14.00   Avg Reward:   12.64   Noise: 0.85\n",
      "Episode 1500   Last length:    10   Reward:   11.00   Avg Reward:   11.68   Noise: 0.85\n",
      "Episode 1550   Last length:     8   Reward:    9.00   Avg Reward:   11.10   Noise: 0.84\n",
      "Episode 1600   Last length:     8   Reward:    9.00   Avg Reward:   11.15   Noise: 0.84\n",
      "Episode 1650   Last length:    10   Reward:   11.00   Avg Reward:   11.00   Noise: 0.83\n",
      "Episode 1700   Last length:     9   Reward:   10.00   Avg Reward:   11.10   Noise: 0.83\n",
      "Episode 1750   Last length:     9   Reward:   10.00   Avg Reward:   12.15   Noise: 0.82\n",
      "Episode 1800   Last length:    10   Reward:   11.00   Avg Reward:   12.41   Noise: 0.82\n",
      "Episode 1850   Last length:     8   Reward:    9.00   Avg Reward:   11.25   Noise: 0.81\n",
      "Episode 1900   Last length:     8   Reward:    9.00   Avg Reward:   10.26   Noise: 0.81\n",
      "Episode 1950   Last length:     9   Reward:   10.00   Avg Reward:    9.97   Noise: 0.81\n",
      "Episode 2000   Last length:     7   Reward:    8.00   Avg Reward:    9.97   Noise: 0.80\n",
      "Episode 2050   Last length:    10   Reward:   11.00   Avg Reward:   10.44   Noise: 0.80\n",
      "Episode 2100   Last length:     9   Reward:   10.00   Avg Reward:   10.63   Noise: 0.79\n",
      "Episode 2150   Last length:     8   Reward:    9.00   Avg Reward:   10.18   Noise: 0.79\n",
      "Episode 2200   Last length:    10   Reward:   11.00   Avg Reward:    9.86   Noise: 0.78\n",
      "Episode 2250   Last length:     9   Reward:   10.00   Avg Reward:    9.97   Noise: 0.78\n",
      "Episode 2300   Last length:    10   Reward:   11.00   Avg Reward:    9.91   Noise: 0.77\n",
      "Episode 2350   Last length:    11   Reward:   12.00   Avg Reward:   10.24   Noise: 0.77\n",
      "Episode 2400   Last length:    11   Reward:   12.00   Avg Reward:   10.98   Noise: 0.76\n",
      "Episode 2450   Last length:    11   Reward:   12.00   Avg Reward:   11.07   Noise: 0.76\n",
      "Episode 2500   Last length:     8   Reward:    9.00   Avg Reward:   10.83   Noise: 0.75\n",
      "Episode 2550   Last length:     8   Reward:    9.00   Avg Reward:   10.37   Noise: 0.74\n",
      "Episode 2600   Last length:     9   Reward:   10.00   Avg Reward:    9.84   Noise: 0.74\n",
      "Episode 2650   Last length:     9   Reward:   10.00   Avg Reward:    9.77   Noise: 0.73\n",
      "Episode 2700   Last length:    10   Reward:   11.00   Avg Reward:    9.62   Noise: 0.73\n",
      "Episode 2750   Last length:     7   Reward:    8.00   Avg Reward:    9.55   Noise: 0.72\n",
      "Episode 2800   Last length:    11   Reward:   12.00   Avg Reward:    9.88   Noise: 0.72\n",
      "Episode 2850   Last length:     8   Reward:    9.00   Avg Reward:   10.60   Noise: 0.71\n",
      "Episode 2900   Last length:    11   Reward:   12.00   Avg Reward:   11.69   Noise: 0.71\n",
      "Episode 2950   Last length:    12   Reward:   13.00   Avg Reward:   12.31   Noise: 0.70\n",
      "Episode 3000   Last length:    11   Reward:   12.00   Avg Reward:   11.80   Noise: 0.70\n",
      "Episode 3050   Last length:     8   Reward:    9.00   Avg Reward:   10.74   Noise: 0.69\n",
      "Episode 3100   Last length:     8   Reward:    9.00   Avg Reward:   10.22   Noise: 0.69\n",
      "Episode 3150   Last length:     9   Reward:   10.00   Avg Reward:    9.80   Noise: 0.69\n",
      "Episode 3200   Last length:    10   Reward:   11.00   Avg Reward:    9.75   Noise: 0.68\n",
      "Episode 3250   Last length:     8   Reward:    9.00   Avg Reward:    9.87   Noise: 0.68\n",
      "Episode 3300   Last length:     9   Reward:   10.00   Avg Reward:    9.81   Noise: 0.67\n",
      "Episode 3350   Last length:     7   Reward:    8.00   Avg Reward:    9.72   Noise: 0.67\n",
      "Episode 3400   Last length:     8   Reward:    9.00   Avg Reward:    9.67   Noise: 0.66\n",
      "Episode 3450   Last length:     9   Reward:   10.00   Avg Reward:   10.11   Noise: 0.66\n",
      "Episode 3500   Last length:    10   Reward:   11.00   Avg Reward:   10.83   Noise: 0.65\n",
      "Episode 3550   Last length:    17   Reward:   18.00   Avg Reward:   12.02   Noise: 0.65\n",
      "Episode 3600   Last length:    11   Reward:   12.00   Avg Reward:   14.50   Noise: 0.64\n",
      "Episode 3650   Last length:    17   Reward:   18.00   Avg Reward:   15.22   Noise: 0.64\n",
      "Episode 3700   Last length:    12   Reward:   13.00   Avg Reward:   14.15   Noise: 0.63\n",
      "Episode 3750   Last length:    15   Reward:   16.00   Avg Reward:   14.35   Noise: 0.62\n",
      "Episode 3800   Last length:    10   Reward:   11.00   Avg Reward:   14.41   Noise: 0.62\n",
      "Episode 3850   Last length:     9   Reward:   10.00   Avg Reward:   14.09   Noise: 0.61\n",
      "Episode 3900   Last length:    14   Reward:   15.00   Avg Reward:   13.54   Noise: 0.61\n",
      "Episode 3950   Last length:    11   Reward:   12.00   Avg Reward:   12.70   Noise: 0.60\n",
      "Episode 4000   Last length:    12   Reward:   13.00   Avg Reward:   12.04   Noise: 0.60\n",
      "Episode 4050   Last length:    10   Reward:   11.00   Avg Reward:   11.68   Noise: 0.59\n",
      "Episode 4100   Last length:    15   Reward:   16.00   Avg Reward:   11.54   Noise: 0.59\n",
      "Episode 4150   Last length:     9   Reward:   10.00   Avg Reward:   11.03   Noise: 0.58\n",
      "Episode 4200   Last length:    11   Reward:   12.00   Avg Reward:   10.69   Noise: 0.58\n",
      "Episode 4250   Last length:     9   Reward:   10.00   Avg Reward:   10.76   Noise: 0.57\n",
      "Episode 4300   Last length:    18   Reward:   19.00   Avg Reward:   11.73   Noise: 0.57\n",
      "Episode 4350   Last length:    16   Reward:   17.00   Avg Reward:   17.44   Noise: 0.56\n",
      "Episode 4400   Last length:    21   Reward:   22.00   Avg Reward:   18.30   Noise: 0.56\n",
      "Episode 4450   Last length:    20   Reward:   21.00   Avg Reward:   14.53   Noise: 0.56\n",
      "Episode 4500   Last length:    22   Reward:   23.00   Avg Reward:   16.11   Noise: 0.55\n",
      "Episode 4550   Last length:    19   Reward:   20.00   Avg Reward:   19.10   Noise: 0.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4600   Last length:    31   Reward:   32.00   Avg Reward:   21.43   Noise: 0.54\n",
      "Episode 4650   Last length:    26   Reward:   27.00   Avg Reward:   20.20   Noise: 0.54\n",
      "Episode 4700   Last length:    10   Reward:   11.00   Avg Reward:   17.54   Noise: 0.53\n",
      "Episode 4750   Last length:    10   Reward:   11.00   Avg Reward:   16.27   Noise: 0.53\n",
      "Episode 4800   Last length:    23   Reward:   24.00   Avg Reward:   15.51   Noise: 0.52\n",
      "Episode 4850   Last length:    21   Reward:   22.00   Avg Reward:   15.65   Noise: 0.52\n",
      "Episode 4900   Last length:    10   Reward:   11.00   Avg Reward:   15.39   Noise: 0.51\n",
      "Episode 4950   Last length:    20   Reward:   21.00   Avg Reward:   14.99   Noise: 0.51\n",
      "Episode 5000   Last length:    17   Reward:   18.00   Avg Reward:   15.51   Noise: 0.50\n",
      "Episode 5050   Last length:    22   Reward:   23.00   Avg Reward:   18.17   Noise: 0.49\n",
      "Episode 5100   Last length:    19   Reward:   20.00   Avg Reward:   28.25   Noise: 0.49\n",
      "Episode 5150   Last length:    59   Reward:   60.00   Avg Reward:   33.98   Noise: 0.48\n",
      "Episode 5200   Last length:    26   Reward:   27.00   Avg Reward:   29.58   Noise: 0.48\n",
      "Episode 5250   Last length:    16   Reward:   17.00   Avg Reward:   22.87   Noise: 0.47\n",
      "Episode 5300   Last length:    14   Reward:   15.00   Avg Reward:   17.48   Noise: 0.47\n",
      "Episode 5350   Last length:    22   Reward:   23.00   Avg Reward:   19.12   Noise: 0.47\n",
      "Episode 5400   Last length:    31   Reward:   32.00   Avg Reward:   24.42   Noise: 0.46\n",
      "Episode 5450   Last length:    46   Reward:   47.00   Avg Reward:   30.97   Noise: 0.46\n",
      "Episode 5500   Last length:    26   Reward:   27.00   Avg Reward:   36.03   Noise: 0.45\n",
      "Episode 5550   Last length:    38   Reward:   39.00   Avg Reward:   37.39   Noise: 0.45\n",
      "Episode 5600   Last length:    36   Reward:   37.00   Avg Reward:   38.27   Noise: 0.44\n",
      "Episode 5650   Last length:    35   Reward:   36.00   Avg Reward:   48.68   Noise: 0.43\n",
      "Episode 5700   Last length:    17   Reward:   18.00   Avg Reward:   45.60   Noise: 0.43\n",
      "Episode 5750   Last length:    15   Reward:   16.00   Avg Reward:   26.54   Noise: 0.42\n",
      "Episode 5800   Last length:    14   Reward:   15.00   Avg Reward:   17.47   Noise: 0.42\n",
      "Episode 5850   Last length:    10   Reward:   11.00   Avg Reward:   13.43   Noise: 0.41\n",
      "Episode 5900   Last length:    10   Reward:   11.00   Avg Reward:   11.53   Noise: 0.41\n",
      "Episode 5950   Last length:     9   Reward:   10.00   Avg Reward:   10.63   Noise: 0.41\n",
      "Episode 6000   Last length:     9   Reward:   10.00   Avg Reward:    9.89   Noise: 0.40\n",
      "Episode 6050   Last length:     8   Reward:    9.00   Avg Reward:    9.63   Noise: 0.40\n",
      "Episode 6100   Last length:    22   Reward:   23.00   Avg Reward:   12.27   Noise: 0.39\n",
      "Episode 6150   Last length:    25   Reward:   26.00   Avg Reward:   20.51   Noise: 0.39\n",
      "Episode 6200   Last length:    28   Reward:   29.00   Avg Reward:   29.63   Noise: 0.38\n",
      "Episode 6250   Last length:    19   Reward:   20.00   Avg Reward:   34.93   Noise: 0.38\n",
      "Episode 6300   Last length:    44   Reward:   45.00   Avg Reward:   39.83   Noise: 0.37\n",
      "Episode 6350   Last length:     9   Reward:   10.00   Avg Reward:   37.90   Noise: 0.36\n",
      "Episode 6400   Last length:    10   Reward:   11.00   Avg Reward:   23.36   Noise: 0.36\n",
      "Episode 6450   Last length:     9   Reward:   10.00   Avg Reward:   12.51   Noise: 0.35\n",
      "Episode 6500   Last length:     8   Reward:    9.00   Avg Reward:   11.61   Noise: 0.35\n",
      "Episode 6550   Last length:     9   Reward:   10.00   Avg Reward:   11.42   Noise: 0.34\n",
      "Episode 6600   Last length:    22   Reward:   23.00   Avg Reward:   11.42   Noise: 0.34\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "\n",
    "from DDPG.ddpg import DDPG\n",
    "\n",
    "args = {\n",
    "    'render': True,\n",
    "    'log_interval': 50\n",
    "}\n",
    "env = gym.make('CartPole-v1')\n",
    "episodes = 100000\n",
    "reward_history = []\n",
    "threshold = 195\n",
    "\n",
    "\n",
    "def main():   \n",
    "    task = {\n",
    "        'state_size': 4,\n",
    "        'action_size': 1,\n",
    "        'action_high': 1,\n",
    "        'action_low': 0\n",
    "    }\n",
    "    agent = DDPG(task)    \n",
    "    for i_episode in range(episodes):\n",
    "        running_reward = 0        \n",
    "        state = env.reset()\n",
    "        for t in range(10000):  # Don't infinite loop while learning\n",
    "            action, noise_coeff = agent.act(state, i_episode)                \n",
    "            state, reward, done, _ = env.step(action)  \n",
    "            agent.step(action, reward, state, done)\n",
    "            if args['render']:\n",
    "                env.render()                   \n",
    "            running_reward += reward            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        reward_history.append(running_reward)\n",
    "        \n",
    "        avg_reward = np.mean(reward_history[-100:])    \n",
    "        if i_episode % args['log_interval'] == 0:                    \n",
    "            print('Episode {}   Last length: {:5d}   Reward: {:7.2f}   Avg Reward: {:7.2f}   Noise: {:.2f}'.format(\n",
    "                i_episode, t, running_reward, avg_reward, noise_coeff))\n",
    "        if avg_reward > threshold and i_episode > 100:\n",
    "            print(\"Solved! Average 100-episode reward is now {}!\".format(avg_reward))\n",
    "            break\n",
    "            \n",
    "print(\"action_space={}\".format(env.action_space))\n",
    "print(\"obs_space={}\".format(env.observation_space))\n",
    "print(\"threshold={} \\n\".format(env.spec.reward_threshold))\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
