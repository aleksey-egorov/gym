{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from gym import wrappers\n",
    "from PIL import Image\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "\n",
    "from DDPG.ddpg import DDPG\n",
    "from DDPG.utils import ReplayBuffer, mkdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'BipedalWalker-v2'\n",
    "lr_base = 0.001\n",
    "lr_decay = 0.0001\n",
    "exp_noise_base = 0.5 \n",
    "exp_noise_decay = 0.002\n",
    "\n",
    "random_seed = 42\n",
    "gamma = 0.99                # discount for future rewards\n",
    "batch_size = 1024           # num of transitions sampled from replay buffer\n",
    "polyak = 0.999               # target policy update parameter (1-tau)\n",
    "max_episodes = 100000         # max num of episodes\n",
    "max_timesteps = 3000        # max timesteps in one episode\n",
    "max_buffer_length = 5000000\n",
    "log_interval = 10           # print avg reward after interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_config = [\n",
    "        {'dim': [None, 64], 'dropout': False, 'activation': 'relu'},\n",
    "        {'dim': [64, 64], 'dropout': False, 'activation': 'relu'},\n",
    "        {'dim': [64, None], 'dropout': False, 'activation': 'sigmoid'}\n",
    "    ]\n",
    "    \n",
    "critic_config = [\n",
    "        {'dim': [None, 64], 'dropout': False, 'activation': 'relu'},\n",
    "        {'dim': [64, 64], 'dropout': False , 'activation':'relu'},\n",
    "        {'dim': [64, 1], 'dropout': False, 'activation': False}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGTrainer():\n",
    "    \n",
    "    def __init__(self, env_name, actor_config, critic_config, random_seed=42, lr_base=0.001, lr_decay=0.00005, \n",
    "                 exp_noise_base=0.3, exp_noise_decay=0.0001, exploration_mu=0, exploration_theta=0.15, \n",
    "                 exploration_sigma=0.2, gamma=0.99, batch_size=1024, polyak=0.9999,\n",
    "                 max_episodes=100000, max_timesteps=3000, max_buffer_length=5000000, \n",
    "                 log_interval=5, threshold=None, lr_minimum=1e-10, exp_noise_minimum=1e-10,\n",
    "                 record_videos=True, record_interval=100):        \n",
    "        \n",
    "        self.algorithm_name = 'ddpg'\n",
    "        self.env_name = env_name\n",
    "        self.env = gym.make(env_name)\n",
    "        self.record_videos = record_videos\n",
    "        self.record_interval = record_interval        \n",
    "        if self.record_videos == True:\n",
    "            videos_dir = mkdir('.', 'videos')\n",
    "            monitor_dir = mkdir(videos_dir, self.algorithm_name)\n",
    "            should_record = lambda i: self.should_record\n",
    "            self.env = wrappers.Monitor(self.env, monitor_dir, video_callable=should_record, force=True)            \n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.shape[0]\n",
    "        self.action_low = self.env.action_space.low\n",
    "        self.action_high = self.env.action_space.high        \n",
    "        self.should_record = False\n",
    "        if not threshold == None:\n",
    "            self.threshold = threshold\n",
    "        else:    \n",
    "            self.threshold = self.env.spec.reward_threshold\n",
    "        \n",
    "        self.actor_config = actor_config\n",
    "        self.critic_config = critic_config\n",
    "        self.actor_config[0]['dim'][0] = self.state_dim\n",
    "        self.actor_config[-1]['dim'][1] = self.action_dim\n",
    "        self.critic_config[0]['dim'][0] = self.state_dim + self.action_dim        \n",
    "        \n",
    "        self.random_seed = random_seed\n",
    "        self.lr_base = lr_base\n",
    "        self.lr_decay = lr_decay\n",
    "        self.lr_minimum = lr_minimum\n",
    "        self.exp_noise_base = exp_noise_base\n",
    "        self.exp_noise_decay = exp_noise_decay     \n",
    "        self.exp_noise_minimum = exp_noise_minimum\n",
    "        self.exploration_mu = exploration_mu\n",
    "        self.exploration_theta = exploration_theta\n",
    "        self.exploration_sigma = exploration_sigma\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size        \n",
    "        self.polyak = polyak        \n",
    "        self.max_episodes = max_episodes\n",
    "        self.max_timesteps = max_timesteps\n",
    "        self.max_buffer_length = max_buffer_length\n",
    "        self.log_interval = log_interval\n",
    "        \n",
    "        prdir = mkdir('.', 'preTrained')\n",
    "        self.directory = mkdir(prdir, self.algorithm_name)\n",
    "        self.filename = \"{}_{}_{}\".format(self.algorithm_name, self.env_name, self.random_seed)\n",
    "        \n",
    "        self.policy = DDPG(self.actor_config, self.critic_config, self.action_dim, self.action_low, self.action_high, \n",
    "                           self.exploration_mu, exploration_theta, exploration_sigma)   \n",
    "        self.replay_buffer = ReplayBuffer(max_length=self.max_buffer_length)\n",
    "        \n",
    "        self.reward_history = []\n",
    "        self.make_plots = False       \n",
    "        \n",
    "        if self.random_seed:\n",
    "            print(\"Random Seed: {}\".format(self.random_seed))\n",
    "            self.env.seed(self.random_seed)\n",
    "            torch.manual_seed(self.random_seed)\n",
    "            np.random.seed(self.random_seed)\n",
    "            \n",
    "    def train(self):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        print(\"Training started ... \\n\")\n",
    "        print(\"action_space={}\".format(self.env.action_space))\n",
    "        print(\"obs_space={}\".format(self.env.observation_space))\n",
    "        print(\"threshold={}\".format(self.threshold)) \n",
    "        print(\"action_low={} action_high={} \\n\".format(self.action_low, self.action_high))    \n",
    "\n",
    "        # loading models\n",
    "        self.policy.load(self.directory, self.filename)\n",
    "                \n",
    "        # logging variables:        \n",
    "        log_f = open(\"train_{}.txt\".format(self.algorithm_name), \"w+\")\n",
    "        \n",
    "        avg_actor_loss = 0.0\n",
    "        avg_critic_loss = 0.0\n",
    "        \n",
    "        # training procedure:        \n",
    "        for episode in range(self.max_episodes):\n",
    "            \n",
    "            # Only record video during evaluation, every n steps\n",
    "            if episode % self.record_interval == 0:\n",
    "                self.should_record = True\n",
    "            \n",
    "            ep_reward = 0.0        \n",
    "            state = self.env.reset()\n",
    "                       \n",
    "            # calculate params\n",
    "            noise_coeff = max(self.exp_noise_base / (1.0 + episode * self.exp_noise_decay), self.exp_noise_minimum)\n",
    "            learning_rate = max(self.lr_base / (1.0 + episode * self.lr_decay), self.lr_minimum)      \n",
    "            self.policy.set_optimizers(lr=learning_rate)\n",
    "           \n",
    "            for t in range(self.max_timesteps):\n",
    "                \n",
    "                action = self.policy.select_action(state, noise_coeff)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                self.replay_buffer.add((state, action, reward, next_state, float(done)))\n",
    "                \n",
    "                # Updating policy\n",
    "                self.policy.update(self.replay_buffer, self.batch_size, self.gamma, self.polyak)\n",
    "                \n",
    "                state = next_state               \n",
    "                ep_reward += reward            \n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            self.reward_history.append(ep_reward)\n",
    "            avg_reward = np.mean(self.reward_history[-100:]) \n",
    "           \n",
    "            \n",
    "            # logging updates:        \n",
    "            log_f.write('{},{}\\n'.format(episode, ep_reward))\n",
    "            log_f.flush()\n",
    "           \n",
    "            if len(self.policy.actor_loss_list) > 0:               \n",
    "                avg_actor_loss = np.mean(self.policy.actor_loss_list[-100:])\n",
    "                avg_critic_loss = np.mean(self.policy.critic_loss_list[-100:])           \n",
    "            \n",
    "            if not self.make_plots and len(self.policy.actor_loss_list) > 200:\n",
    "                self.policy.actor_loss_list.pop(0)\n",
    "                self.policy.critic_loss_list.pop(0)               \n",
    "                self.reward_history.pop(0)                \n",
    "\n",
    "            # Print avg reward every log interval:\n",
    "            if episode % self.log_interval == 0:            \n",
    "                self.policy.save(self.directory, self.filename)\n",
    "                print(\"Ep:{:5d}  Rew:{:8.2f}  Avg Rew:{:8.2f}  LR:{:8.8f}  Bf:{:2.0f}  EN:{:0.4f}  Loss: {:5.3f} {:5.3f}\".format(\n",
    "                    episode, ep_reward, avg_reward, learning_rate, self.replay_buffer.get_fill(), \n",
    "                    noise_coeff, avg_actor_loss, avg_critic_loss))\n",
    "                        \n",
    "            self.should_record = False\n",
    "            \n",
    "            # if avg reward > threshold then save and stop traning:\n",
    "            if avg_reward >= self.threshold and episode > 100: \n",
    "                print(\"Ep:{:5d}  Rew:{:8.2f}  Avg Rew:{:8.2f}  LR:{:8.8f}  Bf:{:2.0f}  EN:{:0.4f}  Loss: {:5.3f} {:5.3f}\".format(\n",
    "                    episode, ep_reward, avg_reward, learning_rate, self.replay_buffer.get_fill(), \n",
    "                    noise_coeff, avg_actor_loss, avg_critic_loss))\n",
    "                print(\"########## Solved! ###########\")\n",
    "                name = self.filename + '_solved'\n",
    "                self.policy.save(self.directory, name)\n",
    "                log_f.close()\n",
    "                self.env.close()  \n",
    "                training_time = time.time() - start_time\n",
    "                print(\"Training time: {:6.2f} sec\".format(training_time))\n",
    "                break    \n",
    "                                \n",
    "    def test(self, episodes=3, render=True, save_gif=True):              \n",
    "\n",
    "        gifdir = mkdir('.','gif')\n",
    "        algdir = mkdir(gifdir, self.algorithm_name)\n",
    "        \n",
    "        for episode in range(1, episodes+1):\n",
    "            ep_reward = 0.0\n",
    "            state = self.env.reset()    \n",
    "            epdir = mkdir(algdir, str(episode))\n",
    "                       \n",
    "            for t in range(self.max_timesteps):\n",
    "                action = self.policy.select_action(state, 0)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                self.replay_buffer.add((state, action, reward, next_state, float(done)))\n",
    "                state = next_state               \n",
    "                ep_reward += reward                                  \n",
    "                \n",
    "                if save_gif:                                       \n",
    "                    img = self.env.render(mode = 'rgb_array')\n",
    "                    img = Image.fromarray(img)\n",
    "                    img.save('{}/{}.jpg'.format(epdir, t))\n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "            print('Test episode: {}\\tReward: {:4.2f}'.format(episode, ep_reward))           \n",
    "            self.env.close()        \n",
    "                   \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "ACTOR=Sequential(\n",
      "  (0): Linear(in_features=24, out_features=64, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=4, bias=True)\n",
      "  (5): Sigmoid()\n",
      ")\n",
      "ACTOR=Sequential(\n",
      "  (0): Linear(in_features=24, out_features=64, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=4, bias=True)\n",
      "  (5): Sigmoid()\n",
      ")\n",
      "CRITIC=Sequential(\n",
      "  (0): Linear(in_features=28, out_features=64, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "CRITIC=Sequential(\n",
      "  (0): Linear(in_features=28, out_features=64, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "Random Seed: 42\n",
      "Training started ... \n",
      "\n",
      "action_space=Box(4,)\n",
      "obs_space=Box(24,)\n",
      "threshold=300\n",
      "action_low=[-1. -1. -1. -1.] action_high=[1. 1. 1. 1.] \n",
      "\n",
      "DIR=./preTrained/ddpg NAME=ddpg_BipedalWalker-v2_42\n",
      "No models to load\n",
      "Ep:    0  Rew:  -97.90  Avg Rew:  -97.90  LR:0.00100000  Bf: 0  EN:0.5000  Loss: 0.000 0.000\n",
      "Ep:   10  Rew: -100.68  Avg Rew:  -86.65  LR:0.00099900  Bf: 0  EN:0.4902  Loss: 0.000 0.000\n",
      "Ep:   20  Rew:  -96.24  Avg Rew:  -83.84  LR:0.00099800  Bf: 0  EN:0.4808  Loss: 0.000 0.000\n",
      "Ep:   30  Rew: -115.81  Avg Rew:  -92.44  LR:0.00099701  Bf: 0  EN:0.4717  Loss: 0.024 14.738\n",
      "Ep:   40  Rew: -115.76  Avg Rew: -107.12  LR:0.00099602  Bf: 0  EN:0.4630  Loss: -0.252 9.846\n",
      "Ep:   50  Rew: -119.30  Avg Rew: -116.56  LR:0.00099502  Bf: 0  EN:0.4545  Loss: -0.500 9.283\n",
      "Ep:   60  Rew: -112.93  Avg Rew: -113.58  LR:0.00099404  Bf: 0  EN:0.4464  Loss: -1.421 3.239\n",
      "Ep:   70  Rew: -111.41  Avg Rew: -114.93  LR:0.00099305  Bf: 0  EN:0.4386  Loss: -1.972 3.457\n",
      "Ep:   80  Rew:  -90.73  Avg Rew: -115.48  LR:0.00099206  Bf: 0  EN:0.4310  Loss: -3.214 1.782\n",
      "Ep:   90  Rew: -109.36  Avg Rew: -113.89  LR:0.00099108  Bf: 1  EN:0.4237  Loss: -7.577 3.223\n",
      "Ep:  100  Rew: -117.06  Avg Rew: -117.10  LR:0.00099010  Bf: 1  EN:0.4167  Loss: -7.958 3.793\n",
      "Ep:  110  Rew: -112.73  Avg Rew: -119.69  LR:0.00098912  Bf: 1  EN:0.4098  Loss: -8.773 3.439\n",
      "Ep:  120  Rew: -118.29  Avg Rew: -119.23  LR:0.00098814  Bf: 1  EN:0.4032  Loss: -8.898 4.811\n",
      "Ep:  130  Rew: -115.51  Avg Rew: -117.42  LR:0.00098717  Bf: 1  EN:0.3968  Loss: -9.250 4.251\n",
      "Ep:  140  Rew: -113.32  Avg Rew: -115.69  LR:0.00098619  Bf: 1  EN:0.3906  Loss: -9.621 5.236\n",
      "Ep:  150  Rew: -123.14  Avg Rew: -118.03  LR:0.00098522  Bf: 1  EN:0.3846  Loss: -9.954 5.966\n",
      "Ep:  160  Rew: -128.34  Avg Rew: -121.44  LR:0.00098425  Bf: 1  EN:0.3788  Loss: -10.377 6.766\n",
      "Ep:  170  Rew: -126.92  Avg Rew: -123.61  LR:0.00098328  Bf: 1  EN:0.3731  Loss: -10.826 7.538\n",
      "Ep:  180  Rew: -111.84  Avg Rew: -123.43  LR:0.00098232  Bf: 1  EN:0.3676  Loss: -11.789 6.711\n",
      "Ep:  190  Rew: -110.99  Avg Rew: -121.97  LR:0.00098135  Bf: 1  EN:0.3623  Loss: -12.267 8.962\n",
      "Ep:  200  Rew: -101.02  Avg Rew: -120.54  LR:0.00098039  Bf: 1  EN:0.3571  Loss: -12.262 7.531\n",
      "Ep:  210  Rew: -123.33  Avg Rew: -117.75  LR:0.00097943  Bf: 1  EN:0.3521  Loss: -12.356 7.158\n",
      "Ep:  220  Rew: -108.63  Avg Rew: -117.45  LR:0.00097847  Bf: 1  EN:0.3472  Loss: -13.892 5.520\n",
      "Ep:  230  Rew: -111.89  Avg Rew: -115.25  LR:0.00097752  Bf: 1  EN:0.3425  Loss: -13.603 7.312\n",
      "Ep:  240  Rew: -110.93  Avg Rew: -112.63  LR:0.00097656  Bf: 1  EN:0.3378  Loss: -13.709 7.139\n",
      "Ep:  250  Rew: -112.86  Avg Rew: -113.44  LR:0.00097561  Bf: 1  EN:0.3333  Loss: -13.756 6.723\n",
      "Ep:  260  Rew: -109.70  Avg Rew: -113.51  LR:0.00097466  Bf: 1  EN:0.3289  Loss: -13.706 6.710\n",
      "Ep:  270  Rew: -131.76  Avg Rew: -113.29  LR:0.00097371  Bf: 1  EN:0.3247  Loss: -13.823 7.652\n",
      "Ep:  280  Rew: -109.89  Avg Rew: -111.46  LR:0.00097276  Bf: 1  EN:0.3205  Loss: -13.985 6.906\n",
      "Ep:  290  Rew: -107.30  Avg Rew: -112.14  LR:0.00097182  Bf: 1  EN:0.3165  Loss: -13.934 7.317\n",
      "Ep:  300  Rew: -111.08  Avg Rew: -111.16  LR:0.00097087  Bf: 1  EN:0.3125  Loss: -14.331 7.413\n",
      "Ep:  310  Rew: -113.52  Avg Rew: -110.99  LR:0.00096993  Bf: 1  EN:0.3086  Loss: -14.290 7.208\n",
      "Ep:  320  Rew: -106.27  Avg Rew: -109.61  LR:0.00096899  Bf: 1  EN:0.3049  Loss: -14.491 7.752\n",
      "Ep:  330  Rew: -108.29  Avg Rew: -109.74  LR:0.00096805  Bf: 1  EN:0.3012  Loss: -14.690 7.900\n",
      "Ep:  340  Rew: -103.92  Avg Rew: -108.62  LR:0.00096712  Bf: 1  EN:0.2976  Loss: -15.122 8.270\n",
      "Ep:  350  Rew: -106.23  Avg Rew: -107.11  LR:0.00096618  Bf: 1  EN:0.2941  Loss: -15.561 7.516\n",
      "Ep:  360  Rew: -106.95  Avg Rew: -105.73  LR:0.00096525  Bf: 1  EN:0.2907  Loss: -16.025 7.901\n",
      "Ep:  370  Rew: -102.95  Avg Rew: -105.35  LR:0.00096432  Bf: 1  EN:0.2874  Loss: -16.987 7.321\n",
      "Ep:  380  Rew: -103.38  Avg Rew: -104.54  LR:0.00096339  Bf: 1  EN:0.2841  Loss: -17.367 7.218\n",
      "Ep:  390  Rew: -101.94  Avg Rew: -102.48  LR:0.00096246  Bf: 1  EN:0.2809  Loss: -18.137 7.083\n",
      "Ep:  400  Rew: -101.21  Avg Rew: -102.26  LR:0.00096154  Bf: 1  EN:0.2778  Loss: -18.821 7.531\n",
      "Ep:  410  Rew: -104.99  Avg Rew: -102.87  LR:0.00096061  Bf: 1  EN:0.2747  Loss: -19.411 6.993\n",
      "Ep:  420  Rew: -103.05  Avg Rew: -103.40  LR:0.00095969  Bf: 1  EN:0.2717  Loss: -19.821 6.478\n",
      "Ep:  430  Rew: -104.36  Avg Rew: -103.91  LR:0.00095877  Bf: 1  EN:0.2688  Loss: -20.232 7.178\n",
      "Ep:  440  Rew: -104.25  Avg Rew: -103.48  LR:0.00095785  Bf: 1  EN:0.2660  Loss: -20.769 7.021\n",
      "Ep:  450  Rew: -104.95  Avg Rew: -103.01  LR:0.00095694  Bf: 1  EN:0.2632  Loss: -21.230 6.402\n",
      "Ep:  460  Rew: -103.97  Avg Rew: -102.16  LR:0.00095602  Bf: 1  EN:0.2604  Loss: -21.706 7.763\n",
      "Ep:  470  Rew: -101.90  Avg Rew: -102.08  LR:0.00095511  Bf: 1  EN:0.2577  Loss: -22.094 8.109\n",
      "Ep:  480  Rew: -101.82  Avg Rew: -102.46  LR:0.00095420  Bf: 1  EN:0.2551  Loss: -22.365 7.233\n",
      "Ep:  490  Rew: -102.35  Avg Rew: -102.21  LR:0.00095329  Bf: 1  EN:0.2525  Loss: -22.669 6.803\n",
      "Ep:  500  Rew: -102.50  Avg Rew: -102.03  LR:0.00095238  Bf: 1  EN:0.2500  Loss: -22.701 8.084\n",
      "Ep:  510  Rew: -116.89  Avg Rew: -103.49  LR:0.00095147  Bf: 1  EN:0.2475  Loss: -23.046 7.406\n",
      "Ep:  520  Rew: -111.65  Avg Rew: -105.66  LR:0.00095057  Bf: 1  EN:0.2451  Loss: -23.172 6.969\n",
      "Ep:  530  Rew: -111.87  Avg Rew: -107.12  LR:0.00094967  Bf: 1  EN:0.2427  Loss: -23.169 7.566\n",
      "Ep:  540  Rew: -104.93  Avg Rew: -106.22  LR:0.00094877  Bf: 1  EN:0.2404  Loss: -23.319 7.394\n",
      "Ep:  550  Rew: -103.05  Avg Rew: -106.06  LR:0.00094787  Bf: 1  EN:0.2381  Loss: -23.182 7.403\n",
      "Ep:  560  Rew: -104.90  Avg Rew: -105.60  LR:0.00094697  Bf: 1  EN:0.2358  Loss: -22.737 7.344\n",
      "Ep:  570  Rew: -116.20  Avg Rew: -106.89  LR:0.00094607  Bf: 1  EN:0.2336  Loss: -22.917 7.806\n",
      "Ep:  580  Rew: -119.06  Avg Rew: -107.52  LR:0.00094518  Bf: 1  EN:0.2315  Loss: -22.697 6.987\n",
      "Ep:  590  Rew: -104.76  Avg Rew: -106.79  LR:0.00094429  Bf: 1  EN:0.2294  Loss: -22.807 7.419\n",
      "Ep:  600  Rew: -102.50  Avg Rew: -105.46  LR:0.00094340  Bf: 1  EN:0.2273  Loss: -22.338 7.520\n",
      "Ep:  610  Rew: -106.06  Avg Rew: -107.68  LR:0.00094251  Bf: 1  EN:0.2252  Loss: -22.321 7.899\n",
      "Ep:  620  Rew: -104.77  Avg Rew: -110.30  LR:0.00094162  Bf: 1  EN:0.2232  Loss: -22.145 8.090\n",
      "Ep:  630  Rew: -104.00  Avg Rew: -108.69  LR:0.00094073  Bf: 1  EN:0.2212  Loss: -22.315 7.884\n",
      "Ep:  640  Rew: -105.80  Avg Rew: -108.52  LR:0.00093985  Bf: 1  EN:0.2193  Loss: -22.146 7.744\n",
      "Ep:  650  Rew: -118.97  Avg Rew: -108.09  LR:0.00093897  Bf: 1  EN:0.2174  Loss: -22.185 8.446\n",
      "Ep:  660  Rew: -105.20  Avg Rew: -108.84  LR:0.00093809  Bf: 1  EN:0.2155  Loss: -21.572 8.607\n",
      "Ep:  670  Rew: -104.08  Avg Rew: -107.28  LR:0.00093721  Bf: 1  EN:0.2137  Loss: -21.727 8.413\n",
      "Ep:  680  Rew: -103.09  Avg Rew: -106.27  LR:0.00093633  Bf: 1  EN:0.2119  Loss: -21.826 8.638\n",
      "Ep:  690  Rew: -120.43  Avg Rew: -106.76  LR:0.00093545  Bf: 1  EN:0.2101  Loss: -21.856 8.445\n",
      "Ep:  700  Rew: -106.51  Avg Rew: -108.40  LR:0.00093458  Bf: 1  EN:0.2083  Loss: -22.813 7.750\n",
      "Ep:  710  Rew: -104.78  Avg Rew: -107.98  LR:0.00093371  Bf: 1  EN:0.2066  Loss: -22.776 8.307\n",
      "Ep:  720  Rew: -100.22  Avg Rew: -107.90  LR:0.00093284  Bf: 1  EN:0.2049  Loss: -23.114 9.165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep:  730  Rew: -100.28  Avg Rew: -106.37  LR:0.00093197  Bf: 2  EN:0.2033  Loss: -23.060 8.886\n",
      "Ep:  740  Rew: -101.48  Avg Rew: -101.92  LR:0.00093110  Bf: 2  EN:0.2016  Loss: -22.771 9.178\n",
      "Ep:  750  Rew:  -99.37  Avg Rew: -101.81  LR:0.00093023  Bf: 2  EN:0.2000  Loss: -22.774 10.606\n",
      "Ep:  760  Rew: -102.94  Avg Rew: -102.26  LR:0.00092937  Bf: 2  EN:0.1984  Loss: -22.531 10.199\n",
      "Ep:  770  Rew: -115.94  Avg Rew: -102.26  LR:0.00092851  Bf: 2  EN:0.1969  Loss: -22.776 10.416\n",
      "Ep:  780  Rew:  -97.16  Avg Rew: -102.93  LR:0.00092764  Bf: 2  EN:0.1953  Loss: -22.762 10.475\n",
      "Ep:  790  Rew: -106.71  Avg Rew: -103.50  LR:0.00092678  Bf: 2  EN:0.1938  Loss: -22.764 10.093\n",
      "Ep:  800  Rew: -105.85  Avg Rew: -103.24  LR:0.00092593  Bf: 2  EN:0.1923  Loss: -22.489 10.496\n",
      "Ep:  810  Rew:  -99.84  Avg Rew: -104.26  LR:0.00092507  Bf: 2  EN:0.1908  Loss: -22.387 11.094\n",
      "Ep:  820  Rew: -107.27  Avg Rew: -105.83  LR:0.00092421  Bf: 2  EN:0.1894  Loss: -22.088 10.646\n",
      "Ep:  830  Rew: -120.17  Avg Rew: -107.02  LR:0.00092336  Bf: 2  EN:0.1880  Loss: -21.799 11.749\n",
      "Ep:  840  Rew: -116.66  Avg Rew: -110.27  LR:0.00092251  Bf: 2  EN:0.1866  Loss: -22.105 10.374\n",
      "Ep:  850  Rew: -105.31  Avg Rew: -110.53  LR:0.00092166  Bf: 2  EN:0.1852  Loss: -21.481 10.813\n",
      "Ep:  860  Rew: -114.64  Avg Rew: -108.61  LR:0.00092081  Bf: 2  EN:0.1838  Loss: -21.369 11.210\n",
      "Ep:  870  Rew: -116.39  Avg Rew: -110.37  LR:0.00091996  Bf: 2  EN:0.1825  Loss: -20.365 11.287\n",
      "Ep:  880  Rew: -110.90  Avg Rew: -112.85  LR:0.00091912  Bf: 2  EN:0.1812  Loss: -20.407 11.564\n",
      "Ep:  890  Rew: -113.68  Avg Rew: -117.36  LR:0.00091827  Bf: 2  EN:0.1799  Loss: -20.655 12.025\n",
      "Ep:  900  Rew: -115.44  Avg Rew: -117.89  LR:0.00091743  Bf: 2  EN:0.1786  Loss: -20.485 11.560\n",
      "Ep:  910  Rew: -117.87  Avg Rew: -122.38  LR:0.00091659  Bf: 2  EN:0.1773  Loss: -20.319 11.322\n",
      "Ep:  920  Rew: -138.46  Avg Rew: -122.10  LR:0.00091575  Bf: 2  EN:0.1761  Loss: -20.776 10.999\n",
      "Ep:  930  Rew: -118.61  Avg Rew: -120.22  LR:0.00091491  Bf: 2  EN:0.1748  Loss: -20.542 11.938\n",
      "Ep:  940  Rew: -111.59  Avg Rew: -115.31  LR:0.00091408  Bf: 2  EN:0.1736  Loss: -20.663 11.111\n",
      "Ep:  950  Rew: -120.87  Avg Rew: -113.83  LR:0.00091324  Bf: 2  EN:0.1724  Loss: -20.256 11.483\n",
      "Ep:  960  Rew: -123.96  Avg Rew: -114.46  LR:0.00091241  Bf: 2  EN:0.1712  Loss: -20.506 11.526\n",
      "Ep:  970  Rew: -114.89  Avg Rew: -114.51  LR:0.00091158  Bf: 2  EN:0.1701  Loss: -20.945 11.685\n",
      "Ep:  980  Rew: -107.38  Avg Rew: -112.74  LR:0.00091075  Bf: 2  EN:0.1689  Loss: -21.560 12.348\n",
      "Ep:  990  Rew: -167.37  Avg Rew: -118.86  LR:0.00090992  Bf: 2  EN:0.1678  Loss: -22.288 11.818\n",
      "Ep: 1000  Rew: -155.19  Avg Rew: -132.35  LR:0.00090909  Bf: 2  EN:0.1667  Loss: -28.064 10.970\n",
      "Ep: 1010  Rew: -105.17  Avg Rew: -135.33  LR:0.00090827  Bf: 3  EN:0.1656  Loss: -28.883 11.389\n",
      "Ep: 1020  Rew: -102.43  Avg Rew: -124.97  LR:0.00090744  Bf: 3  EN:0.1645  Loss: -28.928 11.295\n",
      "Ep: 1030  Rew:  -99.13  Avg Rew: -112.23  LR:0.00090662  Bf: 3  EN:0.1634  Loss: -29.022 11.141\n",
      "Ep: 1040  Rew: -221.34  Avg Rew: -126.94  LR:0.00090580  Bf: 3  EN:0.1623  Loss: -32.260 13.411\n",
      "Ep: 1050  Rew: -124.42  Avg Rew: -142.25  LR:0.00090498  Bf: 3  EN:0.1613  Loss: -40.517 12.916\n",
      "Ep: 1060  Rew:  -97.15  Avg Rew: -141.88  LR:0.00090416  Bf: 3  EN:0.1603  Loss: -43.273 12.419\n",
      "Ep: 1070  Rew:  -88.92  Avg Rew: -122.64  LR:0.00090334  Bf: 4  EN:0.1592  Loss: -46.162 11.714\n",
      "Ep: 1080  Rew:  -65.22  Avg Rew: -115.83  LR:0.00090253  Bf: 4  EN:0.1582  Loss: -48.642 11.198\n",
      "Ep: 1090  Rew: -112.73  Avg Rew: -100.41  LR:0.00090171  Bf: 4  EN:0.1572  Loss: -49.725 10.568\n",
      "Ep: 1100  Rew: -117.85  Avg Rew:  -97.89  LR:0.00090090  Bf: 4  EN:0.1562  Loss: -49.537 10.948\n",
      "Ep: 1110  Rew: -112.19  Avg Rew: -109.90  LR:0.00090009  Bf: 4  EN:0.1553  Loss: -49.348 11.457\n",
      "Ep: 1120  Rew: -116.34  Avg Rew: -113.75  LR:0.00089928  Bf: 4  EN:0.1543  Loss: -49.277 11.291\n",
      "Ep: 1130  Rew: -116.22  Avg Rew: -116.20  LR:0.00089847  Bf: 4  EN:0.1534  Loss: -48.401 11.120\n",
      "Ep: 1140  Rew: -144.36  Avg Rew: -118.84  LR:0.00089767  Bf: 4  EN:0.1524  Loss: -48.047 12.111\n",
      "Ep: 1150  Rew: -143.42  Avg Rew: -125.26  LR:0.00089686  Bf: 5  EN:0.1515  Loss: -48.051 9.897\n",
      "Ep: 1160  Rew: -122.82  Avg Rew: -128.29  LR:0.00089606  Bf: 5  EN:0.1506  Loss: -47.835 11.336\n",
      "Ep: 1170  Rew:   36.20  Avg Rew: -118.97  LR:0.00089526  Bf: 5  EN:0.1497  Loss: -47.660 11.181\n",
      "Ep: 1180  Rew: -122.85  Avg Rew: -103.67  LR:0.00089445  Bf: 5  EN:0.1488  Loss: -48.081 9.913\n",
      "Ep: 1190  Rew: -121.55  Avg Rew: -103.44  LR:0.00089366  Bf: 5  EN:0.1479  Loss: -48.764 11.310\n",
      "Ep: 1200  Rew: -129.11  Avg Rew: -123.99  LR:0.00089286  Bf: 5  EN:0.1471  Loss: -47.873 11.728\n",
      "Ep: 1210  Rew: -127.92  Avg Rew: -128.77  LR:0.00089206  Bf: 5  EN:0.1462  Loss: -48.139 10.876\n",
      "Ep: 1220  Rew: -114.51  Avg Rew: -123.95  LR:0.00089127  Bf: 5  EN:0.1453  Loss: -47.791 11.215\n",
      "Ep: 1230  Rew: -114.74  Avg Rew: -118.82  LR:0.00089047  Bf: 5  EN:0.1445  Loss: -47.681 11.268\n",
      "Ep: 1240  Rew: -142.63  Avg Rew: -121.52  LR:0.00088968  Bf: 6  EN:0.1437  Loss: -49.811 12.532\n",
      "Ep: 1250  Rew: -159.54  Avg Rew: -133.16  LR:0.00088889  Bf: 6  EN:0.1429  Loss: -52.388 12.152\n",
      "Ep: 1260  Rew: -127.81  Avg Rew: -142.37  LR:0.00088810  Bf: 6  EN:0.1420  Loss: -56.612 11.106\n",
      "Ep: 1270  Rew: -147.64  Avg Rew: -138.78  LR:0.00088731  Bf: 6  EN:0.1412  Loss: -56.448 11.345\n",
      "Ep: 1280  Rew: -126.55  Avg Rew: -133.43  LR:0.00088652  Bf: 6  EN:0.1404  Loss: -56.585 11.177\n",
      "Ep: 1290  Rew: -122.54  Avg Rew: -134.19  LR:0.00088574  Bf: 6  EN:0.1397  Loss: -56.291 11.256\n",
      "Ep: 1300  Rew:   57.08  Avg Rew: -135.44  LR:0.00088496  Bf: 7  EN:0.1389  Loss: -56.285 11.848\n",
      "Ep: 1310  Rew: -111.12  Avg Rew:  -99.21  LR:0.00088417  Bf: 7  EN:0.1381  Loss: -54.329 10.644\n",
      "Ep: 1320  Rew: -152.86  Avg Rew: -100.37  LR:0.00088339  Bf: 7  EN:0.1374  Loss: -52.857 10.546\n",
      "Ep: 1330  Rew: -149.94  Avg Rew: -111.70  LR:0.00088261  Bf: 7  EN:0.1366  Loss: -52.101 10.409\n",
      "Ep: 1340  Rew: -186.31  Avg Rew: -134.23  LR:0.00088183  Bf: 7  EN:0.1359  Loss: -51.961 11.127\n",
      "Ep: 1350  Rew: -119.44  Avg Rew: -134.84  LR:0.00088106  Bf: 7  EN:0.1351  Loss: -51.424 9.868\n",
      "Ep: 1360  Rew:    0.30  Avg Rew: -106.45  LR:0.00088028  Bf: 8  EN:0.1344  Loss: -51.112 10.745\n",
      "Ep: 1370  Rew:  -94.19  Avg Rew:  -61.05  LR:0.00087951  Bf: 8  EN:0.1337  Loss: -51.070 11.403\n",
      "Ep: 1380  Rew: -107.65  Avg Rew:  -52.00  LR:0.00087873  Bf: 8  EN:0.1330  Loss: -50.715 10.133\n",
      "Ep: 1390  Rew:  -81.70  Avg Rew:  -55.88  LR:0.00087796  Bf: 8  EN:0.1323  Loss: -50.756 9.273\n",
      "Ep: 1400  Rew: -129.61  Avg Rew:  -93.79  LR:0.00087719  Bf: 8  EN:0.1316  Loss: -50.696 9.651\n",
      "Ep: 1410  Rew: -102.19  Avg Rew: -113.04  LR:0.00087642  Bf: 9  EN:0.1309  Loss: -49.972 8.780\n",
      "Ep: 1420  Rew: -135.60  Avg Rew: -143.88  LR:0.00087566  Bf: 9  EN:0.1302  Loss: -50.073 9.784\n",
      "Ep: 1430  Rew:  -86.06  Avg Rew: -127.10  LR:0.00087489  Bf: 9  EN:0.1295  Loss: -49.861 10.390\n",
      "Ep: 1440  Rew: -131.34  Avg Rew: -111.36  LR:0.00087413  Bf: 9  EN:0.1289  Loss: -49.647 9.416\n",
      "Ep: 1450  Rew:  116.86  Avg Rew:  -97.84  LR:0.00087336  Bf: 9  EN:0.1282  Loss: -49.331 9.187\n",
      "Ep: 1460  Rew: -175.79  Avg Rew: -112.79  LR:0.00087260  Bf: 9  EN:0.1276  Loss: -48.986 10.615\n",
      "Ep: 1470  Rew: -120.98  Avg Rew: -113.31  LR:0.00087184  Bf: 9  EN:0.1269  Loss: -48.519 9.566\n",
      "Ep: 1480  Rew: -148.24  Avg Rew: -129.56  LR:0.00087108  Bf: 9  EN:0.1263  Loss: -47.703 9.370\n",
      "Ep: 1490  Rew:  -44.36  Avg Rew: -127.39  LR:0.00087032  Bf:10  EN:0.1256  Loss: -46.212 9.230\n",
      "Ep: 1500  Rew: -101.32  Avg Rew: -115.87  LR:0.00086957  Bf:10  EN:0.1250  Loss: -45.569 8.734\n",
      "Ep: 1510  Rew:   67.72  Avg Rew:  -92.35  LR:0.00086881  Bf:10  EN:0.1244  Loss: -44.082 8.727\n",
      "Ep: 1520  Rew:   63.24  Avg Rew:  -69.41  LR:0.00086806  Bf:10  EN:0.1238  Loss: -42.098 8.190\n",
      "Ep: 1530  Rew:   58.98  Avg Rew:  -40.95  LR:0.00086730  Bf:10  EN:0.1232  Loss: -41.820 8.342\n",
      "Ep: 1540  Rew: -107.98  Avg Rew:  -51.62  LR:0.00086655  Bf:11  EN:0.1225  Loss: -41.354 8.179\n",
      "Ep: 1550  Rew: -155.26  Avg Rew: -101.40  LR:0.00086580  Bf:11  EN:0.1220  Loss: -40.934 8.290\n",
      "Ep: 1560  Rew: -223.53  Avg Rew: -152.13  LR:0.00086505  Bf:11  EN:0.1214  Loss: -39.363 7.171\n",
      "Ep: 1570  Rew: -136.06  Avg Rew: -168.97  LR:0.00086430  Bf:11  EN:0.1208  Loss: -38.267 7.211\n",
      "Ep: 1580  Rew:   43.64  Avg Rew: -159.34  LR:0.00086356  Bf:12  EN:0.1202  Loss: -37.403 6.447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 1590  Rew:  169.74  Avg Rew:  -38.82  LR:0.00086281  Bf:12  EN:0.1196  Loss: -36.960 5.957\n",
      "Ep: 1600  Rew:  -32.41  Avg Rew:   39.84  LR:0.00086207  Bf:12  EN:0.1190  Loss: -36.292 5.830\n",
      "Ep: 1610  Rew:  -72.82  Avg Rew:   26.29  LR:0.00086133  Bf:12  EN:0.1185  Loss: -36.063 5.793\n",
      "Ep: 1620  Rew:  -76.42  Avg Rew:  -46.97  LR:0.00086059  Bf:12  EN:0.1179  Loss: -35.847 6.859\n",
      "Ep: 1630  Rew: -103.69  Avg Rew:  -43.75  LR:0.00085985  Bf:12  EN:0.1174  Loss: -35.184 6.516\n",
      "Ep: 1640  Rew:  -54.81  Avg Rew:  -50.33  LR:0.00085911  Bf:13  EN:0.1168  Loss: -34.670 6.263\n",
      "Ep: 1650  Rew:   98.10  Avg Rew:  -19.84  LR:0.00085837  Bf:13  EN:0.1163  Loss: -33.856 5.867\n",
      "Ep: 1660  Rew:   67.07  Avg Rew:  -31.37  LR:0.00085763  Bf:13  EN:0.1157  Loss: -33.030 6.612\n",
      "Ep: 1670  Rew:  -54.55  Avg Rew:  -65.35  LR:0.00085690  Bf:13  EN:0.1152  Loss: -32.397 5.212\n",
      "Ep: 1680  Rew:  -74.06  Avg Rew:  -91.51  LR:0.00085616  Bf:13  EN:0.1147  Loss: -32.045 5.286\n",
      "Ep: 1690  Rew:   56.94  Avg Rew:  -44.08  LR:0.00085543  Bf:13  EN:0.1142  Loss: -31.015 4.988\n",
      "Ep: 1700  Rew:  -65.86  Avg Rew:  -17.24  LR:0.00085470  Bf:14  EN:0.1136  Loss: -30.331 5.221\n",
      "Ep: 1710  Rew:  -47.23  Avg Rew:  -24.19  LR:0.00085397  Bf:14  EN:0.1131  Loss: -29.698 4.188\n",
      "Ep: 1720  Rew:  -73.65  Avg Rew:  -64.82  LR:0.00085324  Bf:14  EN:0.1126  Loss: -29.531 4.868\n",
      "Ep: 1730  Rew:  -83.56  Avg Rew:  -70.07  LR:0.00085251  Bf:14  EN:0.1121  Loss: -29.078 5.413\n",
      "Ep: 1740  Rew: -109.03  Avg Rew:   -9.23  LR:0.00085179  Bf:14  EN:0.1116  Loss: -28.226 4.650\n",
      "Ep: 1750  Rew:  155.28  Avg Rew:    4.66  LR:0.00085106  Bf:14  EN:0.1111  Loss: -27.987 4.442\n",
      "Ep: 1760  Rew: -118.60  Avg Rew:  -36.10  LR:0.00085034  Bf:15  EN:0.1106  Loss: -27.732 4.826\n",
      "Ep: 1770  Rew: -101.78  Avg Rew:  -72.64  LR:0.00084962  Bf:15  EN:0.1101  Loss: -27.559 5.122\n",
      "Ep: 1780  Rew: -103.37  Avg Rew:  -74.43  LR:0.00084890  Bf:15  EN:0.1096  Loss: -27.151 4.731\n",
      "Ep: 1790  Rew: -153.41  Avg Rew:  -80.81  LR:0.00084818  Bf:15  EN:0.1092  Loss: -26.952 4.785\n",
      "Ep: 1800  Rew: -101.65  Avg Rew:  -81.36  LR:0.00084746  Bf:15  EN:0.1087  Loss: -26.718 5.554\n",
      "Ep: 1810  Rew:  -30.82  Avg Rew:  -52.02  LR:0.00084674  Bf:15  EN:0.1082  Loss: -26.542 4.787\n",
      "Ep: 1820  Rew: -155.61  Avg Rew:  -14.04  LR:0.00084602  Bf:15  EN:0.1078  Loss: -26.700 4.701\n",
      "Ep: 1830  Rew: -197.81  Avg Rew:  -25.96  LR:0.00084531  Bf:16  EN:0.1073  Loss: -26.769 4.305\n",
      "Ep: 1840  Rew: -115.62  Avg Rew:  -80.13  LR:0.00084459  Bf:16  EN:0.1068  Loss: -26.845 4.313\n",
      "Ep: 1850  Rew:   90.36  Avg Rew:  -92.06  LR:0.00084388  Bf:16  EN:0.1064  Loss: -26.642 4.463\n",
      "Ep: 1860  Rew:  247.72  Avg Rew:    4.19  LR:0.00084317  Bf:16  EN:0.1059  Loss: -27.071 4.489\n",
      "Ep: 1870  Rew:  253.54  Avg Rew:   87.43  LR:0.00084246  Bf:16  EN:0.1055  Loss: -27.116 4.554\n",
      "Ep: 1880  Rew:  225.77  Avg Rew:  129.51  LR:0.00084175  Bf:16  EN:0.1050  Loss: -27.405 4.338\n",
      "Ep: 1890  Rew:  198.51  Avg Rew:  118.69  LR:0.00084104  Bf:16  EN:0.1046  Loss: -26.962 3.842\n",
      "Ep: 1900  Rew: -114.96  Avg Rew:  115.66  LR:0.00084034  Bf:17  EN:0.1042  Loss: -26.872 4.008\n",
      "Ep: 1910  Rew:  242.37  Avg Rew:   48.59  LR:0.00083963  Bf:17  EN:0.1037  Loss: -26.733 3.879\n",
      "Ep: 1920  Rew: -105.38  Avg Rew:   12.08  LR:0.00083893  Bf:17  EN:0.1033  Loss: -26.855 3.708\n",
      "Ep: 1930  Rew: -100.58  Avg Rew:    2.81  LR:0.00083822  Bf:17  EN:0.1029  Loss: -26.788 4.491\n",
      "Ep: 1940  Rew: -103.05  Avg Rew:  -20.53  LR:0.00083752  Bf:17  EN:0.1025  Loss: -26.884 4.286\n",
      "Ep: 1950  Rew:  -61.49  Avg Rew:  -47.52  LR:0.00083682  Bf:17  EN:0.1020  Loss: -26.813 3.889\n",
      "Ep: 1960  Rew: -102.18  Avg Rew:  -81.79  LR:0.00083612  Bf:17  EN:0.1016  Loss: -26.752 3.957\n",
      "Ep: 1970  Rew:  -85.60  Avg Rew: -109.96  LR:0.00083542  Bf:17  EN:0.1012  Loss: -26.921 4.096\n",
      "Ep: 1980  Rew: -119.45  Avg Rew: -115.51  LR:0.00083472  Bf:17  EN:0.1008  Loss: -26.628 4.604\n",
      "Ep: 1990  Rew: -122.34  Avg Rew: -110.84  LR:0.00083403  Bf:18  EN:0.1004  Loss: -26.411 4.281\n",
      "Ep: 2000  Rew: -105.22  Avg Rew: -104.44  LR:0.00083333  Bf:18  EN:0.1000  Loss: -26.492 4.330\n",
      "Ep: 2010  Rew: -100.64  Avg Rew:  -89.93  LR:0.00083264  Bf:18  EN:0.0996  Loss: -26.516 4.449\n",
      "Ep: 2020  Rew:  -19.47  Avg Rew:  -77.25  LR:0.00083195  Bf:18  EN:0.0992  Loss: -26.315 4.297\n",
      "Ep: 2030  Rew:  -24.93  Avg Rew:  -66.35  LR:0.00083126  Bf:18  EN:0.0988  Loss: -26.410 4.215\n",
      "Ep: 2040  Rew:  -48.99  Avg Rew:  -49.39  LR:0.00083056  Bf:18  EN:0.0984  Loss: -26.107 4.335\n",
      "Ep: 2050  Rew:  227.53  Avg Rew:  -17.61  LR:0.00082988  Bf:18  EN:0.0980  Loss: -25.937 4.413\n",
      "Ep: 2060  Rew: -100.60  Avg Rew:  -14.42  LR:0.00082919  Bf:18  EN:0.0977  Loss: -25.852 4.839\n",
      "Ep: 2070  Rew: -106.68  Avg Rew:  -43.30  LR:0.00082850  Bf:18  EN:0.0973  Loss: -25.786 3.966\n",
      "Ep: 2080  Rew: -101.59  Avg Rew:  -68.91  LR:0.00082781  Bf:18  EN:0.0969  Loss: -25.653 4.555\n",
      "Ep: 2090  Rew: -111.41  Avg Rew:  -76.04  LR:0.00082713  Bf:18  EN:0.0965  Loss: -25.543 4.556\n",
      "Ep: 2100  Rew: -110.63  Avg Rew:  -85.40  LR:0.00082645  Bf:18  EN:0.0962  Loss: -25.448 4.539\n",
      "Ep: 2110  Rew:  132.22  Avg Rew:  -80.28  LR:0.00082576  Bf:18  EN:0.0958  Loss: -25.617 4.914\n",
      "Ep: 2120  Rew: -111.09  Avg Rew:  -84.17  LR:0.00082508  Bf:18  EN:0.0954  Loss: -25.656 5.144\n",
      "Ep: 2130  Rew:  -97.52  Avg Rew:  -71.33  LR:0.00082440  Bf:19  EN:0.0951  Loss: -25.786 4.710\n",
      "Ep: 2140  Rew: -116.79  Avg Rew:  -71.30  LR:0.00082372  Bf:19  EN:0.0947  Loss: -25.498 3.983\n",
      "Ep: 2150  Rew: -121.33  Avg Rew:  -98.62  LR:0.00082305  Bf:19  EN:0.0943  Loss: -25.543 4.407\n",
      "Ep: 2160  Rew: -109.82  Avg Rew:  -90.71  LR:0.00082237  Bf:19  EN:0.0940  Loss: -25.518 5.036\n",
      "Ep: 2170  Rew:  -99.93  Avg Rew:  -75.96  LR:0.00082169  Bf:19  EN:0.0936  Loss: -25.325 4.828\n",
      "Ep: 2180  Rew: -111.01  Avg Rew:  -59.30  LR:0.00082102  Bf:19  EN:0.0933  Loss: -25.358 4.586\n",
      "Ep: 2190  Rew:  -22.19  Avg Rew:  -69.43  LR:0.00082034  Bf:19  EN:0.0929  Loss: -25.246 5.270\n",
      "Ep: 2200  Rew:  -39.60  Avg Rew:  -77.64  LR:0.00081967  Bf:19  EN:0.0926  Loss: -24.937 4.613\n",
      "Ep: 2210  Rew: -103.53  Avg Rew:  -75.37  LR:0.00081900  Bf:19  EN:0.0923  Loss: -24.856 4.567\n",
      "Ep: 2220  Rew:  260.26  Avg Rew:  -65.61  LR:0.00081833  Bf:19  EN:0.0919  Loss: -24.702 5.319\n",
      "Ep: 2230  Rew:  -49.86  Avg Rew:  -30.53  LR:0.00081766  Bf:19  EN:0.0916  Loss: -24.673 5.012\n",
      "Ep: 2240  Rew: -112.02  Avg Rew:  -14.61  LR:0.00081699  Bf:19  EN:0.0912  Loss: -24.785 4.887\n",
      "Ep: 2250  Rew: -113.01  Avg Rew:  -39.55  LR:0.00081633  Bf:19  EN:0.0909  Loss: -25.052 5.145\n",
      "Ep: 2260  Rew: -143.48  Avg Rew:  -76.86  LR:0.00081566  Bf:19  EN:0.0906  Loss: -25.910 5.914\n",
      "Ep: 2270  Rew:   32.22  Avg Rew:  -54.58  LR:0.00081500  Bf:20  EN:0.0903  Loss: -26.720 5.856\n",
      "Ep: 2280  Rew:  -53.35  Avg Rew:   18.72  LR:0.00081433  Bf:20  EN:0.0899  Loss: -27.505 6.401\n",
      "Ep: 2290  Rew:  -36.91  Avg Rew:   72.34  LR:0.00081367  Bf:20  EN:0.0896  Loss: -28.447 6.609\n",
      "Ep: 2300  Rew: -138.00  Avg Rew:  107.99  LR:0.00081301  Bf:20  EN:0.0893  Loss: -29.480 6.762\n",
      "Ep: 2310  Rew: -122.29  Avg Rew:   40.86  LR:0.00081235  Bf:20  EN:0.0890  Loss: -30.271 7.050\n",
      "Ep: 2320  Rew: -114.91  Avg Rew:   49.53  LR:0.00081169  Bf:20  EN:0.0887  Loss: -31.568 6.775\n",
      "Ep: 2330  Rew: -121.92  Avg Rew:   38.08  LR:0.00081103  Bf:20  EN:0.0883  Loss: -32.197 7.265\n",
      "Ep: 2340  Rew: -117.51  Avg Rew:  -10.21  LR:0.00081037  Bf:21  EN:0.0880  Loss: -32.766 6.272\n",
      "Ep: 2350  Rew:  -77.68  Avg Rew:  -69.12  LR:0.00080972  Bf:21  EN:0.0877  Loss: -32.665 6.496\n",
      "Ep: 2360  Rew:  -23.76  Avg Rew:  -66.24  LR:0.00080906  Bf:21  EN:0.0874  Loss: -33.508 6.579\n",
      "Ep: 2370  Rew: -116.14  Avg Rew:  -71.13  LR:0.00080841  Bf:21  EN:0.0871  Loss: -33.818 6.666\n",
      "Ep: 2380  Rew: -114.80  Avg Rew:  -73.36  LR:0.00080775  Bf:21  EN:0.0868  Loss: -34.319 6.848\n",
      "Ep: 2390  Rew: -123.10  Avg Rew:  -83.26  LR:0.00080710  Bf:21  EN:0.0865  Loss: -34.844 6.562\n",
      "Ep: 2400  Rew: -117.11  Avg Rew:  -96.96  LR:0.00080645  Bf:21  EN:0.0862  Loss: -35.174 6.878\n",
      "Ep: 2410  Rew:   18.70  Avg Rew: -110.64  LR:0.00080580  Bf:21  EN:0.0859  Loss: -35.483 6.812\n",
      "Ep: 2420  Rew: -146.79  Avg Rew: -111.03  LR:0.00080515  Bf:21  EN:0.0856  Loss: -36.790 7.109\n",
      "Ep: 2430  Rew: -133.98  Avg Rew:  -82.59  LR:0.00080451  Bf:21  EN:0.0853  Loss: -38.257 7.020\n",
      "Ep: 2440  Rew:  -85.62  Avg Rew:  -79.39  LR:0.00080386  Bf:21  EN:0.0850  Loss: -39.275 7.786\n",
      "Ep: 2450  Rew: -106.31  Avg Rew:  -91.27  LR:0.00080321  Bf:22  EN:0.0847  Loss: -40.295 7.460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 2460  Rew: -133.70  Avg Rew: -115.24  LR:0.00080257  Bf:22  EN:0.0845  Loss: -41.645 7.083\n",
      "Ep: 2470  Rew: -125.67  Avg Rew: -137.83  LR:0.00080192  Bf:22  EN:0.0842  Loss: -40.913 6.168\n",
      "Ep: 2480  Rew: -139.39  Avg Rew: -123.33  LR:0.00080128  Bf:23  EN:0.0839  Loss: -39.959 6.819\n",
      "Ep: 2490  Rew:  174.41  Avg Rew: -105.87  LR:0.00080064  Bf:23  EN:0.0836  Loss: -38.348 6.758\n",
      "Ep: 2500  Rew:  230.81  Avg Rew:  -38.49  LR:0.00080000  Bf:23  EN:0.0833  Loss: -37.206 6.428\n",
      "Ep: 2510  Rew:  226.79  Avg Rew:   69.89  LR:0.00079936  Bf:23  EN:0.0831  Loss: -36.488 6.829\n",
      "Ep: 2520  Rew:  -37.62  Avg Rew:  102.42  LR:0.00079872  Bf:23  EN:0.0828  Loss: -35.987 5.963\n",
      "Ep: 2530  Rew: -100.76  Avg Rew:   41.93  LR:0.00079808  Bf:24  EN:0.0825  Loss: -35.643 5.394\n",
      "Ep: 2540  Rew: -112.71  Avg Rew:   -0.23  LR:0.00079745  Bf:24  EN:0.0822  Loss: -35.182 6.156\n",
      "Ep: 2550  Rew:  -97.23  Avg Rew:  -48.72  LR:0.00079681  Bf:24  EN:0.0820  Loss: -34.969 6.050\n",
      "Ep: 2560  Rew:  -18.68  Avg Rew:  -37.75  LR:0.00079618  Bf:24  EN:0.0817  Loss: -34.664 5.882\n",
      "Ep: 2570  Rew:   52.23  Avg Rew:    9.28  LR:0.00079554  Bf:24  EN:0.0814  Loss: -34.632 6.309\n",
      "Ep: 2580  Rew:  -98.05  Avg Rew:   10.86  LR:0.00079491  Bf:24  EN:0.0812  Loss: -34.652 5.983\n",
      "Ep: 2590  Rew: -143.06  Avg Rew:  -35.18  LR:0.00079428  Bf:24  EN:0.0809  Loss: -34.485 7.010\n",
      "Ep: 2600  Rew: -200.57  Avg Rew: -103.51  LR:0.00079365  Bf:25  EN:0.0806  Loss: -34.715 6.007\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-fd1de36530df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                    \u001b[0mpolyak\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolyak\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                    max_buffer_length=max_buffer_length, log_interval=log_interval)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-ed822e47c4c9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0;31m# Updating policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolyak\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/gym/bipedal_walker/DDPG/ddpg.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, replay_buffer, batch_size, gamma, polyak)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mQ_current\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_current\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   1714\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1715\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1716\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pointwise_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_pointwise_loss\u001b[0;34m(lambd, lambd_optimized, input, target, reduction)\u001b[0m\n\u001b[1;32m   1672\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'elementwise_mean'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1673\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1674\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlambd_optimized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "agent = DDPGTrainer(env_name, actor_config, critic_config, random_seed=random_seed, lr_base=lr_base, lr_decay=lr_decay, \n",
    "                   exp_noise_base=exp_noise_base, exp_noise_decay=exp_noise_decay, gamma=gamma, batch_size=batch_size,\n",
    "                   polyak=polyak, max_episodes=max_episodes, max_timesteps=max_timesteps, \n",
    "                   max_buffer_length=max_buffer_length, log_interval=log_interval)\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "agent.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
