{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import gym\n",
    "import numpy as np\n",
    "from gym import wrappers\n",
    "from PIL import Image\n",
    "\n",
    "from TD3.td3 import TD3\n",
    "from TD3.utils import PrioritizedReplayBuffer, mkdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'BipedalWalkerHardcore-v2'\n",
    "lr_base = 0.001\n",
    "lr_decay = 0.00005\n",
    "exp_noise_base = 0.3 \n",
    "exp_noise_decay = 0.0001\n",
    "\n",
    "random_seed = 42\n",
    "gamma = 0.99                # discount for future rewards\n",
    "batch_size = 256        # num of transitions sampled from replay buffer\n",
    "polyak = 0.9999              # target policy update parameter (1-tau)\n",
    "policy_noise = 0.2          # target policy smoothing noise\n",
    "noise_clip = 0.5\n",
    "policy_delay = 2            # delayed policy updates parameter\n",
    "max_episodes = 100000         # max num of episodes\n",
    "max_timesteps = 5000        # max timesteps in one episode\n",
    "max_buffer_length = 5000000\n",
    "log_interval = 10           # print avg reward after interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_config = [\n",
    "        {'dim': [None, 256], 'dropout': False, 'activation': 'relu'},\n",
    "        {'dim': [256, 256], 'dropout': True, 'activation':'relu'},\n",
    "        {'dim': [256, 128], 'dropout': False, 'activation': 'relu'},       \n",
    "        {'dim': [128, None],'dropout': False, 'activation': 'tanh'}\n",
    "    ]\n",
    "    \n",
    "critic_config = [\n",
    "        {'dim': [None, 512], 'dropout': False, 'activation': 'relu'},\n",
    "        {'dim': [512, 512], 'dropout': False , 'activation':'relu'},\n",
    "        {'dim': [512, 128], 'dropout': False, 'activation': 'relu'},       \n",
    "        {'dim': [128, 1], 'dropout': False, 'activation': False},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3Trainer():\n",
    "    \n",
    "    def __init__(self, env_name, actor_config, critic_config, random_seed=42, lr_base=0.001, lr_decay=0.00005, \n",
    "                 exp_noise_base=0.3, exp_noise_decay=0.0001, gamma=0.99, batch_size=1024, \n",
    "                 polyak=0.9999, policy_noise=0.2, noise_clip=0.5, policy_delay=2, \n",
    "                 max_episodes=100000, max_timesteps=3000, max_buffer_length=5000000, \n",
    "                 log_interval=5, threshold=None, lr_minimum=1e-10, exp_noise_minimum=1e-10,\n",
    "                 record_videos=True, record_interval=100, beta_multiplier=0.0001):        \n",
    "        \n",
    "        self.algorithm_name = 'td3'\n",
    "        self.env_name = env_name\n",
    "        self.env = gym.make(env_name)\n",
    "        self.record_videos = record_videos\n",
    "        self.record_interval = record_interval        \n",
    "        if self.record_videos == True:\n",
    "            videos_dir = mkdir('.', 'videos')\n",
    "            monitor_dir = mkdir(videos_dir, self.algorithm_name)\n",
    "            should_record = lambda i: self.should_record\n",
    "            self.env = wrappers.Monitor(self.env, monitor_dir, video_callable=should_record, force=True)            \n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.shape[0]\n",
    "        self.action_low = self.env.action_space.low\n",
    "        self.action_high = self.env.action_space.high        \n",
    "        self.should_record = False\n",
    "        if not threshold == None:\n",
    "            self.threshold = threshold\n",
    "        else:    \n",
    "            self.threshold = self.env.spec.reward_threshold\n",
    "        \n",
    "        self.actor_config = actor_config\n",
    "        self.critic_config = critic_config\n",
    "        self.actor_config[0]['dim'][0] = self.state_dim\n",
    "        self.actor_config[-1]['dim'][1] = self.action_dim\n",
    "        self.critic_config[0]['dim'][0] = self.state_dim + self.action_dim\n",
    "        \n",
    "        self.actor_config = actor_config\n",
    "        self.critic_config = critic_config\n",
    "        self.random_seed = random_seed\n",
    "        self.lr_base = lr_base\n",
    "        self.lr_decay = lr_decay   \n",
    "        self.lr_minimum = lr_minimum\n",
    "        self.exp_noise_base = exp_noise_base\n",
    "        self.exp_noise_decay = exp_noise_decay     \n",
    "        self.exp_noise_minimum = exp_noise_minimum                \n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size        \n",
    "        self.polyak = polyak\n",
    "        self.beta_multiplier = beta_multiplier\n",
    "        self.policy_noise = policy_noise\n",
    "        self.noise_clip = noise_clip\n",
    "        self.policy_delay = policy_delay\n",
    "        self.max_episodes = max_episodes\n",
    "        self.max_timesteps = max_timesteps\n",
    "        self.max_buffer_length = max_buffer_length\n",
    "        self.log_interval = log_interval\n",
    "\n",
    "        \n",
    "        prdir = mkdir('.', 'preTrained')\n",
    "        self.directory = mkdir(prdir, self.algorithm_name)\n",
    "        self.filename = \"{}_{}_{}\".format(self.algorithm_name, self.env_name, self.random_seed)\n",
    "                \n",
    "        self.policy = TD3(self.actor_config, self.critic_config, self.action_low, self.action_high)   \n",
    "        #self.replay_buffer = ReplayBuffer(max_length=self.max_buffer_length)\n",
    "        self.replay_buffer = PrioritizedReplayBuffer(size=self.max_buffer_length, alpha=0.8)\n",
    "        \n",
    "        self.reward_history = []\n",
    "        self.make_plots = False       \n",
    "        \n",
    "        if self.random_seed:\n",
    "            print(\"Random Seed: {}\".format(self.random_seed))\n",
    "            self.env.seed(self.random_seed)\n",
    "            torch.manual_seed(self.random_seed)\n",
    "            np.random.seed(self.random_seed)\n",
    "        \n",
    "    def train(self):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        print(\"Training started ... \\n\")\n",
    "        print(\"action_space={}\".format(self.env.action_space))\n",
    "        print(\"obs_space={}\".format(self.env.observation_space))\n",
    "        print(\"threshold={}\".format(self.threshold))     \n",
    "        print(\"action_low={} action_high={} \\n\".format(self.action_low, self.action_high))         \n",
    "\n",
    "        # loading models\n",
    "        self.policy.load(self.directory, self.filename)\n",
    "                \n",
    "        # logging variables:        \n",
    "        log_f = open(\"train_{}.txt\".format(self.algorithm_name), \"w+\")\n",
    "\n",
    "        # training procedure:\n",
    "        for episode in range(1, self.max_episodes+1):\n",
    "            \n",
    "            # Only record video during evaluation, every n steps\n",
    "            if episode % self.record_interval == 0:\n",
    "                self.should_record = True\n",
    "            \n",
    "            ep_reward = 0.0\n",
    "            state = self.env.reset()\n",
    "            \n",
    "            # calculate params\n",
    "            exploration_noise = max(self.exp_noise_base / (1.0 + episode * self.exp_noise_decay), self.exp_noise_minimum)\n",
    "            learning_rate = max(self.lr_base / (1.0 + episode * self.lr_decay), self.lr_minimum)      \n",
    "            beta = min(episode * self.beta_multiplier, 1)\n",
    "            self.policy.set_optimizers(lr=learning_rate)\n",
    "\n",
    "            for t in range(self.max_timesteps):\n",
    "                \n",
    "                # select action and add exploration noise:\n",
    "                action = self.policy.select_action(state)               \n",
    "                action = action + np.random.normal(0, exploration_noise, size=self.action_dim)\n",
    "                action = action.clip(self.action_low, self.action_high)\n",
    "\n",
    "                # take action in env:\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                self.replay_buffer.add(state, action, reward, next_state, float(done))\n",
    "                state = next_state\n",
    "\n",
    "                ep_reward += reward\n",
    "\n",
    "                # if episode is done then update policy:\n",
    "                if done or t==(self.max_timesteps-1):\n",
    "                    self.policy.update(self.replay_buffer, t, self.batch_size, self.gamma, self.polyak, \n",
    "                                       self.policy_noise, self.noise_clip, self.policy_delay, beta)\n",
    "                    break\n",
    "\n",
    "            self.reward_history.append(ep_reward)\n",
    "            avg_reward = np.mean(self.reward_history[-100:]) \n",
    "\n",
    "            # logging updates:        \n",
    "            log_f.write('{},{}\\n'.format(episode, ep_reward))\n",
    "            log_f.flush()\n",
    "            \n",
    "            # Calculate polyak\n",
    "            #part = (env.spec.reward_threshold - avg_reward) / (env.spec.reward_threshold + 150)\n",
    "            #if part > 1:\n",
    "            #    part = 1\n",
    "            #polyak = polyak_int[0] + (1 - part) * (polyak_int[1] - polyak_int[0])     \n",
    "\n",
    "            # Calculate LR\n",
    "            #part = min((env.spec.reward_threshold - avg_reward) / (env.spec.reward_threshold + 150), 1)\n",
    "                        \n",
    "            avg_actor_loss = np.mean(self.policy.actor_loss_list[-100:])\n",
    "            avg_Q1_loss = np.mean(self.policy.Q1_loss_list[-100:])\n",
    "            avg_Q2_loss = np.mean(self.policy.Q2_loss_list[-100:])\n",
    "\n",
    "            # Truncate training history if we don't plan to plot it later\n",
    "            if not self.make_plots:\n",
    "                self.policy.truncate_loss_lists() \n",
    "                if len(self.reward_history) > 100:\n",
    "                    self.reward_history.pop(0)    \n",
    "\n",
    "            # Print avg reward every log interval:\n",
    "            if episode % self.log_interval == 0:            \n",
    "                self.policy.save(self.directory, self.filename)\n",
    "                print(\"Ep:{:5d}  Rew:{:8.2f}  Avg Rew:{:8.2f}  LR:{:8.8f}  Bf:{:2.0f} {:0.4f}  EN:{:0.4f}  Loss: {:5.3f} {:5.3f} {:5.3f}\".format(\n",
    "                    episode, ep_reward, avg_reward, learning_rate, self.replay_buffer.get_fill(), beta, \n",
    "                    exploration_noise, avg_actor_loss, avg_Q1_loss, avg_Q2_loss))\n",
    "                \n",
    "            self.should_record = False    \n",
    "                \n",
    "            # if avg reward > threshold then save and stop traning:\n",
    "            if avg_reward >= self.threshold and episode > 100: \n",
    "                print(\"Ep:{:5d}  Rew:{:8.2f}  Avg Rew:{:8.2f}  LR:{:8.8f}  Bf:{:2.0f}  EN:{:0.4f}  Loss: {:5.3f} {:5.3f} {:5.3f}\".format(\n",
    "                    episode, ep_reward, avg_reward, learning_rate, self.replay_buffer.get_fill(), beta,\n",
    "                    exploration_noise, avg_actor_loss, avg_Q1_loss, avg_Q2_loss))\n",
    "                print(\"########## Solved! ###########\")\n",
    "                name = self.filename + '_solved'\n",
    "                self.policy.save(self.directory, name)\n",
    "                log_f.close()\n",
    "                training_time = time.time() - start_time\n",
    "                print(\"Training time: {:6.2f} sec\".format(training_time))\n",
    "                break    \n",
    "       \n",
    "    def test(self, episodes=3, render=True, save_gif=True):   \n",
    "        \n",
    "        gifdir = mkdir('.','gif')\n",
    "        algdir = mkdir(gifdir, self.algorithm_name)\n",
    "\n",
    "        for episode in range(1, episodes+1):\n",
    "            ep_reward = 0.0\n",
    "            state = self.env.reset()\n",
    "            epdir = mkdir(algdir, str(episode))\n",
    "            \n",
    "            for t in range(self.max_timesteps):\n",
    "                action = self.policy.select_action(state)\n",
    "                state, reward, done, _ = self.env.step(action)\n",
    "                ep_reward += reward\n",
    "                \n",
    "                if save_gif:                                       \n",
    "                    img = self.env.render(mode = 'rgb_array')\n",
    "                    img = Image.fromarray(img)\n",
    "                    img.save('{}/{}.jpg'.format(epdir, t))\n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "\n",
    "            print('Test episode: {}\\tReward: {:4.2f}'.format(episode, ep_reward))           \n",
    "            self.env.close()        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTOR=Sequential(\n",
      "  (0): Linear(in_features=24, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (3): Dropout(p=0.2)\n",
      "  (4): ReLU()\n",
      "  (5): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (6): ReLU()\n",
      "  (7): Linear(in_features=128, out_features=4, bias=True)\n",
      "  (8): Tanh()\n",
      ")\n",
      "ACTOR=Sequential(\n",
      "  (0): Linear(in_features=24, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (3): Dropout(p=0.2)\n",
      "  (4): ReLU()\n",
      "  (5): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (6): ReLU()\n",
      "  (7): Linear(in_features=128, out_features=4, bias=True)\n",
      "  (8): Tanh()\n",
      ")\n",
      "CRITIC=Sequential(\n",
      "  (0): Linear(in_features=28, out_features=512, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "CRITIC=Sequential(\n",
      "  (0): Linear(in_features=28, out_features=512, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "CRITIC=Sequential(\n",
      "  (0): Linear(in_features=28, out_features=512, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "CRITIC=Sequential(\n",
      "  (0): Linear(in_features=28, out_features=512, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Random Seed: 42\n",
      "Training started ... \n",
      "\n",
      "action_space=Box(4,)\n",
      "obs_space=Box(24,)\n",
      "threshold=300\n",
      "action_low=[-1. -1. -1. -1.] action_high=[1. 1. 1. 1.] \n",
      "\n",
      "DIR=./preTrained/td3 NAME=td3_BipedalWalkerHardcore-v2_42\n",
      "Models loaded\n",
      "Ep:   10  Rew: -118.42  Avg Rew: -108.82  LR:0.00099950  Bf: 0 0.0010  EN:0.2997  Loss: 0.090 0.123 0.121\n",
      "Ep:   20  Rew: -182.50  Avg Rew: -125.00  LR:0.00099900  Bf: 0 0.0020  EN:0.2994  Loss: -0.099 0.713 0.456\n",
      "Ep:   30  Rew: -111.32  Avg Rew: -122.74  LR:0.00099850  Bf: 0 0.0030  EN:0.2991  Loss: -0.257 1.386 0.982\n",
      "Ep:   40  Rew: -113.09  Avg Rew: -121.15  LR:0.00099800  Bf: 1 0.0040  EN:0.2988  Loss: -0.135 0.588 0.811\n",
      "Ep:   50  Rew: -110.88  Avg Rew: -119.55  LR:0.00099751  Bf: 1 0.0050  EN:0.2985  Loss: -0.071 1.841 2.373\n",
      "Ep:   60  Rew: -111.15  Avg Rew: -119.22  LR:0.00099701  Bf: 1 0.0060  EN:0.2982  Loss: -0.067 4.784 5.467\n",
      "Ep:   70  Rew: -106.61  Avg Rew: -118.32  LR:0.00099651  Bf: 1 0.0070  EN:0.2979  Loss: -0.021 7.386 7.015\n",
      "Ep:   80  Rew: -108.95  Avg Rew: -117.76  LR:0.00099602  Bf: 1 0.0080  EN:0.2976  Loss: -0.110 3.216 2.209\n",
      "Ep:   90  Rew: -107.76  Avg Rew: -117.70  LR:0.00099552  Bf: 1 0.0090  EN:0.2973  Loss: -0.115 2.699 2.243\n",
      "Ep:  100  Rew: -111.07  Avg Rew: -116.97  LR:0.00099502  Bf: 1 0.0100  EN:0.2970  Loss: 0.047 4.913 4.214\n",
      "Ep:  110  Rew:  -93.40  Avg Rew: -121.30  LR:0.00099453  Bf: 1 0.0110  EN:0.2967  Loss: -0.593 3.119 2.997\n",
      "Ep:  120  Rew: -110.00  Avg Rew: -119.29  LR:0.00099404  Bf: 1 0.0120  EN:0.2964  Loss: -0.705 2.269 1.957\n",
      "Ep:  130  Rew: -151.32  Avg Rew: -122.98  LR:0.00099354  Bf: 1 0.0130  EN:0.2962  Loss: -0.684 1.069 1.034\n",
      "Ep:  140  Rew: -111.06  Avg Rew: -122.65  LR:0.00099305  Bf: 1 0.0140  EN:0.2959  Loss: -0.630 2.351 1.308\n",
      "Ep:  150  Rew: -133.34  Avg Rew: -122.59  LR:0.00099256  Bf: 1 0.0150  EN:0.2956  Loss: -0.598 1.420 0.923\n",
      "Ep:  160  Rew: -119.56  Avg Rew: -122.43  LR:0.00099206  Bf: 2 0.0160  EN:0.2953  Loss: -0.639 0.751 0.464\n",
      "Ep:  170  Rew: -106.69  Avg Rew: -123.69  LR:0.00099157  Bf: 2 0.0170  EN:0.2950  Loss: -0.616 0.347 0.464\n",
      "Ep:  180  Rew: -103.39  Avg Rew: -124.43  LR:0.00099108  Bf: 2 0.0180  EN:0.2947  Loss: -0.626 1.425 1.047\n",
      "Ep:  190  Rew: -103.61  Avg Rew: -123.46  LR:0.00099059  Bf: 2 0.0190  EN:0.2944  Loss: -0.706 0.910 0.460\n",
      "Ep:  200  Rew: -146.95  Avg Rew: -125.28  LR:0.00099010  Bf: 2 0.0200  EN:0.2941  Loss: -0.740 0.566 0.508\n",
      "Ep:  210  Rew: -100.08  Avg Rew: -123.01  LR:0.00098961  Bf: 3 0.0210  EN:0.2938  Loss: -0.599 0.775 0.478\n",
      "Ep:  220  Rew: -157.36  Avg Rew: -124.96  LR:0.00098912  Bf: 3 0.0220  EN:0.2935  Loss: -0.479 0.511 0.450\n",
      "Ep:  230  Rew: -102.53  Avg Rew: -121.40  LR:0.00098863  Bf: 3 0.0230  EN:0.2933  Loss: -0.394 0.448 0.433\n",
      "Ep:  240  Rew: -108.88  Avg Rew: -122.30  LR:0.00098814  Bf: 3 0.0240  EN:0.2930  Loss: -0.423 0.412 0.373\n",
      "Ep:  250  Rew: -154.24  Avg Rew: -124.26  LR:0.00098765  Bf: 3 0.0250  EN:0.2927  Loss: -0.343 0.248 0.443\n",
      "Ep:  260  Rew: -146.66  Avg Rew: -126.73  LR:0.00098717  Bf: 3 0.0260  EN:0.2924  Loss: -0.314 0.401 0.358\n",
      "Ep:  270  Rew: -125.18  Avg Rew: -128.27  LR:0.00098668  Bf: 4 0.0270  EN:0.2921  Loss: -0.121 0.745 0.924\n",
      "Ep:  280  Rew: -104.40  Avg Rew: -128.89  LR:0.00098619  Bf: 4 0.0280  EN:0.2918  Loss: -0.049 1.177 0.545\n",
      "Ep:  290  Rew: -136.16  Avg Rew: -131.36  LR:0.00098571  Bf: 4 0.0290  EN:0.2915  Loss: -0.120 1.230 1.189\n",
      "Ep:  300  Rew: -109.75  Avg Rew: -134.14  LR:0.00098522  Bf: 4 0.0300  EN:0.2913  Loss: -0.168 1.481 1.023\n",
      "Ep:  310  Rew: -136.57  Avg Rew: -134.07  LR:0.00098474  Bf: 4 0.0310  EN:0.2910  Loss: -0.151 0.761 0.788\n",
      "Ep:  320  Rew: -109.36  Avg Rew: -131.59  LR:0.00098425  Bf: 4 0.0320  EN:0.2907  Loss: -0.071 1.233 1.216\n",
      "Ep:  330  Rew: -112.33  Avg Rew: -132.16  LR:0.00098377  Bf: 4 0.0330  EN:0.2904  Loss: -0.063 0.542 0.576\n",
      "Ep:  340  Rew: -109.42  Avg Rew: -131.39  LR:0.00098328  Bf: 4 0.0340  EN:0.2901  Loss: -0.180 1.607 1.297\n",
      "Ep:  350  Rew: -115.97  Avg Rew: -129.50  LR:0.00098280  Bf: 4 0.0350  EN:0.2899  Loss: -0.137 0.870 0.799\n",
      "Ep:  360  Rew: -112.46  Avg Rew: -128.85  LR:0.00098232  Bf: 4 0.0360  EN:0.2896  Loss: -0.047 0.841 1.088\n",
      "Ep:  370  Rew: -122.87  Avg Rew: -126.74  LR:0.00098184  Bf: 4 0.0370  EN:0.2893  Loss: 0.001 1.165 1.110\n",
      "Ep:  380  Rew: -130.44  Avg Rew: -127.26  LR:0.00098135  Bf: 5 0.0380  EN:0.2890  Loss: 0.048 1.384 1.331\n",
      "Ep:  390  Rew: -110.96  Avg Rew: -126.14  LR:0.00098087  Bf: 5 0.0390  EN:0.2887  Loss: 0.092 1.025 1.112\n",
      "Ep:  400  Rew: -158.95  Avg Rew: -123.29  LR:0.00098039  Bf: 5 0.0400  EN:0.2885  Loss: 0.074 0.786 0.905\n",
      "Ep:  410  Rew: -132.22  Avg Rew: -123.23  LR:0.00097991  Bf: 5 0.0410  EN:0.2882  Loss: 0.171 0.822 0.997\n",
      "Ep:  420  Rew: -109.44  Avg Rew: -125.37  LR:0.00097943  Bf: 5 0.0420  EN:0.2879  Loss: 0.195 0.776 0.758\n",
      "Ep:  430  Rew: -105.13  Avg Rew: -125.13  LR:0.00097895  Bf: 5 0.0430  EN:0.2876  Loss: 0.235 0.808 0.935\n",
      "Ep:  440  Rew: -142.18  Avg Rew: -124.96  LR:0.00097847  Bf: 5 0.0440  EN:0.2874  Loss: 0.318 0.619 0.455\n",
      "Ep:  450  Rew: -105.76  Avg Rew: -124.54  LR:0.00097800  Bf: 5 0.0450  EN:0.2871  Loss: 0.458 0.585 0.541\n",
      "Ep:  460  Rew: -104.69  Avg Rew: -121.59  LR:0.00097752  Bf: 5 0.0460  EN:0.2868  Loss: 0.328 1.110 0.918\n",
      "Ep:  470  Rew: -102.28  Avg Rew: -120.88  LR:0.00097704  Bf: 5 0.0470  EN:0.2865  Loss: 0.284 1.117 1.090\n",
      "Ep:  480  Rew: -117.51  Avg Rew: -118.25  LR:0.00097656  Bf: 5 0.0480  EN:0.2863  Loss: 0.410 0.858 0.944\n",
      "Ep:  490  Rew: -104.61  Avg Rew: -117.76  LR:0.00097609  Bf: 5 0.0490  EN:0.2860  Loss: 0.496 1.034 0.777\n",
      "Ep:  500  Rew: -167.48  Avg Rew: -116.93  LR:0.00097561  Bf: 5 0.0500  EN:0.2857  Loss: 0.411 1.386 1.168\n",
      "Ep:  510  Rew: -134.13  Avg Rew: -117.21  LR:0.00097513  Bf: 6 0.0510  EN:0.2854  Loss: 0.492 0.605 0.887\n",
      "Ep:  520  Rew: -130.46  Avg Rew: -116.95  LR:0.00097466  Bf: 6 0.0520  EN:0.2852  Loss: 0.675 1.707 1.304\n",
      "Ep:  530  Rew: -117.02  Avg Rew: -118.01  LR:0.00097418  Bf: 6 0.0530  EN:0.2849  Loss: 0.591 0.709 0.753\n",
      "Ep:  540  Rew: -124.57  Avg Rew: -118.56  LR:0.00097371  Bf: 6 0.0540  EN:0.2846  Loss: 0.705 0.600 0.454\n",
      "Ep:  550  Rew:  -95.49  Avg Rew: -119.35  LR:0.00097324  Bf: 6 0.0550  EN:0.2844  Loss: 0.718 0.513 0.547\n",
      "Ep:  560  Rew: -102.16  Avg Rew: -120.52  LR:0.00097276  Bf: 6 0.0560  EN:0.2841  Loss: 0.853 0.713 0.628\n",
      "Ep:  570  Rew: -133.06  Avg Rew: -121.04  LR:0.00097229  Bf: 6 0.0570  EN:0.2838  Loss: 0.732 0.618 0.812\n",
      "Ep:  580  Rew: -117.35  Avg Rew: -121.50  LR:0.00097182  Bf: 6 0.0580  EN:0.2836  Loss: 0.942 0.668 0.689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep:  590  Rew: -113.99  Avg Rew: -121.04  LR:0.00097135  Bf: 7 0.0590  EN:0.2833  Loss: 0.866 0.871 0.996\n",
      "Ep:  600  Rew: -107.86  Avg Rew: -120.06  LR:0.00097087  Bf: 7 0.0600  EN:0.2830  Loss: 0.932 0.855 0.677\n",
      "Ep:  610  Rew: -106.14  Avg Rew: -117.68  LR:0.00097040  Bf: 7 0.0610  EN:0.2828  Loss: 1.045 0.820 0.993\n",
      "Ep:  620  Rew: -102.94  Avg Rew: -114.42  LR:0.00096993  Bf: 7 0.0620  EN:0.2825  Loss: 1.235 1.155 1.045\n",
      "Ep:  630  Rew: -115.91  Avg Rew: -111.27  LR:0.00096946  Bf: 7 0.0630  EN:0.2822  Loss: 1.084 0.909 0.860\n",
      "Ep:  640  Rew: -100.99  Avg Rew: -110.24  LR:0.00096899  Bf: 7 0.0640  EN:0.2820  Loss: 1.056 1.131 1.552\n",
      "Ep:  650  Rew: -101.04  Avg Rew: -108.59  LR:0.00096852  Bf: 7 0.0650  EN:0.2817  Loss: 1.127 0.697 0.714\n",
      "Ep:  660  Rew:  -98.34  Avg Rew: -107.27  LR:0.00096805  Bf: 7 0.0660  EN:0.2814  Loss: 1.191 0.747 0.712\n",
      "Ep:  670  Rew:  -98.67  Avg Rew: -105.73  LR:0.00096759  Bf: 7 0.0670  EN:0.2812  Loss: 1.209 0.772 0.787\n",
      "Ep:  680  Rew: -123.64  Avg Rew: -105.41  LR:0.00096712  Bf: 7 0.0680  EN:0.2809  Loss: 1.272 1.080 1.032\n",
      "Ep:  690  Rew: -104.44  Avg Rew: -104.68  LR:0.00096665  Bf: 7 0.0690  EN:0.2806  Loss: 1.384 0.968 0.825\n",
      "Ep:  700  Rew:  -93.82  Avg Rew: -103.98  LR:0.00096618  Bf: 7 0.0700  EN:0.2804  Loss: 1.359 1.241 1.539\n",
      "Ep:  710  Rew: -141.08  Avg Rew: -103.99  LR:0.00096572  Bf: 7 0.0710  EN:0.2801  Loss: 1.465 1.222 1.266\n",
      "Ep:  720  Rew: -113.44  Avg Rew: -104.40  LR:0.00096525  Bf: 7 0.0720  EN:0.2799  Loss: 1.758 0.984 1.107\n",
      "Ep:  730  Rew: -118.80  Avg Rew: -104.96  LR:0.00096479  Bf: 7 0.0730  EN:0.2796  Loss: 1.545 1.435 1.681\n",
      "Ep:  740  Rew: -108.58  Avg Rew: -104.67  LR:0.00096432  Bf: 7 0.0740  EN:0.2793  Loss: 1.570 0.950 0.970\n",
      "Ep:  750  Rew:  -98.40  Avg Rew: -104.90  LR:0.00096386  Bf: 7 0.0750  EN:0.2791  Loss: 1.584 1.057 0.769\n",
      "Ep:  760  Rew:  -97.18  Avg Rew: -105.38  LR:0.00096339  Bf: 7 0.0760  EN:0.2788  Loss: 1.744 0.800 1.048\n",
      "Ep:  770  Rew: -100.10  Avg Rew: -106.35  LR:0.00096293  Bf: 7 0.0770  EN:0.2786  Loss: 1.737 0.805 0.841\n",
      "Ep:  780  Rew:  -94.83  Avg Rew: -107.10  LR:0.00096246  Bf: 7 0.0780  EN:0.2783  Loss: 1.821 0.852 0.986\n",
      "Ep:  790  Rew: -149.91  Avg Rew: -107.73  LR:0.00096200  Bf: 7 0.0790  EN:0.2780  Loss: 1.802 0.901 1.239\n",
      "Ep:  800  Rew: -105.04  Avg Rew: -109.63  LR:0.00096154  Bf: 7 0.0800  EN:0.2778  Loss: 1.906 1.016 1.100\n",
      "Ep:  810  Rew:  -97.43  Avg Rew: -111.77  LR:0.00096108  Bf: 8 0.0810  EN:0.2775  Loss: 1.863 0.921 0.903\n",
      "Ep:  820  Rew: -119.78  Avg Rew: -112.30  LR:0.00096061  Bf: 8 0.0820  EN:0.2773  Loss: 1.989 1.054 1.068\n",
      "Ep:  830  Rew: -175.23  Avg Rew: -114.34  LR:0.00096015  Bf: 8 0.0830  EN:0.2770  Loss: 2.034 0.938 0.913\n",
      "Ep:  840  Rew:  -95.99  Avg Rew: -115.78  LR:0.00095969  Bf: 8 0.0840  EN:0.2768  Loss: 2.129 0.676 0.715\n",
      "Ep:  850  Rew: -123.33  Avg Rew: -117.21  LR:0.00095923  Bf: 8 0.0850  EN:0.2765  Loss: 1.954 0.833 0.820\n",
      "Ep:  860  Rew: -115.06  Avg Rew: -117.58  LR:0.00095877  Bf: 8 0.0860  EN:0.2762  Loss: 2.123 0.779 0.848\n",
      "Ep:  870  Rew: -115.92  Avg Rew: -118.12  LR:0.00095831  Bf: 9 0.0870  EN:0.2760  Loss: 2.177 0.791 0.766\n",
      "Ep:  880  Rew: -100.55  Avg Rew: -118.43  LR:0.00095785  Bf: 9 0.0880  EN:0.2757  Loss: 2.253 0.771 0.802\n",
      "Ep:  890  Rew: -119.08  Avg Rew: -118.51  LR:0.00095740  Bf: 9 0.0890  EN:0.2755  Loss: 2.325 0.833 1.208\n",
      "Ep:  900  Rew: -102.99  Avg Rew: -117.44  LR:0.00095694  Bf: 9 0.0900  EN:0.2752  Loss: 2.527 1.120 1.019\n",
      "Ep:  910  Rew: -121.65  Avg Rew: -115.65  LR:0.00095648  Bf: 9 0.0910  EN:0.2750  Loss: 2.487 0.907 0.911\n",
      "Ep:  920  Rew: -116.90  Avg Rew: -115.44  LR:0.00095602  Bf: 9 0.0920  EN:0.2747  Loss: 2.316 1.046 1.050\n",
      "Ep:  930  Rew: -100.11  Avg Rew: -113.43  LR:0.00095557  Bf: 9 0.0930  EN:0.2745  Loss: 2.570 0.890 0.886\n",
      "Ep:  940  Rew: -115.86  Avg Rew: -113.38  LR:0.00095511  Bf: 9 0.0940  EN:0.2742  Loss: 2.398 0.947 1.284\n",
      "Ep:  950  Rew: -102.38  Avg Rew: -112.98  LR:0.00095465  Bf: 9 0.0950  EN:0.2740  Loss: 2.472 1.002 1.254\n",
      "Ep:  960  Rew: -108.78  Avg Rew: -112.17  LR:0.00095420  Bf: 9 0.0960  EN:0.2737  Loss: 2.703 0.921 0.956\n",
      "Ep:  970  Rew:  -99.61  Avg Rew: -112.60  LR:0.00095374  Bf: 9 0.0970  EN:0.2735  Loss: 2.748 1.350 1.086\n",
      "Ep:  980  Rew: -142.66  Avg Rew: -112.81  LR:0.00095329  Bf: 9 0.0980  EN:0.2732  Loss: 2.741 1.122 1.103\n",
      "Ep:  990  Rew: -108.34  Avg Rew: -113.34  LR:0.00095283  Bf: 9 0.0990  EN:0.2730  Loss: 2.699 1.391 1.343\n",
      "Ep: 1000  Rew: -125.49  Avg Rew: -113.85  LR:0.00095238  Bf: 9 0.1000  EN:0.2727  Loss: 2.783 1.206 1.494\n",
      "Ep: 1010  Rew: -103.62  Avg Rew: -113.87  LR:0.00095193  Bf: 9 0.1010  EN:0.2725  Loss: 2.962 0.971 0.981\n",
      "Ep: 1020  Rew:  -96.79  Avg Rew: -113.85  LR:0.00095147  Bf:10 0.1020  EN:0.2722  Loss: 2.785 1.062 1.421\n",
      "Ep: 1030  Rew: -130.83  Avg Rew: -113.72  LR:0.00095102  Bf:10 0.1030  EN:0.2720  Loss: 2.905 0.884 0.911\n",
      "Ep: 1040  Rew: -145.54  Avg Rew: -112.65  LR:0.00095057  Bf:10 0.1040  EN:0.2717  Loss: 2.842 1.376 1.474\n",
      "Ep: 1050  Rew:  -94.59  Avg Rew: -112.74  LR:0.00095012  Bf:10 0.1050  EN:0.2715  Loss: 2.962 0.990 1.020\n",
      "Ep: 1060  Rew: -103.51  Avg Rew: -114.40  LR:0.00094967  Bf:10 0.1060  EN:0.2712  Loss: 2.871 0.985 1.035\n",
      "Ep: 1070  Rew: -107.45  Avg Rew: -112.59  LR:0.00094922  Bf:10 0.1070  EN:0.2710  Loss: 3.279 1.099 1.157\n",
      "Ep: 1080  Rew:  -95.17  Avg Rew: -110.70  LR:0.00094877  Bf:10 0.1080  EN:0.2708  Loss: 3.017 1.068 1.052\n",
      "Ep: 1090  Rew:  -93.06  Avg Rew: -111.12  LR:0.00094832  Bf:10 0.1090  EN:0.2705  Loss: 3.174 0.985 1.037\n",
      "Ep: 1100  Rew: -114.23  Avg Rew: -109.87  LR:0.00094787  Bf:10 0.1100  EN:0.2703  Loss: 3.336 1.046 1.091\n",
      "Ep: 1110  Rew: -108.99  Avg Rew: -110.02  LR:0.00094742  Bf:10 0.1110  EN:0.2700  Loss: 3.196 0.969 1.102\n",
      "Ep: 1120  Rew:  -99.03  Avg Rew: -111.46  LR:0.00094697  Bf:10 0.1120  EN:0.2698  Loss: 3.385 1.004 0.998\n",
      "Ep: 1130  Rew: -114.98  Avg Rew: -114.07  LR:0.00094652  Bf:11 0.1130  EN:0.2695  Loss: 3.334 1.156 1.167\n",
      "Ep: 1140  Rew: -105.37  Avg Rew: -117.04  LR:0.00094607  Bf:11 0.1140  EN:0.2693  Loss: 3.249 1.179 1.187\n",
      "Ep: 1150  Rew: -105.30  Avg Rew: -116.54  LR:0.00094563  Bf:11 0.1150  EN:0.2691  Loss: 3.506 1.051 1.046\n",
      "Ep: 1160  Rew:  -97.23  Avg Rew: -118.77  LR:0.00094518  Bf:11 0.1160  EN:0.2688  Loss: 3.320 1.172 1.265\n",
      "Ep: 1170  Rew: -101.15  Avg Rew: -119.86  LR:0.00094473  Bf:11 0.1170  EN:0.2686  Loss: 3.661 1.165 1.152\n",
      "Ep: 1180  Rew: -116.04  Avg Rew: -122.08  LR:0.00094429  Bf:11 0.1180  EN:0.2683  Loss: 3.632 1.076 1.244\n",
      "Ep: 1190  Rew: -113.45  Avg Rew: -120.65  LR:0.00094384  Bf:11 0.1190  EN:0.2681  Loss: 3.608 1.377 1.279\n",
      "Ep: 1200  Rew:  -99.06  Avg Rew: -121.50  LR:0.00094340  Bf:11 0.1200  EN:0.2679  Loss: 3.609 1.299 1.377\n",
      "Ep: 1210  Rew: -104.53  Avg Rew: -120.67  LR:0.00094295  Bf:11 0.1210  EN:0.2676  Loss: 3.571 1.201 1.113\n",
      "Ep: 1220  Rew:  -92.07  Avg Rew: -118.64  LR:0.00094251  Bf:12 0.1220  EN:0.2674  Loss: 3.772 1.445 1.495\n",
      "Ep: 1230  Rew:  -96.98  Avg Rew: -115.53  LR:0.00094206  Bf:12 0.1230  EN:0.2671  Loss: 3.571 1.102 1.218\n",
      "Ep: 1240  Rew: -111.91  Avg Rew: -111.79  LR:0.00094162  Bf:12 0.1240  EN:0.2669  Loss: 3.710 1.238 1.192\n",
      "Ep: 1250  Rew: -103.84  Avg Rew: -111.38  LR:0.00094118  Bf:12 0.1250  EN:0.2667  Loss: 3.864 1.587 1.415\n",
      "Ep: 1260  Rew: -107.57  Avg Rew: -107.65  LR:0.00094073  Bf:12 0.1260  EN:0.2664  Loss: 4.007 1.184 1.282\n",
      "Ep: 1270  Rew: -101.87  Avg Rew: -106.14  LR:0.00094029  Bf:12 0.1270  EN:0.2662  Loss: 3.766 1.241 1.224\n",
      "Ep: 1280  Rew: -121.41  Avg Rew: -104.89  LR:0.00093985  Bf:12 0.1280  EN:0.2660  Loss: 3.832 1.877 1.874\n",
      "Ep: 1290  Rew: -125.52  Avg Rew: -105.87  LR:0.00093941  Bf:12 0.1290  EN:0.2657  Loss: 4.039 1.258 1.191\n",
      "Ep: 1300  Rew: -103.15  Avg Rew: -106.58  LR:0.00093897  Bf:12 0.1300  EN:0.2655  Loss: 3.985 1.340 1.315\n",
      "Ep: 1310  Rew: -121.46  Avg Rew: -109.45  LR:0.00093853  Bf:13 0.1310  EN:0.2653  Loss: 4.083 1.431 1.519\n",
      "Ep: 1320  Rew: -111.90  Avg Rew: -110.85  LR:0.00093809  Bf:13 0.1320  EN:0.2650  Loss: 3.936 1.412 1.305\n",
      "Ep: 1330  Rew:  -76.45  Avg Rew: -112.86  LR:0.00093765  Bf:13 0.1330  EN:0.2648  Loss: 4.195 1.347 1.415\n",
      "Ep: 1340  Rew: -106.82  Avg Rew: -114.46  LR:0.00093721  Bf:13 0.1340  EN:0.2646  Loss: 4.212 1.524 1.437\n",
      "Ep: 1350  Rew: -131.56  Avg Rew: -116.65  LR:0.00093677  Bf:14 0.1350  EN:0.2643  Loss: 4.271 1.672 1.851\n",
      "Ep: 1360  Rew: -120.50  Avg Rew: -117.86  LR:0.00093633  Bf:14 0.1360  EN:0.2641  Loss: 4.408 1.490 1.466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 1370  Rew:  -99.82  Avg Rew: -119.09  LR:0.00093589  Bf:14 0.1370  EN:0.2639  Loss: 4.629 1.421 1.368\n",
      "Ep: 1380  Rew: -120.32  Avg Rew: -118.51  LR:0.00093545  Bf:14 0.1380  EN:0.2636  Loss: 4.304 1.441 1.475\n",
      "Ep: 1390  Rew: -186.61  Avg Rew: -119.39  LR:0.00093502  Bf:14 0.1390  EN:0.2634  Loss: 4.503 1.464 1.338\n",
      "Ep: 1400  Rew: -116.55  Avg Rew: -118.05  LR:0.00093458  Bf:15 0.1400  EN:0.2632  Loss: 4.299 1.449 1.468\n",
      "Ep: 1410  Rew: -101.77  Avg Rew: -115.18  LR:0.00093414  Bf:15 0.1410  EN:0.2629  Loss: 4.361 2.015 2.159\n",
      "Ep: 1420  Rew: -138.89  Avg Rew: -115.50  LR:0.00093371  Bf:15 0.1420  EN:0.2627  Loss: 4.521 1.591 1.597\n",
      "Ep: 1430  Rew: -113.52  Avg Rew: -115.02  LR:0.00093327  Bf:15 0.1430  EN:0.2625  Loss: 4.436 1.594 1.590\n",
      "Ep: 1440  Rew: -110.99  Avg Rew: -115.44  LR:0.00093284  Bf:15 0.1440  EN:0.2622  Loss: 4.642 1.694 1.719\n",
      "Ep: 1450  Rew: -102.73  Avg Rew: -115.84  LR:0.00093240  Bf:16 0.1450  EN:0.2620  Loss: 4.654 1.191 1.406\n",
      "Ep: 1460  Rew: -110.41  Avg Rew: -116.13  LR:0.00093197  Bf:16 0.1460  EN:0.2618  Loss: 4.793 1.582 1.639\n",
      "Ep: 1470  Rew: -121.90  Avg Rew: -116.91  LR:0.00093153  Bf:16 0.1470  EN:0.2616  Loss: 4.472 1.438 1.551\n",
      "Ep: 1480  Rew:  -82.49  Avg Rew: -118.32  LR:0.00093110  Bf:16 0.1480  EN:0.2613  Loss: 4.648 1.474 1.558\n",
      "Ep: 1490  Rew:  -78.75  Avg Rew: -115.99  LR:0.00093067  Bf:16 0.1490  EN:0.2611  Loss: 4.665 1.542 1.451\n",
      "Ep: 1500  Rew:  -94.56  Avg Rew: -115.59  LR:0.00093023  Bf:17 0.1500  EN:0.2609  Loss: 4.718 1.436 1.451\n",
      "Ep: 1510  Rew:  -98.16  Avg Rew: -115.96  LR:0.00092980  Bf:17 0.1510  EN:0.2606  Loss: 4.565 1.621 1.980\n",
      "Ep: 1520  Rew: -140.19  Avg Rew: -114.83  LR:0.00092937  Bf:17 0.1520  EN:0.2604  Loss: 4.814 1.747 1.637\n",
      "Ep: 1530  Rew: -134.98  Avg Rew: -113.96  LR:0.00092894  Bf:17 0.1530  EN:0.2602  Loss: 4.815 1.756 1.737\n",
      "Ep: 1540  Rew:  -66.45  Avg Rew: -112.83  LR:0.00092851  Bf:17 0.1540  EN:0.2600  Loss: 4.832 1.726 1.809\n",
      "Ep: 1550  Rew: -138.39  Avg Rew: -111.09  LR:0.00092807  Bf:17 0.1550  EN:0.2597  Loss: 4.929 1.624 1.670\n",
      "Ep: 1560  Rew:  -75.08  Avg Rew: -109.82  LR:0.00092764  Bf:18 0.1560  EN:0.2595  Loss: 4.932 1.817 2.043\n",
      "Ep: 1570  Rew: -112.91  Avg Rew: -106.67  LR:0.00092721  Bf:18 0.1570  EN:0.2593  Loss: 4.895 1.758 1.749\n",
      "Ep: 1580  Rew:  -83.37  Avg Rew: -104.84  LR:0.00092678  Bf:18 0.1580  EN:0.2591  Loss: 4.845 1.826 1.849\n",
      "Ep: 1590  Rew:  -93.46  Avg Rew: -104.68  LR:0.00092635  Bf:18 0.1590  EN:0.2588  Loss: 4.877 1.649 1.684\n",
      "Ep: 1600  Rew: -108.17  Avg Rew: -102.75  LR:0.00092593  Bf:18 0.1600  EN:0.2586  Loss: 5.120 1.811 1.872\n",
      "Ep: 1610  Rew:  -83.93  Avg Rew: -100.88  LR:0.00092550  Bf:18 0.1610  EN:0.2584  Loss: 5.281 1.710 1.750\n",
      "Ep: 1620  Rew: -112.86  Avg Rew:  -99.01  LR:0.00092507  Bf:18 0.1620  EN:0.2582  Loss: 5.153 1.874 2.030\n",
      "Ep: 1630  Rew: -110.95  Avg Rew:  -98.13  LR:0.00092464  Bf:18 0.1630  EN:0.2580  Loss: 5.151 1.778 1.602\n",
      "Ep: 1640  Rew: -100.10  Avg Rew:  -96.55  LR:0.00092421  Bf:19 0.1640  EN:0.2577  Loss: 5.238 1.791 1.658\n",
      "Ep: 1650  Rew:  -98.57  Avg Rew:  -93.49  LR:0.00092379  Bf:19 0.1650  EN:0.2575  Loss: 5.316 1.709 1.772\n",
      "Ep: 1660  Rew:  -91.92  Avg Rew:  -92.59  LR:0.00092336  Bf:19 0.1660  EN:0.2573  Loss: 5.206 1.780 1.908\n",
      "Ep: 1670  Rew:  -85.60  Avg Rew:  -93.67  LR:0.00092293  Bf:19 0.1670  EN:0.2571  Loss: 5.234 1.736 1.848\n",
      "Ep: 1680  Rew:  -99.69  Avg Rew:  -93.11  LR:0.00092251  Bf:19 0.1680  EN:0.2568  Loss: 5.444 1.859 1.870\n",
      "Ep: 1690  Rew: -104.45  Avg Rew:  -92.68  LR:0.00092208  Bf:19 0.1690  EN:0.2566  Loss: 5.644 2.078 2.221\n",
      "Ep: 1700  Rew:  -92.19  Avg Rew:  -94.70  LR:0.00092166  Bf:19 0.1700  EN:0.2564  Loss: 5.567 1.696 1.819\n",
      "Ep: 1710  Rew:  -93.22  Avg Rew:  -94.60  LR:0.00092123  Bf:20 0.1710  EN:0.2562  Loss: 5.855 1.841 1.801\n",
      "Ep: 1720  Rew: -118.24  Avg Rew:  -97.20  LR:0.00092081  Bf:20 0.1720  EN:0.2560  Loss: 5.677 2.099 1.984\n",
      "Ep: 1730  Rew: -108.33  Avg Rew:  -98.71  LR:0.00092039  Bf:20 0.1730  EN:0.2558  Loss: 5.683 1.884 1.970\n",
      "Ep: 1740  Rew:  -83.13  Avg Rew:  -97.86  LR:0.00091996  Bf:20 0.1740  EN:0.2555  Loss: 5.933 1.807 1.701\n",
      "Ep: 1750  Rew:  -84.98  Avg Rew:  -99.80  LR:0.00091954  Bf:20 0.1750  EN:0.2553  Loss: 6.016 1.985 1.924\n",
      "Ep: 1760  Rew:  -64.47  Avg Rew:  -99.99  LR:0.00091912  Bf:20 0.1760  EN:0.2551  Loss: 6.192 1.665 1.843\n",
      "Ep: 1770  Rew:  -58.14  Avg Rew: -100.45  LR:0.00091870  Bf:21 0.1770  EN:0.2549  Loss: 6.528 2.076 1.991\n",
      "Ep: 1780  Rew: -100.99  Avg Rew: -100.87  LR:0.00091827  Bf:21 0.1780  EN:0.2547  Loss: 6.808 1.812 1.851\n",
      "Ep: 1790  Rew:  -82.65  Avg Rew: -100.16  LR:0.00091785  Bf:21 0.1790  EN:0.2545  Loss: 7.320 2.084 2.006\n",
      "Ep: 1800  Rew: -105.22  Avg Rew:  -99.77  LR:0.00091743  Bf:21 0.1800  EN:0.2542  Loss: 7.418 2.009 1.973\n",
      "Ep: 1810  Rew:  -87.38  Avg Rew: -101.22  LR:0.00091701  Bf:21 0.1810  EN:0.2540  Loss: 7.576 1.935 1.957\n",
      "Ep: 1820  Rew:  -90.77  Avg Rew:  -99.16  LR:0.00091659  Bf:22 0.1820  EN:0.2538  Loss: 7.580 2.023 2.183\n",
      "Ep: 1830  Rew:  -80.53  Avg Rew:  -98.22  LR:0.00091617  Bf:22 0.1830  EN:0.2536  Loss: 7.519 2.079 1.997\n",
      "Ep: 1840  Rew: -102.45  Avg Rew:  -99.78  LR:0.00091575  Bf:22 0.1840  EN:0.2534  Loss: 7.742 2.120 2.284\n",
      "Ep: 1850  Rew: -120.70  Avg Rew: -101.27  LR:0.00091533  Bf:22 0.1850  EN:0.2532  Loss: 7.534 1.906 1.893\n",
      "Ep: 1860  Rew: -103.02  Avg Rew: -102.69  LR:0.00091491  Bf:23 0.1860  EN:0.2530  Loss: 7.818 1.900 1.844\n",
      "Ep: 1870  Rew: -133.07  Avg Rew: -102.95  LR:0.00091449  Bf:23 0.1870  EN:0.2527  Loss: 7.913 1.954 1.934\n",
      "Ep: 1880  Rew:  -96.88  Avg Rew: -103.44  LR:0.00091408  Bf:23 0.1880  EN:0.2525  Loss: 7.847 2.144 1.982\n",
      "Ep: 1890  Rew: -116.41  Avg Rew: -106.16  LR:0.00091366  Bf:23 0.1890  EN:0.2523  Loss: 7.677 2.015 1.977\n",
      "Ep: 1900  Rew: -125.07  Avg Rew: -108.96  LR:0.00091324  Bf:23 0.1900  EN:0.2521  Loss: 7.815 1.856 1.909\n",
      "Ep: 1910  Rew:  -95.93  Avg Rew: -111.20  LR:0.00091283  Bf:24 0.1910  EN:0.2519  Loss: 7.828 1.851 1.936\n",
      "Ep: 1920  Rew: -100.73  Avg Rew: -111.10  LR:0.00091241  Bf:24 0.1920  EN:0.2517  Loss: 7.932 1.879 1.862\n",
      "Ep: 1930  Rew:  -41.84  Avg Rew: -109.91  LR:0.00091199  Bf:24 0.1930  EN:0.2515  Loss: 7.935 2.166 2.228\n",
      "Ep: 1940  Rew: -115.97  Avg Rew: -110.81  LR:0.00091158  Bf:24 0.1940  EN:0.2513  Loss: 8.034 2.231 2.175\n",
      "Ep: 1950  Rew: -100.08  Avg Rew: -109.49  LR:0.00091116  Bf:24 0.1950  EN:0.2510  Loss: 8.273 1.685 1.662\n",
      "Ep: 1960  Rew: -110.52  Avg Rew: -108.34  LR:0.00091075  Bf:24 0.1960  EN:0.2508  Loss: 8.425 1.840 1.822\n",
      "Ep: 1970  Rew: -111.40  Avg Rew: -107.51  LR:0.00091033  Bf:25 0.1970  EN:0.2506  Loss: 8.406 2.227 2.387\n",
      "Ep: 1980  Rew: -116.01  Avg Rew: -107.81  LR:0.00090992  Bf:25 0.1980  EN:0.2504  Loss: 8.507 1.829 2.142\n",
      "Ep: 1990  Rew:  -81.47  Avg Rew: -106.43  LR:0.00090950  Bf:25 0.1990  EN:0.2502  Loss: 8.554 2.165 2.109\n",
      "Ep: 2000  Rew:  -47.27  Avg Rew: -102.66  LR:0.00090909  Bf:25 0.2000  EN:0.2500  Loss: 8.515 2.159 2.277\n",
      "Ep: 2010  Rew: -110.71  Avg Rew: -100.65  LR:0.00090868  Bf:25 0.2010  EN:0.2498  Loss: 8.417 1.979 1.876\n",
      "Ep: 2020  Rew: -107.00  Avg Rew: -100.76  LR:0.00090827  Bf:26 0.2020  EN:0.2496  Loss: 8.588 2.149 2.038\n",
      "Ep: 2030  Rew: -105.97  Avg Rew: -101.28  LR:0.00090785  Bf:26 0.2030  EN:0.2494  Loss: 8.726 2.124 2.125\n",
      "Ep: 2040  Rew: -152.31  Avg Rew: -101.34  LR:0.00090744  Bf:26 0.2040  EN:0.2492  Loss: 8.479 2.216 1.986\n",
      "Ep: 2050  Rew: -102.94  Avg Rew: -101.17  LR:0.00090703  Bf:26 0.2050  EN:0.2490  Loss: 8.772 2.329 2.716\n",
      "Ep: 2060  Rew: -122.73  Avg Rew: -101.63  LR:0.00090662  Bf:26 0.2060  EN:0.2488  Loss: 8.819 2.235 2.326\n",
      "Ep: 2070  Rew: -122.02  Avg Rew: -102.46  LR:0.00090621  Bf:26 0.2070  EN:0.2486  Loss: 8.694 2.265 2.327\n",
      "Ep: 2080  Rew: -106.47  Avg Rew: -102.84  LR:0.00090580  Bf:27 0.2080  EN:0.2483  Loss: 8.726 2.138 2.031\n",
      "Ep: 2090  Rew: -110.37  Avg Rew: -102.70  LR:0.00090539  Bf:27 0.2090  EN:0.2481  Loss: 8.852 2.470 2.507\n",
      "Ep: 2100  Rew: -112.27  Avg Rew: -104.91  LR:0.00090498  Bf:27 0.2100  EN:0.2479  Loss: 8.910 2.462 2.531\n",
      "Ep: 2110  Rew:  -88.59  Avg Rew: -102.66  LR:0.00090457  Bf:27 0.2110  EN:0.2477  Loss: 8.874 2.406 2.484\n",
      "Ep: 2120  Rew: -206.71  Avg Rew: -103.81  LR:0.00090416  Bf:28 0.2120  EN:0.2475  Loss: 8.972 2.094 2.112\n",
      "Ep: 2130  Rew:  -74.48  Avg Rew: -104.11  LR:0.00090375  Bf:28 0.2130  EN:0.2473  Loss: 9.049 2.084 2.308\n",
      "Ep: 2140  Rew:   42.25  Avg Rew: -100.64  LR:0.00090334  Bf:28 0.2140  EN:0.2471  Loss: 9.000 2.176 2.214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 2150  Rew: -121.43  Avg Rew: -101.97  LR:0.00090293  Bf:28 0.2150  EN:0.2469  Loss: 9.042 1.903 1.915\n",
      "Ep: 2160  Rew: -148.04  Avg Rew: -101.35  LR:0.00090253  Bf:28 0.2160  EN:0.2467  Loss: 9.150 2.378 2.305\n",
      "Ep: 2170  Rew: -109.75  Avg Rew: -101.50  LR:0.00090212  Bf:29 0.2170  EN:0.2465  Loss: 9.151 1.936 1.910\n",
      "Ep: 2180  Rew: -114.63  Avg Rew: -101.21  LR:0.00090171  Bf:29 0.2180  EN:0.2463  Loss: 9.080 2.407 2.275\n",
      "Ep: 2190  Rew:  -99.16  Avg Rew: -102.57  LR:0.00090131  Bf:29 0.2190  EN:0.2461  Loss: 9.321 2.152 2.075\n",
      "Ep: 2200  Rew: -142.93  Avg Rew: -101.73  LR:0.00090090  Bf:29 0.2200  EN:0.2459  Loss: 9.263 2.085 2.123\n",
      "Ep: 2210  Rew:  -96.44  Avg Rew: -103.35  LR:0.00090050  Bf:30 0.2210  EN:0.2457  Loss: 9.190 1.955 1.861\n",
      "Ep: 2220  Rew: -114.27  Avg Rew: -103.34  LR:0.00090009  Bf:30 0.2220  EN:0.2455  Loss: 9.321 2.230 2.187\n",
      "Ep: 2230  Rew: -118.99  Avg Rew: -103.03  LR:0.00089969  Bf:30 0.2230  EN:0.2453  Loss: 9.220 2.117 2.036\n",
      "Ep: 2240  Rew:  -80.09  Avg Rew: -105.21  LR:0.00089928  Bf:30 0.2240  EN:0.2451  Loss: 9.738 2.058 2.279\n",
      "Ep: 2250  Rew: -107.18  Avg Rew: -104.17  LR:0.00089888  Bf:31 0.2250  EN:0.2449  Loss: 9.334 2.086 2.086\n",
      "Ep: 2260  Rew: -131.49  Avg Rew: -105.21  LR:0.00089847  Bf:31 0.2260  EN:0.2447  Loss: 9.568 1.998 1.894\n",
      "Ep: 2270  Rew:  -82.18  Avg Rew: -106.95  LR:0.00089807  Bf:31 0.2270  EN:0.2445  Loss: 9.580 2.240 2.468\n",
      "Ep: 2280  Rew: -110.90  Avg Rew: -106.61  LR:0.00089767  Bf:31 0.2280  EN:0.2443  Loss: 9.477 2.173 2.282\n",
      "Ep: 2290  Rew: -133.60  Avg Rew: -105.89  LR:0.00089726  Bf:32 0.2290  EN:0.2441  Loss: 9.817 2.219 2.158\n",
      "Ep: 2300  Rew: -107.11  Avg Rew: -105.91  LR:0.00089686  Bf:32 0.2300  EN:0.2439  Loss: 9.706 2.052 2.143\n",
      "Ep: 2310  Rew: -118.82  Avg Rew: -107.11  LR:0.00089646  Bf:32 0.2310  EN:0.2437  Loss: 9.714 2.083 2.205\n",
      "Ep: 2320  Rew: -106.68  Avg Rew: -107.85  LR:0.00089606  Bf:33 0.2320  EN:0.2435  Loss: 9.856 2.076 1.959\n",
      "Ep: 2330  Rew: -120.97  Avg Rew: -107.19  LR:0.00089566  Bf:33 0.2330  EN:0.2433  Loss: 9.868 1.971 2.008\n",
      "Ep: 2340  Rew: -141.98  Avg Rew: -109.63  LR:0.00089526  Bf:33 0.2340  EN:0.2431  Loss: 9.733 2.212 2.312\n",
      "Ep: 2350  Rew: -196.33  Avg Rew: -110.39  LR:0.00089485  Bf:34 0.2350  EN:0.2429  Loss: 9.880 2.033 2.299\n",
      "Ep: 2360  Rew: -114.01  Avg Rew: -110.25  LR:0.00089445  Bf:34 0.2360  EN:0.2427  Loss: 9.939 1.851 1.926\n",
      "Ep: 2370  Rew:  -69.47  Avg Rew: -108.46  LR:0.00089405  Bf:35 0.2370  EN:0.2425  Loss: 9.724 1.752 1.793\n",
      "Ep: 2380  Rew: -119.33  Avg Rew: -109.53  LR:0.00089366  Bf:35 0.2380  EN:0.2423  Loss: 9.931 2.149 2.023\n",
      "Ep: 2390  Rew:  -22.01  Avg Rew: -107.57  LR:0.00089326  Bf:35 0.2390  EN:0.2421  Loss: 9.738 2.364 2.369\n",
      "Ep: 2400  Rew: -187.90  Avg Rew: -108.55  LR:0.00089286  Bf:36 0.2400  EN:0.2419  Loss: 9.694 2.042 1.848\n",
      "Ep: 2410  Rew: -102.03  Avg Rew: -108.73  LR:0.00089246  Bf:36 0.2410  EN:0.2417  Loss: 9.825 1.725 1.640\n",
      "Ep: 2420  Rew: -122.66  Avg Rew: -107.30  LR:0.00089206  Bf:36 0.2420  EN:0.2415  Loss: 9.845 1.829 1.837\n",
      "Ep: 2430  Rew: -129.63  Avg Rew: -108.63  LR:0.00089166  Bf:37 0.2430  EN:0.2414  Loss: 9.948 1.835 1.762\n",
      "Ep: 2440  Rew: -115.16  Avg Rew: -107.35  LR:0.00089127  Bf:37 0.2440  EN:0.2412  Loss: 9.777 1.938 1.947\n",
      "Ep: 2450  Rew:  -57.43  Avg Rew: -107.12  LR:0.00089087  Bf:37 0.2450  EN:0.2410  Loss: 9.910 1.952 1.914\n",
      "Ep: 2460  Rew: -104.36  Avg Rew: -106.76  LR:0.00089047  Bf:38 0.2460  EN:0.2408  Loss: 9.908 1.783 1.911\n",
      "Ep: 2470  Rew: -153.41  Avg Rew: -107.46  LR:0.00089008  Bf:38 0.2470  EN:0.2406  Loss: 9.857 1.900 2.034\n",
      "Ep: 2480  Rew: -134.13  Avg Rew: -106.53  LR:0.00088968  Bf:38 0.2480  EN:0.2404  Loss: 9.988 1.875 1.861\n",
      "Ep: 2490  Rew: -146.92  Avg Rew: -108.83  LR:0.00088928  Bf:39 0.2490  EN:0.2402  Loss: 9.926 1.886 2.088\n",
      "Ep: 2500  Rew: -151.96  Avg Rew: -109.32  LR:0.00088889  Bf:39 0.2500  EN:0.2400  Loss: 9.864 1.910 1.913\n",
      "Ep: 2510  Rew:  -52.92  Avg Rew: -110.06  LR:0.00088849  Bf:39 0.2510  EN:0.2398  Loss: 9.881 1.935 1.981\n",
      "Ep: 2520  Rew: -117.01  Avg Rew: -111.62  LR:0.00088810  Bf:40 0.2520  EN:0.2396  Loss: 9.870 1.783 1.943\n",
      "Ep: 2530  Rew:    0.02  Avg Rew: -111.71  LR:0.00088771  Bf:40 0.2530  EN:0.2394  Loss: 9.723 1.876 1.772\n",
      "Ep: 2540  Rew: -155.18  Avg Rew: -111.91  LR:0.00088731  Bf:41 0.2540  EN:0.2392  Loss: 9.770 1.843 1.780\n",
      "Ep: 2550  Rew: -129.93  Avg Rew: -112.68  LR:0.00088692  Bf:41 0.2550  EN:0.2390  Loss: 9.885 1.993 2.003\n",
      "Ep: 2560  Rew:  -95.28  Avg Rew: -113.94  LR:0.00088652  Bf:41 0.2560  EN:0.2389  Loss: 9.869 1.783 1.807\n",
      "Ep: 2570  Rew: -115.39  Avg Rew: -114.19  LR:0.00088613  Bf:42 0.2570  EN:0.2387  Loss: 9.695 1.622 1.727\n",
      "Ep: 2580  Rew: -114.74  Avg Rew: -114.46  LR:0.00088574  Bf:42 0.2580  EN:0.2385  Loss: 9.762 1.659 1.556\n",
      "Ep: 2590  Rew: -154.92  Avg Rew: -116.11  LR:0.00088535  Bf:42 0.2590  EN:0.2383  Loss: 9.738 1.743 1.772\n",
      "Ep: 2600  Rew: -121.13  Avg Rew: -117.93  LR:0.00088496  Bf:43 0.2600  EN:0.2381  Loss: 9.607 1.633 1.765\n",
      "Ep: 2610  Rew:  -29.43  Avg Rew: -116.12  LR:0.00088456  Bf:43 0.2610  EN:0.2379  Loss: 9.543 1.864 1.773\n",
      "Ep: 2620  Rew: -150.30  Avg Rew: -117.86  LR:0.00088417  Bf:44 0.2620  EN:0.2377  Loss: 9.615 1.883 2.116\n",
      "Ep: 2630  Rew: -111.49  Avg Rew: -119.79  LR:0.00088378  Bf:44 0.2630  EN:0.2375  Loss: 9.557 1.746 1.699\n",
      "Ep: 2640  Rew: -130.69  Avg Rew: -121.38  LR:0.00088339  Bf:44 0.2640  EN:0.2373  Loss: 9.426 1.825 1.674\n",
      "Ep: 2650  Rew:  -88.68  Avg Rew: -117.52  LR:0.00088300  Bf:45 0.2650  EN:0.2372  Loss: 9.419 1.717 1.719\n",
      "Ep: 2660  Rew:  -87.87  Avg Rew: -115.24  LR:0.00088261  Bf:45 0.2660  EN:0.2370  Loss: 9.386 1.568 1.786\n",
      "Ep: 2670  Rew: -118.73  Avg Rew: -115.21  LR:0.00088222  Bf:45 0.2670  EN:0.2368  Loss: 9.541 1.785 1.865\n",
      "Ep: 2680  Rew:  -78.25  Avg Rew: -114.60  LR:0.00088183  Bf:46 0.2680  EN:0.2366  Loss: 9.386 1.754 1.684\n",
      "Ep: 2690  Rew: -117.47  Avg Rew: -112.11  LR:0.00088145  Bf:46 0.2690  EN:0.2364  Loss: 9.429 1.844 2.066\n",
      "Ep: 2700  Rew: -124.68  Avg Rew: -110.21  LR:0.00088106  Bf:47 0.2700  EN:0.2362  Loss: 9.428 1.863 1.707\n",
      "Ep: 2710  Rew: -145.74  Avg Rew: -111.19  LR:0.00088067  Bf:47 0.2710  EN:0.2360  Loss: 9.390 1.687 1.699\n",
      "Ep: 2720  Rew:  -96.27  Avg Rew: -108.28  LR:0.00088028  Bf:47 0.2720  EN:0.2358  Loss: 9.607 1.647 1.645\n",
      "Ep: 2730  Rew:  -32.02  Avg Rew: -106.85  LR:0.00087989  Bf:48 0.2730  EN:0.2357  Loss: 9.622 1.722 1.623\n",
      "Ep: 2740  Rew: -148.26  Avg Rew: -105.77  LR:0.00087951  Bf:48 0.2740  EN:0.2355  Loss: 9.382 1.745 1.947\n",
      "Ep: 2750  Rew:  -93.94  Avg Rew: -106.98  LR:0.00087912  Bf:48 0.2750  EN:0.2353  Loss: 9.666 1.815 1.724\n",
      "Ep: 2760  Rew:  -26.50  Avg Rew: -107.40  LR:0.00087873  Bf:49 0.2760  EN:0.2351  Loss: 9.618 1.626 1.806\n",
      "Ep: 2770  Rew: -102.14  Avg Rew: -103.98  LR:0.00087835  Bf:49 0.2770  EN:0.2349  Loss: 9.507 1.712 1.730\n",
      "Ep: 2780  Rew: -129.22  Avg Rew: -105.09  LR:0.00087796  Bf:50 0.2780  EN:0.2347  Loss: 9.489 1.715 1.741\n",
      "Ep: 2790  Rew:  -95.57  Avg Rew: -106.00  LR:0.00087758  Bf:50 0.2790  EN:0.2346  Loss: 9.513 1.593 1.696\n",
      "Ep: 2800  Rew:  -91.42  Avg Rew: -103.68  LR:0.00087719  Bf:50 0.2800  EN:0.2344  Loss: 9.432 1.911 1.789\n",
      "Ep: 2810  Rew: -147.22  Avg Rew: -103.47  LR:0.00087681  Bf:51 0.2810  EN:0.2342  Loss: 9.790 1.531 1.501\n",
      "Ep: 2820  Rew:  -30.43  Avg Rew: -102.73  LR:0.00087642  Bf:51 0.2820  EN:0.2340  Loss: 9.508 1.534 1.497\n",
      "Ep: 2830  Rew: -124.37  Avg Rew: -101.26  LR:0.00087604  Bf:51 0.2830  EN:0.2338  Loss: 9.598 1.525 1.558\n",
      "Ep: 2840  Rew:  -51.42  Avg Rew:  -98.76  LR:0.00087566  Bf:51 0.2840  EN:0.2336  Loss: 9.645 1.879 1.733\n",
      "Ep: 2850  Rew:  -87.38  Avg Rew: -102.87  LR:0.00087527  Bf:52 0.2850  EN:0.2335  Loss: 9.456 1.569 1.620\n",
      "Ep: 2860  Rew: -119.33  Avg Rew: -102.62  LR:0.00087489  Bf:52 0.2860  EN:0.2333  Loss: 9.623 1.560 1.690\n",
      "Ep: 2870  Rew: -115.16  Avg Rew: -104.05  LR:0.00087451  Bf:52 0.2870  EN:0.2331  Loss: 9.563 2.070 1.884\n",
      "Ep: 2880  Rew:  -77.45  Avg Rew: -101.04  LR:0.00087413  Bf:53 0.2880  EN:0.2329  Loss: 9.721 1.854 1.678\n",
      "Ep: 2890  Rew: -157.80  Avg Rew: -102.46  LR:0.00087374  Bf:53 0.2890  EN:0.2327  Loss: 9.517 1.658 1.720\n",
      "Ep: 2900  Rew: -131.48  Avg Rew: -103.97  LR:0.00087336  Bf:53 0.2900  EN:0.2326  Loss: 9.669 1.469 1.467\n",
      "Ep: 2910  Rew: -161.50  Avg Rew: -102.12  LR:0.00087298  Bf:54 0.2910  EN:0.2324  Loss: 9.596 1.710 1.787\n",
      "Ep: 2920  Rew: -108.09  Avg Rew: -100.80  LR:0.00087260  Bf:54 0.2920  EN:0.2322  Loss: 9.726 1.614 1.707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 2930  Rew: -119.35  Avg Rew:  -98.90  LR:0.00087222  Bf:54 0.2930  EN:0.2320  Loss: 9.736 1.448 1.449\n",
      "Ep: 2940  Rew:  -97.62  Avg Rew: -100.78  LR:0.00087184  Bf:55 0.2940  EN:0.2318  Loss: 9.740 1.579 1.584\n",
      "Ep: 2950  Rew: -106.43  Avg Rew:  -96.92  LR:0.00087146  Bf:55 0.2950  EN:0.2317  Loss: 9.707 1.452 1.448\n",
      "Ep: 2960  Rew:  -61.36  Avg Rew:  -95.72  LR:0.00087108  Bf:55 0.2960  EN:0.2315  Loss: 9.728 1.488 1.566\n",
      "Ep: 2970  Rew:  -98.21  Avg Rew:  -98.26  LR:0.00087070  Bf:56 0.2970  EN:0.2313  Loss: 9.641 1.547 1.545\n",
      "Ep: 2980  Rew: -146.90  Avg Rew:  -99.80  LR:0.00087032  Bf:56 0.2980  EN:0.2311  Loss: 9.727 1.659 1.779\n",
      "Ep: 2990  Rew: -111.39  Avg Rew:  -98.57  LR:0.00086994  Bf:56 0.2990  EN:0.2309  Loss: 9.597 1.661 1.626\n",
      "Ep: 3000  Rew:  -85.38  Avg Rew:  -96.93  LR:0.00086957  Bf:57 0.3000  EN:0.2308  Loss: 9.701 1.655 1.561\n",
      "Ep: 3010  Rew: -126.00  Avg Rew: -101.15  LR:0.00086919  Bf:57 0.3010  EN:0.2306  Loss: 9.621 1.656 1.569\n",
      "Ep: 3020  Rew: -120.41  Avg Rew: -103.63  LR:0.00086881  Bf:57 0.3020  EN:0.2304  Loss: 9.460 1.373 1.318\n",
      "Ep: 3030  Rew:  -96.37  Avg Rew: -107.08  LR:0.00086843  Bf:58 0.3030  EN:0.2302  Loss: 9.543 1.645 1.681\n",
      "Ep: 3040  Rew:  -96.84  Avg Rew: -104.13  LR:0.00086806  Bf:58 0.3040  EN:0.2301  Loss: 9.586 1.674 1.751\n",
      "Ep: 3050  Rew: -117.68  Avg Rew: -102.99  LR:0.00086768  Bf:58 0.3050  EN:0.2299  Loss: 9.504 1.396 1.425\n",
      "Ep: 3060  Rew:  -35.42  Avg Rew: -104.21  LR:0.00086730  Bf:59 0.3060  EN:0.2297  Loss: 9.595 1.504 1.406\n",
      "Ep: 3070  Rew:  -60.88  Avg Rew: -102.56  LR:0.00086693  Bf:59 0.3070  EN:0.2295  Loss: 9.755 1.504 1.530\n",
      "Ep: 3080  Rew: -106.15  Avg Rew: -103.14  LR:0.00086655  Bf:59 0.3080  EN:0.2294  Loss: 9.803 1.605 1.660\n",
      "Ep: 3090  Rew: -128.11  Avg Rew: -104.02  LR:0.00086618  Bf:60 0.3090  EN:0.2292  Loss: 9.690 1.359 1.433\n",
      "Ep: 3100  Rew: -110.94  Avg Rew: -106.39  LR:0.00086580  Bf:60 0.3100  EN:0.2290  Loss: 9.649 1.684 1.710\n",
      "Ep: 3110  Rew:  -97.12  Avg Rew: -102.82  LR:0.00086543  Bf:60 0.3110  EN:0.2288  Loss: 9.720 1.455 1.359\n",
      "Ep: 3120  Rew:  -94.56  Avg Rew: -100.03  LR:0.00086505  Bf:61 0.3120  EN:0.2287  Loss: 9.712 1.779 1.767\n",
      "Ep: 3130  Rew: -107.09  Avg Rew: -100.06  LR:0.00086468  Bf:61 0.3130  EN:0.2285  Loss: 9.680 1.361 1.433\n",
      "Ep: 3140  Rew: -164.36  Avg Rew: -103.45  LR:0.00086430  Bf:61 0.3140  EN:0.2283  Loss: 9.684 1.725 1.572\n",
      "Ep: 3150  Rew: -120.55  Avg Rew: -105.92  LR:0.00086393  Bf:62 0.3150  EN:0.2281  Loss: 9.810 1.602 1.552\n",
      "Ep: 3160  Rew:  -64.43  Avg Rew: -105.08  LR:0.00086356  Bf:62 0.3160  EN:0.2280  Loss: 9.740 1.444 1.406\n",
      "Ep: 3170  Rew:  -94.75  Avg Rew: -104.00  LR:0.00086319  Bf:62 0.3170  EN:0.2278  Loss: 9.669 1.567 1.496\n",
      "Ep: 3180  Rew: -112.61  Avg Rew: -103.70  LR:0.00086281  Bf:63 0.3180  EN:0.2276  Loss: 9.639 1.368 1.435\n",
      "Ep: 3190  Rew: -113.34  Avg Rew: -102.03  LR:0.00086244  Bf:63 0.3190  EN:0.2274  Loss: 9.682 1.680 1.493\n",
      "Ep: 3200  Rew:  -88.00  Avg Rew: -100.82  LR:0.00086207  Bf:63 0.3200  EN:0.2273  Loss: 9.805 1.549 1.642\n",
      "Ep: 3210  Rew:  -33.16  Avg Rew: -102.00  LR:0.00086170  Bf:64 0.3210  EN:0.2271  Loss: 9.763 1.335 1.360\n",
      "Ep: 3220  Rew:  -77.28  Avg Rew: -102.93  LR:0.00086133  Bf:64 0.3220  EN:0.2269  Loss: 9.676 1.567 1.741\n",
      "Ep: 3230  Rew: -116.44  Avg Rew: -102.85  LR:0.00086096  Bf:65 0.3230  EN:0.2268  Loss: 9.631 1.474 1.585\n",
      "Ep: 3240  Rew: -126.43  Avg Rew: -102.71  LR:0.00086059  Bf:65 0.3240  EN:0.2266  Loss: 9.639 1.752 2.002\n",
      "Ep: 3250  Rew:  -39.90  Avg Rew: -100.51  LR:0.00086022  Bf:65 0.3250  EN:0.2264  Loss: 9.708 1.656 1.703\n",
      "Ep: 3260  Rew:  -15.20  Avg Rew:  -98.83  LR:0.00085985  Bf:66 0.3260  EN:0.2262  Loss: 9.649 1.504 1.491\n",
      "Ep: 3270  Rew: -144.76  Avg Rew: -101.26  LR:0.00085948  Bf:66 0.3270  EN:0.2261  Loss: 9.668 1.363 1.367\n",
      "Ep: 3280  Rew: -107.80  Avg Rew: -102.38  LR:0.00085911  Bf:66 0.3280  EN:0.2259  Loss: 9.904 1.346 1.336\n",
      "Ep: 3290  Rew: -139.04  Avg Rew: -102.78  LR:0.00085874  Bf:67 0.3290  EN:0.2257  Loss: 9.902 1.666 1.647\n",
      "Ep: 3300  Rew: -177.65  Avg Rew: -103.61  LR:0.00085837  Bf:67 0.3300  EN:0.2256  Loss: 9.879 1.421 1.399\n",
      "Ep: 3310  Rew:  -72.78  Avg Rew: -103.30  LR:0.00085800  Bf:68 0.3310  EN:0.2254  Loss: 9.744 1.584 1.576\n",
      "Ep: 3320  Rew:  -91.79  Avg Rew: -105.77  LR:0.00085763  Bf:68 0.3320  EN:0.2252  Loss: 9.802 1.252 1.212\n",
      "Ep: 3330  Rew:  -99.51  Avg Rew: -104.35  LR:0.00085727  Bf:68 0.3330  EN:0.2251  Loss: 9.642 1.395 1.450\n",
      "Ep: 3340  Rew:  -72.37  Avg Rew: -104.35  LR:0.00085690  Bf:69 0.3340  EN:0.2249  Loss: 9.975 1.456 1.368\n",
      "Ep: 3350  Rew: -125.90  Avg Rew: -109.33  LR:0.00085653  Bf:69 0.3350  EN:0.2247  Loss: 9.571 1.392 1.384\n",
      "Ep: 3360  Rew:  -88.09  Avg Rew: -110.67  LR:0.00085616  Bf:69 0.3360  EN:0.2246  Loss: 9.887 1.480 1.393\n",
      "Ep: 3370  Rew: -106.61  Avg Rew: -109.47  LR:0.00085580  Bf:70 0.3370  EN:0.2244  Loss: 9.659 1.411 1.443\n",
      "Ep: 3380  Rew: -108.07  Avg Rew: -106.32  LR:0.00085543  Bf:70 0.3380  EN:0.2242  Loss: 9.755 1.579 1.452\n",
      "Ep: 3390  Rew: -111.02  Avg Rew: -104.45  LR:0.00085507  Bf:71 0.3390  EN:0.2240  Loss: 9.847 1.339 1.306\n",
      "Ep: 3400  Rew:  -80.64  Avg Rew: -103.54  LR:0.00085470  Bf:71 0.3400  EN:0.2239  Loss: 9.799 1.514 1.734\n",
      "Ep: 3410  Rew: -120.77  Avg Rew: -103.89  LR:0.00085434  Bf:71 0.3410  EN:0.2237  Loss: 9.657 1.272 1.323\n",
      "Ep: 3420  Rew: -117.03  Avg Rew: -104.12  LR:0.00085397  Bf:72 0.3420  EN:0.2235  Loss: 9.760 1.628 1.593\n",
      "Ep: 3430  Rew: -140.15  Avg Rew: -105.24  LR:0.00085361  Bf:72 0.3430  EN:0.2234  Loss: 9.670 1.404 1.595\n",
      "Ep: 3440  Rew: -126.83  Avg Rew: -104.16  LR:0.00085324  Bf:72 0.3440  EN:0.2232  Loss: 9.611 1.433 1.340\n",
      "Ep: 3450  Rew:  -81.67  Avg Rew: -100.98  LR:0.00085288  Bf:73 0.3450  EN:0.2230  Loss: 9.568 1.324 1.532\n",
      "Ep: 3460  Rew: -125.53  Avg Rew: -103.69  LR:0.00085251  Bf:73 0.3460  EN:0.2229  Loss: 9.687 1.419 1.612\n",
      "Ep: 3470  Rew: -116.06  Avg Rew: -103.41  LR:0.00085215  Bf:74 0.3470  EN:0.2227  Loss: 9.624 1.492 1.640\n",
      "Ep: 3480  Rew: -113.58  Avg Rew: -106.78  LR:0.00085179  Bf:74 0.3480  EN:0.2226  Loss: 9.655 1.686 1.412\n",
      "Ep: 3490  Rew:  -97.84  Avg Rew: -109.22  LR:0.00085143  Bf:74 0.3490  EN:0.2224  Loss: 9.643 1.248 1.240\n",
      "Ep: 3500  Rew: -116.63  Avg Rew: -109.55  LR:0.00085106  Bf:75 0.3500  EN:0.2222  Loss: 9.743 1.349 1.534\n",
      "Ep: 3510  Rew:  -30.24  Avg Rew: -107.24  LR:0.00085070  Bf:75 0.3510  EN:0.2221  Loss: 9.538 1.299 1.245\n",
      "Ep: 3520  Rew: -113.65  Avg Rew: -105.24  LR:0.00085034  Bf:76 0.3520  EN:0.2219  Loss: 9.678 1.343 1.333\n",
      "Ep: 3530  Rew: -103.16  Avg Rew: -104.87  LR:0.00084998  Bf:76 0.3530  EN:0.2217  Loss: 9.615 1.410 1.334\n",
      "Ep: 3540  Rew: -109.10  Avg Rew: -107.32  LR:0.00084962  Bf:76 0.3540  EN:0.2216  Loss: 9.632 1.140 1.142\n",
      "Ep: 3550  Rew: -107.93  Avg Rew: -108.01  LR:0.00084926  Bf:77 0.3550  EN:0.2214  Loss: 9.626 1.226 1.262\n",
      "Ep: 3560  Rew: -174.66  Avg Rew: -108.06  LR:0.00084890  Bf:77 0.3560  EN:0.2212  Loss: 9.629 1.654 1.432\n",
      "Ep: 3570  Rew: -116.75  Avg Rew: -109.82  LR:0.00084854  Bf:78 0.3570  EN:0.2211  Loss: 9.532 1.701 1.652\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d712ea419883>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                    \u001b[0mmax_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_timesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_buffer_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_buffer_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                    log_interval=log_interval)\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-f996c980288b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_timesteps\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                     self.policy.update(self.replay_buffer, t, self.batch_size, self.gamma, self.polyak, \n\u001b[0;32m--> 123\u001b[0;31m                                        self.policy_noise, self.noise_clip, self.policy_delay, beta)\n\u001b[0m\u001b[1;32m    124\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/gym/bipedal_walker_hardcore/TD3/td3.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, replay_buffer, n_iter, batch_size, gamma, polyak, policy_noise, noise_clip, policy_delay, beta)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mtarget_Q2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_2_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mtarget_Q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_Q1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_Q2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mtarget_Q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtarget_Q\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;31m# Optimize Critic 1:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = TD3Trainer(env_name, actor_config, critic_config, random_seed=random_seed, lr_base=lr_base, lr_decay=lr_decay, \n",
    "                   exp_noise_base=exp_noise_base, exp_noise_decay=exp_noise_decay, gamma=gamma, batch_size=batch_size,\n",
    "                   polyak=polyak, policy_noise=policy_noise, noise_clip=noise_clip, policy_delay=policy_delay, \n",
    "                   max_episodes=max_episodes, max_timesteps=max_timesteps, max_buffer_length=max_buffer_length, \n",
    "                   log_interval=log_interval)\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
