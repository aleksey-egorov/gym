{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "ACTOR=Sequential(\n",
      "  (0): Linear(in_features=24, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=320, bias=True)\n",
      "  (3): Dropout(p=0.2)\n",
      "  (4): ReLU()\n",
      "  (5): Linear(in_features=320, out_features=160, bias=True)\n",
      "  (6): ReLU()\n",
      "  (7): Linear(in_features=160, out_features=64, bias=True)\n",
      "  (8): ReLU()\n",
      "  (9): Linear(in_features=64, out_features=4, bias=True)\n",
      ")\n",
      "ACTOR=Sequential(\n",
      "  (0): Linear(in_features=24, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=320, bias=True)\n",
      "  (3): Dropout(p=0.2)\n",
      "  (4): ReLU()\n",
      "  (5): Linear(in_features=320, out_features=160, bias=True)\n",
      "  (6): ReLU()\n",
      "  (7): Linear(in_features=160, out_features=64, bias=True)\n",
      "  (8): ReLU()\n",
      "  (9): Linear(in_features=64, out_features=4, bias=True)\n",
      ")\n",
      "CRITIC=Sequential(\n",
      "  (0): Linear(in_features=28, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=320, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=320, out_features=160, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=160, out_features=1, bias=True)\n",
      ")\n",
      "CRITIC=Sequential(\n",
      "  (0): Linear(in_features=28, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=320, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=320, out_features=160, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=160, out_features=1, bias=True)\n",
      ")\n",
      "CRITIC=Sequential(\n",
      "  (0): Linear(in_features=28, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=320, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=320, out_features=160, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=160, out_features=1, bias=True)\n",
      ")\n",
      "CRITIC=Sequential(\n",
      "  (0): Linear(in_features=28, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=320, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=320, out_features=160, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=160, out_features=1, bias=True)\n",
      ")\n",
      "action_space=Box(4,)\n",
      "obs_space=Box(24,)\n",
      "threshold=300 \n",
      "\n",
      "Random Seed: 222\n",
      "DIR=./preTrained/td3_torch/BipedalWalkerHardcore-v2 NAME=TD3_torch_BipedalWalkerHardcore-v2_222\n",
      "Models loaded\n",
      "Ep:10   Rew:-76.50  Avg Rew:-78.64  LR:0.00008573   Polyak:0.99990  Bf: 1  EN:0.0000  Loss: 4.850 0.093 0.120\n",
      "Ep:20   Rew:-67.32  Avg Rew:-69.95  LR:0.00008399   Polyak:0.99990  Bf: 1  EN:0.0000  Loss: 4.621 0.492 0.463\n",
      "Ep:30   Rew:-141.89  Avg Rew:-66.23  LR:0.00008325   Polyak:0.99990  Bf: 2  EN:0.0000  Loss: 5.453 0.389 0.339\n",
      "Ep:40   Rew:-91.27  Avg Rew:-59.97  LR:0.00008199   Polyak:0.99990  Bf: 3  EN:0.0000  Loss: 5.770 0.402 0.310\n",
      "Ep:50   Rew:-65.56  Avg Rew:-56.06  LR:0.00008121   Polyak:0.99990  Bf: 4  EN:0.0000  Loss: 4.800 0.235 0.255\n",
      "Ep:60   Rew:-5.74  Avg Rew:-49.37  LR:0.00007987   Polyak:0.99990  Bf: 4  EN:0.0000  Loss: 4.733 0.413 0.471\n",
      "Ep:70   Rew:-5.10  Avg Rew:-38.29  LR:0.00007766   Polyak:0.99990  Bf: 5  EN:0.0000  Loss: 4.388 0.644 0.483\n",
      "Ep:80   Rew:-53.05  Avg Rew:-32.12  LR:0.00007642   Polyak:0.99990  Bf: 6  EN:0.0000  Loss: 4.736 0.450 0.484\n",
      "Ep:90   Rew:-25.93  Avg Rew:-29.72  LR:0.00007594   Polyak:0.99990  Bf: 7  EN:0.0000  Loss: 4.317 0.359 0.413\n",
      "Ep:100   Rew:-55.91  Avg Rew:-30.72  LR:0.00007614   Polyak:0.99990  Bf: 7  EN:0.0000  Loss: 4.460 0.348 0.311\n",
      "Ep:110   Rew:-71.91  Avg Rew:-21.47  LR:0.00007429   Polyak:0.99990  Bf: 8  EN:0.0000  Loss: 4.419 0.474 0.419\n",
      "Ep:120   Rew:-41.61  Avg Rew:-17.38  LR:0.00007348   Polyak:0.99990  Bf: 9  EN:0.0000  Loss: 4.726 0.296 0.341\n",
      "Ep:130   Rew:48.26  Avg Rew:-12.71  LR:0.00007254   Polyak:0.99990  Bf:10  EN:0.0000  Loss: 4.810 0.610 0.575\n",
      "Ep:140   Rew:-25.91  Avg Rew:-9.22  LR:0.00007184   Polyak:0.99990  Bf:11  EN:0.0000  Loss: 4.492 0.287 0.282\n",
      "Ep:150   Rew:-23.46  Avg Rew:-4.89  LR:0.00007098   Polyak:0.99990  Bf:12  EN:0.0000  Loss: 4.516 0.196 0.265\n",
      "Ep:160   Rew:-18.60  Avg Rew:-3.51  LR:0.00007070   Polyak:0.99990  Bf:13  EN:0.0000  Loss: 4.941 0.508 0.371\n",
      "Ep:170   Rew:270.89  Avg Rew:-1.21  LR:0.00007024   Polyak:0.99990  Bf:13  EN:0.0000  Loss: 5.055 0.390 0.539\n",
      "Ep:180   Rew:5.52  Avg Rew:1.61  LR:0.00006968   Polyak:0.99990  Bf:14  EN:0.0000  Loss: 4.530 0.250 0.345\n",
      "Ep:190   Rew:-2.78  Avg Rew:-2.17  LR:0.00007043   Polyak:0.99990  Bf:15  EN:0.0000  Loss: 4.905 0.434 0.376\n",
      "Ep:200   Rew:-26.97  Avg Rew:4.50  LR:0.00006910   Polyak:0.99990  Bf:16  EN:0.0000  Loss: 4.744 0.311 0.277\n",
      "Ep:210   Rew:-86.26  Avg Rew:3.39  LR:0.00006932   Polyak:0.99990  Bf:17  EN:0.0000  Loss: 5.360 0.428 0.398\n",
      "Ep:220   Rew:1.06  Avg Rew:3.87  LR:0.00006923   Polyak:0.99990  Bf:18  EN:0.0000  Loss: 4.876 0.519 0.556\n",
      "Ep:230   Rew:124.85  Avg Rew:5.85  LR:0.00006883   Polyak:0.99990  Bf:19  EN:0.0000  Loss: 4.930 0.418 0.321\n",
      "Ep:240   Rew:-9.38  Avg Rew:4.83  LR:0.00006903   Polyak:0.99990  Bf:20  EN:0.0000  Loss: 4.785 0.546 0.453\n",
      "Ep:250   Rew:-83.18  Avg Rew:4.76  LR:0.00006905   Polyak:0.99990  Bf:21  EN:0.0000  Loss: 4.972 0.247 0.245\n",
      "Ep:260   Rew:95.28  Avg Rew:4.69  LR:0.00006906   Polyak:0.99990  Bf:21  EN:0.0000  Loss: 4.819 0.458 0.541\n",
      "Ep:270   Rew:-7.40  Avg Rew:-2.18  LR:0.00007044   Polyak:0.99990  Bf:22  EN:0.0000  Loss: 4.773 2.080 1.890\n",
      "Ep:280   Rew:16.12  Avg Rew:-8.43  LR:0.00007169   Polyak:0.99990  Bf:23  EN:0.0000  Loss: 4.697 0.631 0.559\n",
      "Ep:290   Rew:-123.08  Avg Rew:-0.33  LR:0.00007007   Polyak:0.99990  Bf:24  EN:0.0000  Loss: 4.886 0.650 0.715\n",
      "Ep:300   Rew:276.21  Avg Rew:2.03  LR:0.00006959   Polyak:0.99990  Bf:25  EN:0.0000  Loss: 4.824 0.385 0.445\n",
      "Ep:310   Rew:-29.50  Avg Rew:2.71  LR:0.00006946   Polyak:0.99990  Bf:26  EN:0.0000  Loss: 4.722 0.403 0.336\n",
      "Ep:320   Rew:-22.04  Avg Rew:3.82  LR:0.00006924   Polyak:0.99990  Bf:27  EN:0.0000  Loss: 4.472 0.408 0.342\n",
      "Ep:330   Rew:-42.90  Avg Rew:5.45  LR:0.00006891   Polyak:0.99990  Bf:27  EN:0.0000  Loss: 4.801 1.037 1.516\n",
      "Ep:340   Rew:24.17  Avg Rew:8.96  LR:0.00006821   Polyak:0.99990  Bf:28  EN:0.0000  Loss: 4.791 0.391 0.299\n",
      "Ep:350   Rew:101.59  Avg Rew:10.32  LR:0.00006794   Polyak:0.99990  Bf:29  EN:0.0000  Loss: 4.692 0.315 0.281\n",
      "Ep:360   Rew:-37.97  Avg Rew:15.54  LR:0.00006689   Polyak:0.99990  Bf:30  EN:0.0000  Loss: 5.121 0.437 0.447\n",
      "Ep:370   Rew:-49.31  Avg Rew:18.69  LR:0.00006626   Polyak:0.99990  Bf:31  EN:0.0000  Loss: 4.894 0.659 0.724\n",
      "Ep:380   Rew:-142.66  Avg Rew:17.59  LR:0.00006648   Polyak:0.99990  Bf:32  EN:0.0000  Loss: 4.528 0.436 0.392\n",
      "Ep:390   Rew:-40.03  Avg Rew:17.19  LR:0.00006656   Polyak:0.99990  Bf:33  EN:0.0000  Loss: 4.810 0.327 0.398\n",
      "Ep:400   Rew:41.15  Avg Rew:12.76  LR:0.00006745   Polyak:0.99990  Bf:34  EN:0.0000  Loss: 5.024 0.282 0.362\n",
      "Ep:410   Rew:9.60  Avg Rew:12.93  LR:0.00006741   Polyak:0.99990  Bf:34  EN:0.0000  Loss: 5.323 1.342 2.123\n",
      "Ep:420   Rew:-17.79  Avg Rew:19.65  LR:0.00006607   Polyak:0.99990  Bf:35  EN:0.0000  Loss: 4.862 0.343 0.333\n",
      "Ep:430   Rew:51.33  Avg Rew:20.25  LR:0.00006595   Polyak:0.99990  Bf:36  EN:0.0000  Loss: 4.739 0.452 0.420\n",
      "Ep:440   Rew:22.24  Avg Rew:17.85  LR:0.00006643   Polyak:0.99990  Bf:37  EN:0.0000  Loss: 4.435 0.307 0.336\n",
      "Ep:450   Rew:14.99  Avg Rew:16.09  LR:0.00006678   Polyak:0.99990  Bf:38  EN:0.0000  Loss: 4.782 0.244 0.289\n",
      "Ep:460   Rew:94.85  Avg Rew:12.04  LR:0.00006759   Polyak:0.99990  Bf:39  EN:0.0000  Loss: 4.646 0.537 0.552\n",
      "Ep:470   Rew:63.57  Avg Rew:9.98  LR:0.00006800   Polyak:0.99990  Bf:40  EN:0.0000  Loss: 5.153 0.490 0.385\n",
      "Ep:480   Rew:4.30  Avg Rew:16.14  LR:0.00006677   Polyak:0.99990  Bf:41  EN:0.0000  Loss: 4.648 0.305 0.423\n",
      "Ep:490   Rew:98.24  Avg Rew:13.04  LR:0.00006739   Polyak:0.99990  Bf:42  EN:0.0000  Loss: 4.760 0.457 0.400\n",
      "Ep:500   Rew:-58.81  Avg Rew:10.46  LR:0.00006791   Polyak:0.99990  Bf:43  EN:0.0000  Loss: 4.696 0.366 0.332\n",
      "Ep:510   Rew:-40.03  Avg Rew:10.50  LR:0.00006790   Polyak:0.99990  Bf:44  EN:0.0000  Loss: 5.238 0.402 0.573\n",
      "Ep:520   Rew:-59.27  Avg Rew:6.16  LR:0.00006877   Polyak:0.99990  Bf:45  EN:0.0000  Loss: 4.585 1.676 1.830\n",
      "Ep:530   Rew:-20.56  Avg Rew:1.60  LR:0.00006968   Polyak:0.99990  Bf:46  EN:0.0000  Loss: 4.877 0.310 0.324\n",
      "Ep:540   Rew:94.95  Avg Rew:1.67  LR:0.00006967   Polyak:0.99990  Bf:47  EN:0.0000  Loss: 4.971 0.814 0.530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep:550   Rew:53.77  Avg Rew:2.43  LR:0.00006951   Polyak:0.99990  Bf:48  EN:0.0000  Loss: 4.541 0.378 0.425\n",
      "Ep:560   Rew:-3.53  Avg Rew:4.98  LR:0.00006900   Polyak:0.99990  Bf:49  EN:0.0000  Loss: 4.931 0.459 0.384\n",
      "Ep:570   Rew:61.01  Avg Rew:6.92  LR:0.00006862   Polyak:0.99990  Bf:50  EN:0.0000  Loss: 4.801 1.450 0.842\n",
      "Ep:580   Rew:19.51  Avg Rew:2.70  LR:0.00006946   Polyak:0.99990  Bf:51  EN:0.0000  Loss: 4.823 0.403 0.485\n",
      "Ep:590   Rew:-63.39  Avg Rew:2.72  LR:0.00006946   Polyak:0.99990  Bf:51  EN:0.0000  Loss: 4.685 0.435 0.373\n",
      "Ep:600   Rew:-59.82  Avg Rew:-0.81  LR:0.00007016   Polyak:0.99990  Bf:52  EN:0.0000  Loss: 4.694 0.619 0.633\n",
      "Ep:610   Rew:25.44  Avg Rew:3.69  LR:0.00006926   Polyak:0.99990  Bf:53  EN:0.0000  Loss: 4.440 0.260 0.253\n",
      "Ep:620   Rew:-136.86  Avg Rew:0.22  LR:0.00006996   Polyak:0.99990  Bf:54  EN:0.0000  Loss: 4.903 0.537 0.593\n",
      "Ep:630   Rew:-34.43  Avg Rew:6.78  LR:0.00006864   Polyak:0.99990  Bf:55  EN:0.0000  Loss: 4.913 0.325 0.325\n",
      "Ep:640   Rew:34.36  Avg Rew:7.51  LR:0.00006850   Polyak:0.99990  Bf:56  EN:0.0000  Loss: 5.045 0.507 0.393\n",
      "Ep:650   Rew:33.79  Avg Rew:12.19  LR:0.00006756   Polyak:0.99990  Bf:57  EN:0.0000  Loss: 5.101 0.393 0.361\n",
      "Ep:660   Rew:-181.97  Avg Rew:10.00  LR:0.00006800   Polyak:0.99990  Bf:58  EN:0.0000  Loss: 4.611 0.390 0.239\n",
      "Ep:670   Rew:122.06  Avg Rew:14.23  LR:0.00006715   Polyak:0.99990  Bf:59  EN:0.0000  Loss: 4.730 0.213 0.363\n",
      "Ep:680   Rew:-16.33  Avg Rew:14.35  LR:0.00006713   Polyak:0.99990  Bf:60  EN:0.0000  Loss: 4.709 0.308 0.306\n",
      "Ep:690   Rew:-24.62  Avg Rew:16.12  LR:0.00006678   Polyak:0.99990  Bf:60  EN:0.0000  Loss: 4.846 0.493 0.284\n",
      "Ep:700   Rew:-26.99  Avg Rew:26.78  LR:0.00006464   Polyak:0.99990  Bf:61  EN:0.0000  Loss: 5.178 0.268 0.230\n",
      "Ep:710   Rew:-47.35  Avg Rew:21.63  LR:0.00006567   Polyak:0.99990  Bf:62  EN:0.0000  Loss: 4.893 0.335 0.420\n",
      "Ep:720   Rew:53.09  Avg Rew:24.86  LR:0.00006503   Polyak:0.99990  Bf:63  EN:0.0000  Loss: 4.929 0.279 0.370\n",
      "Ep:730   Rew:-30.21  Avg Rew:16.96  LR:0.00006661   Polyak:0.99990  Bf:64  EN:0.0000  Loss: 4.774 0.396 0.315\n",
      "Ep:740   Rew:-18.74  Avg Rew:19.29  LR:0.00006614   Polyak:0.99990  Bf:65  EN:0.0000  Loss: 4.688 0.275 0.234\n",
      "Ep:750   Rew:63.85  Avg Rew:17.96  LR:0.00006641   Polyak:0.99990  Bf:66  EN:0.0000  Loss: 5.119 0.281 0.275\n",
      "Ep:760   Rew:49.42  Avg Rew:20.24  LR:0.00006595   Polyak:0.99990  Bf:67  EN:0.0000  Loss: 4.905 0.469 0.524\n",
      "Ep:770   Rew:-40.96  Avg Rew:13.98  LR:0.00006720   Polyak:0.99990  Bf:68  EN:0.0000  Loss: 4.923 0.464 0.435\n",
      "Ep:780   Rew:114.66  Avg Rew:18.94  LR:0.00006621   Polyak:0.99990  Bf:69  EN:0.0000  Loss: 4.585 0.475 0.415\n",
      "Ep:790   Rew:-47.25  Avg Rew:20.33  LR:0.00006593   Polyak:0.99990  Bf:70  EN:0.0000  Loss: 4.775 0.440 0.348\n",
      "Ep:800   Rew:80.54  Avg Rew:15.88  LR:0.00006682   Polyak:0.99990  Bf:71  EN:0.0000  Loss: 4.575 0.908 0.411\n",
      "Ep:810   Rew:21.40  Avg Rew:13.82  LR:0.00006724   Polyak:0.99990  Bf:72  EN:0.0000  Loss: 4.682 0.207 0.239\n",
      "Ep:820   Rew:204.48  Avg Rew:18.80  LR:0.00006624   Polyak:0.99990  Bf:73  EN:0.0000  Loss: 4.724 0.227 0.240\n",
      "Ep:830   Rew:19.27  Avg Rew:21.81  LR:0.00006564   Polyak:0.99990  Bf:74  EN:0.0000  Loss: 5.072 0.890 0.827\n",
      "Ep:840   Rew:91.04  Avg Rew:22.33  LR:0.00006553   Polyak:0.99990  Bf:75  EN:0.0000  Loss: 4.750 2.289 8.844\n",
      "Ep:850   Rew:-47.00  Avg Rew:18.27  LR:0.00006635   Polyak:0.99990  Bf:76  EN:0.0000  Loss: 4.943 0.389 0.311\n",
      "Ep:860   Rew:-16.01  Avg Rew:18.32  LR:0.00006634   Polyak:0.99990  Bf:77  EN:0.0000  Loss: 4.767 0.292 0.282\n",
      "Ep:870   Rew:82.79  Avg Rew:20.82  LR:0.00006584   Polyak:0.99990  Bf:78  EN:0.0000  Loss: 4.828 0.507 0.547\n",
      "Ep:880   Rew:-36.82  Avg Rew:24.85  LR:0.00006503   Polyak:0.99990  Bf:79  EN:0.0000  Loss: 4.786 0.280 0.236\n",
      "Ep:890   Rew:15.42  Avg Rew:26.81  LR:0.00006464   Polyak:0.99990  Bf:80  EN:0.0000  Loss: 4.542 0.235 0.278\n",
      "Ep:900   Rew:-21.25  Avg Rew:25.05  LR:0.00006499   Polyak:0.99990  Bf:80  EN:0.0000  Loss: 4.878 0.557 0.430\n",
      "Ep:910   Rew:39.60  Avg Rew:33.99  LR:0.00006320   Polyak:0.99990  Bf:81  EN:0.0000  Loss: 4.690 0.453 0.353\n",
      "Ep:920   Rew:65.60  Avg Rew:32.61  LR:0.00006348   Polyak:0.99990  Bf:82  EN:0.0000  Loss: 4.648 0.295 0.320\n",
      "Ep:930   Rew:-66.04  Avg Rew:31.58  LR:0.00006368   Polyak:0.99990  Bf:83  EN:0.0000  Loss: 4.624 0.515 0.380\n",
      "Ep:940   Rew:-56.16  Avg Rew:33.31  LR:0.00006334   Polyak:0.99990  Bf:84  EN:0.0000  Loss: 4.753 0.406 0.385\n",
      "Ep:950   Rew:122.91  Avg Rew:36.06  LR:0.00006279   Polyak:0.99990  Bf:85  EN:0.0000  Loss: 4.669 0.393 0.299\n",
      "Ep:960   Rew:-25.68  Avg Rew:29.78  LR:0.00006404   Polyak:0.99990  Bf:86  EN:0.0000  Loss: 4.954 0.303 0.424\n",
      "Ep:970   Rew:104.01  Avg Rew:29.09  LR:0.00006418   Polyak:0.99990  Bf:87  EN:0.0000  Loss: 4.838 0.580 0.604\n",
      "Ep:980   Rew:288.26  Avg Rew:24.70  LR:0.00006506   Polyak:0.99990  Bf:88  EN:0.0000  Loss: 4.944 0.538 0.533\n",
      "Ep:990   Rew:-27.18  Avg Rew:22.05  LR:0.00006559   Polyak:0.99990  Bf:89  EN:0.0000  Loss: 4.835 0.703 0.627\n",
      "Ep:1000   Rew:0.51  Avg Rew:20.81  LR:0.00006584   Polyak:0.99990  Bf:90  EN:0.0000  Loss: 4.865 0.366 0.514\n",
      "Ep:1010   Rew:94.02  Avg Rew:17.85  LR:0.00006643   Polyak:0.99990  Bf:91  EN:0.0000  Loss: 4.924 0.425 0.459\n",
      "Ep:1020   Rew:51.99  Avg Rew:18.66  LR:0.00006627   Polyak:0.99990  Bf:92  EN:0.0000  Loss: 4.950 0.399 0.539\n",
      "Ep:1030   Rew:107.02  Avg Rew:20.54  LR:0.00006589   Polyak:0.99990  Bf:93  EN:0.0000  Loss: 4.773 0.537 0.399\n",
      "Ep:1040   Rew:51.74  Avg Rew:15.77  LR:0.00006685   Polyak:0.99990  Bf:94  EN:0.0000  Loss: 4.741 0.546 1.520\n",
      "Ep:1050   Rew:-24.38  Avg Rew:14.43  LR:0.00006711   Polyak:0.99990  Bf:95  EN:0.0000  Loss: 5.027 0.682 0.511\n",
      "Ep:1060   Rew:-32.86  Avg Rew:17.22  LR:0.00006656   Polyak:0.99990  Bf:96  EN:0.0000  Loss: 4.550 0.876 0.615\n",
      "Ep:1070   Rew:76.85  Avg Rew:22.40  LR:0.00006552   Polyak:0.99990  Bf:97  EN:0.0000  Loss: 5.127 0.489 0.512\n",
      "Ep:1080   Rew:155.79  Avg Rew:24.63  LR:0.00006507   Polyak:0.99990  Bf:98  EN:0.0000  Loss: 5.020 0.995 1.123\n",
      "Ep:1090   Rew:-18.57  Avg Rew:24.98  LR:0.00006500   Polyak:0.99990  Bf:98  EN:0.0000  Loss: 4.940 0.510 0.479\n",
      "Ep:1100   Rew:-27.11  Avg Rew:37.12  LR:0.00006258   Polyak:0.99990  Bf:99  EN:0.0000  Loss: 5.045 0.412 0.317\n",
      "Ep:1110   Rew:-9.39  Avg Rew:34.02  LR:0.00006320   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.606 0.617 0.475\n",
      "Ep:1120   Rew:157.52  Avg Rew:28.49  LR:0.00006430   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 5.014 0.451 0.740\n",
      "Ep:1130   Rew:-40.15  Avg Rew:30.86  LR:0.00006383   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.854 0.368 0.382\n",
      "Ep:1140   Rew:20.10  Avg Rew:30.87  LR:0.00006383   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.728 0.428 0.414\n",
      "Ep:1150   Rew:-12.87  Avg Rew:34.19  LR:0.00006316   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.844 0.271 0.439\n",
      "Ep:1160   Rew:-12.81  Avg Rew:31.94  LR:0.00006361   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.433 0.337 0.326\n",
      "Ep:1170   Rew:69.27  Avg Rew:30.68  LR:0.00006386   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.940 0.443 0.653\n",
      "Ep:1180   Rew:-36.75  Avg Rew:24.52  LR:0.00006510   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.856 0.406 0.509\n",
      "Ep:1190   Rew:67.02  Avg Rew:25.77  LR:0.00006485   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 5.021 0.398 0.383\n",
      "Ep:1200   Rew:73.82  Avg Rew:18.56  LR:0.00006629   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.608 0.262 0.244\n",
      "Ep:1210   Rew:91.45  Avg Rew:21.02  LR:0.00006580   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.560 0.450 0.514\n",
      "Ep:1220   Rew:-9.96  Avg Rew:20.26  LR:0.00006595   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 5.156 0.238 0.332\n",
      "Ep:1230   Rew:51.55  Avg Rew:18.08  LR:0.00006638   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.331 0.397 0.334\n",
      "Ep:1240   Rew:43.22  Avg Rew:24.98  LR:0.00006500   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.444 0.364 0.298\n",
      "Ep:1250   Rew:38.50  Avg Rew:28.70  LR:0.00006426   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.232 0.315 0.306\n",
      "Ep:1260   Rew:142.52  Avg Rew:33.33  LR:0.00006333   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.737 0.298 0.286\n",
      "Ep:1270   Rew:56.64  Avg Rew:29.50  LR:0.00006410   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.650 0.678 0.427\n",
      "Ep:1280   Rew:-10.97  Avg Rew:29.80  LR:0.00006404   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.501 0.330 0.427\n",
      "Ep:1290   Rew:181.73  Avg Rew:32.39  LR:0.00006352   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.646 0.215 0.475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep:1300   Rew:117.51  Avg Rew:32.47  LR:0.00006351   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.601 0.328 0.297\n",
      "Ep:1310   Rew:-8.23  Avg Rew:34.40  LR:0.00006312   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.630 0.311 0.463\n",
      "Ep:1320   Rew:-2.93  Avg Rew:35.83  LR:0.00006283   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.309 0.191 0.216\n",
      "Ep:1330   Rew:-42.37  Avg Rew:37.18  LR:0.00006256   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.696 0.316 0.244\n",
      "Ep:1340   Rew:50.53  Avg Rew:31.32  LR:0.00006374   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.450 0.479 0.465\n",
      "Ep:1350   Rew:87.01  Avg Rew:24.64  LR:0.00006507   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.647 0.472 0.665\n",
      "Ep:1360   Rew:-60.37  Avg Rew:22.98  LR:0.00006540   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.259 0.674 0.538\n",
      "Ep:1370   Rew:-13.71  Avg Rew:20.48  LR:0.00006590   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.705 0.540 0.671\n",
      "Ep:1380   Rew:75.47  Avg Rew:25.79  LR:0.00006484   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.330 0.206 0.200\n",
      "Ep:1390   Rew:-23.06  Avg Rew:21.45  LR:0.00006571   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.422 0.287 0.264\n",
      "Ep:1400   Rew:110.11  Avg Rew:25.21  LR:0.00006496   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.470 0.320 0.241\n",
      "Ep:1410   Rew:33.68  Avg Rew:20.44  LR:0.00006591   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.325 0.327 0.276\n",
      "Ep:1420   Rew:282.92  Avg Rew:18.80  LR:0.00006624   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.275 0.155 0.138\n",
      "Ep:1430   Rew:-3.04  Avg Rew:15.35  LR:0.00006693   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.750 0.459 0.490\n",
      "Ep:1440   Rew:-72.70  Avg Rew:10.70  LR:0.00006786   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.414 0.397 0.482\n",
      "Ep:1450   Rew:-12.68  Avg Rew:11.84  LR:0.00006763   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.391 0.337 0.427\n",
      "Ep:1460   Rew:-56.17  Avg Rew:10.51  LR:0.00006790   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.420 0.355 0.325\n",
      "Ep:1470   Rew:66.91  Avg Rew:14.59  LR:0.00006708   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.528 0.330 0.234\n",
      "Ep:1480   Rew:21.11  Avg Rew:10.61  LR:0.00006788   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.499 0.550 0.288\n",
      "Ep:1490   Rew:91.42  Avg Rew:9.51  LR:0.00006810   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.059 0.242 0.206\n",
      "Ep:1500   Rew:132.99  Avg Rew:6.62  LR:0.00006868   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.237 0.176 0.204\n",
      "Ep:1510   Rew:210.14  Avg Rew:8.38  LR:0.00006832   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.487 0.244 0.298\n",
      "Ep:1520   Rew:-16.35  Avg Rew:8.11  LR:0.00006838   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.326 0.181 0.223\n",
      "Ep:1530   Rew:-26.23  Avg Rew:9.22  LR:0.00006816   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.335 0.542 0.426\n",
      "Ep:1540   Rew:66.98  Avg Rew:13.54  LR:0.00006729   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.192 0.200 0.254\n",
      "Ep:1550   Rew:2.11  Avg Rew:16.83  LR:0.00006663   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.262 0.307 0.246\n",
      "Ep:1560   Rew:188.52  Avg Rew:24.42  LR:0.00006512   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.230 0.362 0.357\n",
      "Ep:1570   Rew:155.05  Avg Rew:24.05  LR:0.00006519   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.274 0.501 0.731\n",
      "Ep:1580   Rew:-28.57  Avg Rew:27.45  LR:0.00006451   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.254 0.215 0.183\n",
      "Ep:1590   Rew:-49.35  Avg Rew:24.69  LR:0.00006506   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.650 0.427 0.387\n",
      "Ep:1600   Rew:64.91  Avg Rew:22.37  LR:0.00006553   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.405 0.242 0.215\n",
      "Ep:1610   Rew:-25.45  Avg Rew:23.31  LR:0.00006534   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.295 0.293 0.328\n",
      "Ep:1620   Rew:172.22  Avg Rew:28.77  LR:0.00006425   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.458 0.245 0.217\n",
      "Ep:1630   Rew:46.59  Avg Rew:28.01  LR:0.00006440   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.116 0.327 0.285\n",
      "Ep:1640   Rew:-29.88  Avg Rew:34.19  LR:0.00006316   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.031 0.311 0.349\n",
      "Ep:1650   Rew:118.90  Avg Rew:28.50  LR:0.00006430   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.217 0.452 0.382\n",
      "Ep:1660   Rew:34.20  Avg Rew:21.84  LR:0.00006563   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.085 0.239 0.215\n",
      "Ep:1670   Rew:-7.24  Avg Rew:23.16  LR:0.00006537   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.237 0.367 0.266\n",
      "Ep:1680   Rew:-32.87  Avg Rew:19.72  LR:0.00006606   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.256 0.144 0.191\n",
      "Ep:1690   Rew:-8.84  Avg Rew:25.00  LR:0.00006500   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.312 0.136 0.219\n",
      "Ep:1700   Rew:-2.25  Avg Rew:28.08  LR:0.00006438   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.262 0.276 0.238\n",
      "Ep:1710   Rew:17.71  Avg Rew:22.45  LR:0.00006551   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.064 0.225 0.273\n",
      "Ep:1720   Rew:142.90  Avg Rew:19.20  LR:0.00006616   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.284 0.232 0.210\n",
      "Ep:1730   Rew:-24.52  Avg Rew:17.97  LR:0.00006641   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.290 0.131 0.185\n",
      "Ep:1740   Rew:26.46  Avg Rew:11.31  LR:0.00006774   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.351 0.225 0.143\n",
      "Ep:1750   Rew:-48.82  Avg Rew:11.12  LR:0.00006778   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.241 0.362 0.293\n",
      "Ep:1760   Rew:-64.66  Avg Rew:10.95  LR:0.00006781   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 3.996 0.222 0.225\n",
      "Ep:1770   Rew:2.88  Avg Rew:13.31  LR:0.00006734   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.094 0.117 0.171\n",
      "Ep:1780   Rew:-4.01  Avg Rew:11.88  LR:0.00006762   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.318 0.282 0.293\n",
      "Ep:1790   Rew:-12.57  Avg Rew:9.50  LR:0.00006810   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.255 0.350 0.310\n",
      "Ep:1800   Rew:-54.05  Avg Rew:0.63  LR:0.00006987   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.453 0.361 0.491\n",
      "Ep:1810   Rew:-37.67  Avg Rew:8.09  LR:0.00006838   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.210 0.264 0.208\n",
      "Ep:1820   Rew:-32.79  Avg Rew:10.49  LR:0.00006790   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.159 0.143 0.187\n",
      "Ep:1830   Rew:45.32  Avg Rew:15.50  LR:0.00006690   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.065 0.186 0.178\n",
      "Ep:1840   Rew:283.86  Avg Rew:22.65  LR:0.00006547   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.156 0.250 0.146\n",
      "Ep:1850   Rew:105.18  Avg Rew:27.28  LR:0.00006454   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.235 0.571 0.669\n",
      "Ep:1860   Rew:-49.18  Avg Rew:25.45  LR:0.00006491   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.085 0.251 0.165\n",
      "Ep:1870   Rew:149.41  Avg Rew:22.71  LR:0.00006546   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.227 0.156 0.240\n",
      "Ep:1880   Rew:-33.76  Avg Rew:27.37  LR:0.00006453   Polyak:0.99990  Bf:100  EN:0.0000  Loss: 4.305 0.249 0.268\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bb224ce8ad16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 episode, ep_reward, avg_reward, learning_rate, polyak, replay_buffer.get_fill(), exploration_noise, policy.actor_loss, policy.loss_Q1, policy.loss_Q2))\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-bb224ce8ad16>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;31m# take action in env:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/reinforcement/lib/python3.6/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/reinforcement/lib/python3.6/site-packages/gym/envs/box2d/bipedal_walker.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    401\u001b[0m             self.lidar[i].p2 = (\n\u001b[1;32m    402\u001b[0m                 \u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mLIDAR_RANGE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m                 pos[1] - math.cos(1.5*i/10.0)*LIDAR_RANGE)\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRayCast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlidar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlidar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlidar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import gym\n",
    "import numpy as np\n",
    "from TD3_torch.TD3 import TD3\n",
    "from PIL import Image\n",
    "from TD3_torch.utils import ReplayBuffer\n",
    "\n",
    "env_name = 'BipedalWalkerHardcore-v2'\n",
    "learning_rate_base = 0.0001\n",
    "log_interval = 10           # print avg reward after interval\n",
    "random_seed = 222\n",
    "gamma = 0.99                # discount for future rewards\n",
    "batch_size = 1024        # num of transitions sampled from replay buffer\n",
    "exploration_noise_base = 0.0 \n",
    "polyak_int = [0.9999, 0.999999]              # target policy update parameter (1-tau)\n",
    "policy_noise = 0.2          # target policy smoothing noise\n",
    "noise_clip = 0.5\n",
    "policy_delay = 2            # delayed policy updates parameter\n",
    "max_episodes = 100000         # max num of episodes\n",
    "max_timesteps = 3000        # max timesteps in one episode\n",
    "max_buffer_length = 2000000\n",
    "directory = \"./preTrained/td3_torch/{}\".format(env_name) # save trained models\n",
    "filename = \"TD3_torch_{}_{}\".format(env_name, random_seed)\n",
    "reward_history = []\n",
    "\n",
    "\n",
    "\n",
    "def train():\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    max_action = float(env.action_space.high[0])\n",
    "    polyak = polyak_int[0]\n",
    "    exploration_noise = exploration_noise_base\n",
    "    \n",
    "    actor_config = [\n",
    "        {'dim': (state_dim, 256), 'dropout': False, 'activation': 'relu'},\n",
    "        {'dim': (256, 320), 'dropout': True, 'activation':'relu'},\n",
    "        {'dim': (320, 160), 'dropout': False, 'activation': 'relu'},\n",
    "        {'dim': (160, 64), 'dropout': False, 'activation': 'relu'},\n",
    "        {'dim': (64, action_dim),'dropout': False, 'activation': False}\n",
    "    ]\n",
    "    \n",
    "    critic_config = [\n",
    "        {'dim': (state_dim + action_dim, 256), 'dropout': False, 'activation': 'relu'},\n",
    "        {'dim': (256, 320), 'dropout': False , 'activation':'relu'},\n",
    "        {'dim': (320, 160), 'dropout': False, 'activation': 'relu'},\n",
    "        {'dim': (160, 1), 'dropout': False, 'activation': False}\n",
    "    ]\n",
    "    \n",
    "    policy = TD3(actor_config, critic_config, max_action, lr=learning_rate_base)   \n",
    "    replay_buffer = ReplayBuffer(max_length=max_buffer_length)\n",
    "    \n",
    "    print(\"action_space={}\".format(env.action_space))\n",
    "    print(\"obs_space={}\".format(env.observation_space))\n",
    "    print(\"threshold={} \\n\".format(env.spec.reward_threshold))     \n",
    "    \n",
    "    if random_seed:\n",
    "        print(\"Random Seed: {}\".format(random_seed))\n",
    "        env.seed(random_seed)\n",
    "        torch.manual_seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "    \n",
    "    # loading models\n",
    "    policy.load(directory, filename)\n",
    "    \n",
    "    # logging variables:        \n",
    "    log_f = open(\"log.txt\",\"w+\")\n",
    "    \n",
    "    # training procedure:\n",
    "    for episode in range(1, max_episodes+1):\n",
    "        ep_reward = 0\n",
    "        state = env.reset()\n",
    "       \n",
    "        for t in range(max_timesteps):\n",
    "            # select action and add exploration noise:\n",
    "            action = policy.select_action(state)\n",
    "            action = action + np.random.normal(0, exploration_noise, size=env.action_space.shape[0])\n",
    "            action = action.clip(env.action_space.low, env.action_space.high)\n",
    "            \n",
    "            # take action in env:\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            replay_buffer.add((state, action, reward, next_state, float(done)))\n",
    "            state = next_state\n",
    "            \n",
    "            ep_reward += reward\n",
    "            \n",
    "            # if episode is done then update policy:\n",
    "            if done or t==(max_timesteps-1):\n",
    "                policy.update(replay_buffer, t, batch_size, gamma, polyak, policy_noise, noise_clip, policy_delay)\n",
    "                break\n",
    "        \n",
    "        reward_history.append(ep_reward)\n",
    "        avg_reward = np.mean(reward_history[-100:]) \n",
    "        \n",
    "        # logging updates:        \n",
    "        log_f.write('{},{}\\n'.format(episode, ep_reward))\n",
    "        log_f.flush()\n",
    "       \n",
    "        \n",
    "        # if avg reward > 300 then save and stop traning:\n",
    "        if avg_reward >= env.spec.reward_threshold: \n",
    "            print(\"########## Solved! ###########\")\n",
    "            name = filename + '_solved'\n",
    "            policy.save(directory, name)\n",
    "            log_f.close()\n",
    "            break\n",
    "            \n",
    "        # Calculate polyak\n",
    "        #part = (env.spec.reward_threshold - avg_reward) / (env.spec.reward_threshold + 150)\n",
    "        #if part > 1:\n",
    "        #    part = 1\n",
    "        #polyak = polyak_int[0] + (1 - part) * (polyak_int[1] - polyak_int[0])     \n",
    "        \n",
    "        # Calculate LR\n",
    "        part = (env.spec.reward_threshold - avg_reward) / (env.spec.reward_threshold + 150)\n",
    "        if part > 1:\n",
    "            part = 1\n",
    "        learning_rate = learning_rate_base - learning_rate_base * (1 - part) * 0.9\n",
    "        policy.set_optimizers(lr=learning_rate)\n",
    "        \n",
    "        # Calculate Exploration Noise\n",
    "        exploration_noise = exploration_noise_base - exploration_noise_base * (1 - part) * 0.9\n",
    "        \n",
    "        \n",
    "        if episode > 500:\n",
    "            policy.save(directory, filename)\n",
    "        \n",
    "        # print avg reward every log interval:\n",
    "        if episode % log_interval == 0:            \n",
    "            print(\"Ep:{}   Rew:{:3.2f}  Avg Rew:{:3.2f}  LR:{:8.8f}   Polyak:{:5.5f}  Bf:{:2.0f}  EN:{:0.4f}  Loss: {:5.3f} {:5.3f} {:5.3f}\".format(\n",
    "                episode, ep_reward, avg_reward, learning_rate, polyak, replay_buffer.get_fill(), exploration_noise, policy.actor_loss, policy.loss_Q1, policy.loss_Q2))\n",
    "\n",
    "train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():  \n",
    "    random_seed = 0\n",
    "    n_episodes = 3\n",
    "    max_timesteps = 2000\n",
    "    render = True\n",
    "    save_gif = True\n",
    "    \n",
    "    filename = \"TD3_torch_{}_{}\".format(env_name, random_seed)\n",
    "    filename += ''\n",
    "    directory = \"./preTrained/td3_torch/{}\".format(env_name)\n",
    "    \n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    max_action = float(env.action_space.high[0])\n",
    "    \n",
    "    policy = TD3(state_dim, action_dim, max_action)\n",
    "    \n",
    "    policy.load_actor(directory, filename)\n",
    "    \n",
    "    for ep in range(1, n_episodes+1):\n",
    "        ep_reward = 0\n",
    "        state = env.reset()\n",
    "        for t in range(max_timesteps):\n",
    "            action = policy.select_action(state)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            ep_reward += reward\n",
    "            if render:\n",
    "                env.render()\n",
    "                if save_gif:\n",
    "                    dirname = './gif/td3_torch/{}'.format(ep)\n",
    "                    if not os.path.isdir(dirname):\n",
    "                        os.mkdir(dirname)\n",
    "                    img = env.render(mode = 'rgb_array')\n",
    "                    img = Image.fromarray(img)\n",
    "                    img.save('./gif/td3_torch/{}/{}.jpg'.format(ep,t))\n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "        print('Episode: {}\\tReward: {}'.format(ep, int(ep_reward)))\n",
    "        ep_reward = 0\n",
    "        env.close()        \n",
    "                \n",
    "test()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
