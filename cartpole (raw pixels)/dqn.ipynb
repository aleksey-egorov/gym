{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gym import wrappers\n",
    "from PIL import Image\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "\n",
    "from DQN.dqn import DQN_Conv\n",
    "from DQN.buffer import ReplayBuffer\n",
    "from DQN.utils import get_screen, mkdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v1'\n",
    "lr_base = 0.001\n",
    "lr_decay = 0.0001\n",
    "epsilon_base = 0.5 \n",
    "epsilon_decay = 0.002\n",
    "\n",
    "random_seed = 42\n",
    "gamma = 0.99                # discount for future rewards\n",
    "batch_size = 32        # num of transitions sampled from replay buffer\n",
    "polyak = 0.999               # target policy update parameter (1-tau)\n",
    "max_episodes = 100000         # max num of episodes\n",
    "max_timesteps = 3000        # max timesteps in one episode\n",
    "max_buffer_length = 5000000\n",
    "log_interval = 10           # print avg reward after interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_config = [\n",
    "    {'dim': [3,16], 'kernel': 5, 'stride': 2, 'batch_norm': True, 'activation': 'relu'},\n",
    "    {'dim': [16,32], 'kernel': 5, 'stride': 2, 'batch_norm': True, 'activation': 'relu'},\n",
    "    {'dim': [32,32], 'kernel': 5, 'stride': 2, 'batch_norm': True, 'activation': 'relu'},\n",
    "]\n",
    "\n",
    "fc_config = [\n",
    "        {'dim': [448, None], 'dropout': False, 'activation': False}\n",
    "    ] \n",
    "config = [conv_config, fc_config]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_Conv_Trainer():\n",
    "    \n",
    "    def __init__(self, env_name, fc_config, random_seed=42, lr_base=0.001, lr_decay=0.00005, \n",
    "                 epsilon_base=0.3, epsilon_decay=0.0001, gamma=0.99, batch_size=1024, \n",
    "                 max_episodes=100000, max_timesteps=3000, max_buffer_length=5000000, \n",
    "                 log_interval=5, threshold=None, lr_minimum=1e-10, epsilon_minimum=1e-10,\n",
    "                 record_videos=True, record_interval=100):        \n",
    "        \n",
    "        self.algorithm_name = 'dqn'\n",
    "        self.env_name = env_name\n",
    "        self.env = gym.make(env_name)\n",
    "        self.record_videos = record_videos\n",
    "        self.record_interval = record_interval        \n",
    "        if self.record_videos == True:\n",
    "            videos_dir = mkdir('.', 'videos')\n",
    "            monitor_dir = mkdir(videos_dir, self.algorithm_name)\n",
    "            should_record = lambda i: self.should_record\n",
    "            self.env = wrappers.Monitor(self.env, monitor_dir, video_callable=should_record, force=True)            \n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.n    \n",
    "        self.should_record = False\n",
    "        if not threshold == None:\n",
    "            self.threshold = threshold\n",
    "        else:    \n",
    "            self.threshold = self.env.spec.reward_threshold\n",
    "              \n",
    "        self.config = config\n",
    "        #self.fc_config[0]['dim'][0] = self.state_dim\n",
    "        self.config[1][-1]['dim'][1] = self.action_dim    \n",
    "        self.conv_config = self.config[0]\n",
    "        self.fc_config = self.config[1]\n",
    "        \n",
    "        self.random_seed = random_seed\n",
    "        self.lr_base = lr_base\n",
    "        self.lr_decay = lr_decay\n",
    "        self.lr_minimum = lr_minimum  \n",
    "        self.epsilon_base = epsilon_base\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_minimum = epsilon_minimum\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size              \n",
    "        self.max_episodes = max_episodes\n",
    "        self.max_timesteps = max_timesteps\n",
    "        self.max_buffer_length = max_buffer_length\n",
    "        self.log_interval = log_interval\n",
    "        \n",
    "        prdir = mkdir('.', 'preTrained')\n",
    "        self.directory = mkdir(prdir, self.algorithm_name)\n",
    "        self.filename = \"{}_{}_{}\".format(self.algorithm_name, self.env_name, self.random_seed)\n",
    "        \n",
    "        self.policy = DQN_Conv(self.env, self.conv_config, self.fc_config)   \n",
    "        self.replay_buffer = ReplayBuffer(size=self.max_buffer_length)\n",
    "        \n",
    "        self.reward_history = []\n",
    "        self.make_plots = False       \n",
    "        \n",
    "        if self.random_seed:\n",
    "            print(\"Random Seed: {}\".format(self.random_seed))\n",
    "            self.env.seed(self.random_seed)\n",
    "            torch.manual_seed(self.random_seed)\n",
    "            np.random.seed(self.random_seed)\n",
    "            \n",
    "    def train(self):\n",
    "        \n",
    "        start_time = time.time()   \n",
    "        self.env.reset()\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.imshow(get_screen(self.env).cpu().squeeze(0).permute(1, 2, 0).numpy(),interpolation='none')\n",
    "        plt.title('Example extracted screen')\n",
    "        plt.show()\n",
    "                \n",
    "        print(\"Action_space={}\".format(self.env.action_space))\n",
    "        print(\"Obs_space={}\".format(self.env.observation_space))\n",
    "        print(\"Threshold={} \\n\".format(self.threshold))        \n",
    "\n",
    "        # loading models\n",
    "        self.policy.load(self.directory, self.filename)\n",
    "                \n",
    "        # logging variables:        \n",
    "        log_f = open(\"train_{}.txt\".format(self.algorithm_name), \"w+\")\n",
    "        \n",
    "        print(\"\\nTraining started ... \")\n",
    "                \n",
    "        # training procedure:        \n",
    "        for episode in range(self.max_episodes):\n",
    "            \n",
    "            # Only record video during evaluation, every n steps\n",
    "            if episode % self.record_interval == 0:\n",
    "                self.should_record = True\n",
    "            \n",
    "            self.env.reset()\n",
    "            ep_reward = 0.0                    \n",
    "            last_screen = get_screen(self.env)\n",
    "            current_screen = get_screen(self.env)\n",
    "            state = current_screen - last_screen\n",
    "                       \n",
    "            # calculate params\n",
    "            epsilon = max(self.epsilon_base / (1.0 + episode * self.epsilon_decay), self.epsilon_minimum)\n",
    "            learning_rate = max(self.lr_base / (1.0 + episode * self.lr_decay), self.lr_minimum)      \n",
    "            self.policy.set_optimizers(lr=learning_rate)\n",
    "           \n",
    "            for t in range(self.max_timesteps):\n",
    "                \n",
    "                action = self.policy.select_action(state, epsilon)                \n",
    "                _, reward, done, _ = self.env.step(action)          \n",
    "                \n",
    "                last_screen = current_screen\n",
    "                current_screen = get_screen(self.env)\n",
    "                if not done:\n",
    "                    next_state = current_screen - last_screen\n",
    "                \n",
    "                state = state.squeeze(0)\n",
    "                next_state = next_state.squeeze(0)\n",
    "                #print (\"STATE: {} NEXT: {}\".format(state.shape, next_state.shape))\n",
    "                self.replay_buffer.add(state, action, reward, next_state, float(done))\n",
    "                \n",
    "                # Updating policy\n",
    "                self.policy.update(self.replay_buffer, t, self.batch_size, self.gamma)\n",
    "                                \n",
    "                state = next_state               \n",
    "                ep_reward += reward            \n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "           \n",
    "            self.reward_history.append(ep_reward)\n",
    "            avg_reward = np.mean(self.reward_history[-100:]) \n",
    "                       \n",
    "            # logging updates:        \n",
    "            log_f.write('{},{}\\n'.format(episode, ep_reward))\n",
    "            log_f.flush()\n",
    "           \n",
    "            if len(self.policy.Q_loss_list) > 0:               \n",
    "                avg_Q_loss = np.mean(self.policy.Q_loss_list[-100:])     \n",
    "            \n",
    "            # Truncate training history if we don't plan to plot it later\n",
    "            if not self.make_plots:\n",
    "                self.policy.truncate_loss_lists() \n",
    "                if len(self.reward_history) > 100:\n",
    "                    self.reward_history.pop(0)    \n",
    "         \n",
    "            # Print avg reward every log interval:\n",
    "            if episode % self.log_interval == 0:            \n",
    "                self.policy.save(self.directory, self.filename)\n",
    "                print(\"Ep:{:5d}  Rew:{:8.2f}  Avg Rew:{:8.2f}  LR:{:8.8f}  Bf:{:2.0f}  EPS:{:0.4f}  Loss: {:5.3f}\".format(\n",
    "                    episode, ep_reward, avg_reward, learning_rate, self.replay_buffer.get_fill(), \n",
    "                    epsilon, avg_Q_loss))\n",
    "                        \n",
    "            self.should_record = False\n",
    "            \n",
    "            # if avg reward > threshold then save and stop traning:\n",
    "            if avg_reward >= self.threshold and episode > 100: \n",
    "                print(\"Ep:{:5d}  Rew:{:8.2f}  Avg Rew:{:8.2f}  LR:{:8.8f}  Bf:{:2.0f}  EPS:{:0.4f}  Loss: {:5.3f}\".format(\n",
    "                    episode, ep_reward, avg_reward, learning_rate, self.replay_buffer.get_fill(), \n",
    "                    epsilon, avg_Q_loss))\n",
    "                print(\"########## Solved! ###########\")\n",
    "                name = self.filename + '_solved'\n",
    "                self.policy.save(self.directory, name)\n",
    "                log_f.close()\n",
    "                self.env.close()  \n",
    "                training_time = time.time() - start_time\n",
    "                print(\"Training time: {:6.2f} sec\".format(training_time))\n",
    "                break    \n",
    "                                \n",
    "    def test(self, episodes=3, render=True, save_gif=True):              \n",
    "\n",
    "        gifdir = mkdir('.','gif')\n",
    "        algdir = mkdir(gifdir, self.algorithm_name)\n",
    "        \n",
    "        for episode in range(1, episodes+1):\n",
    "           \n",
    "            self.env.reset()\n",
    "            ep_reward = 0.0                    \n",
    "            last_screen = get_screen(self.env)\n",
    "            current_screen = get_screen(self.env)\n",
    "            state = current_screen - last_screen\n",
    "            epdir = mkdir(algdir, str(episode))\n",
    "                       \n",
    "            for t in range(self.max_timesteps):\n",
    "                action = self.policy.select_action(state, 0)\n",
    "                _, reward, done, _ = self.env.step(action) \n",
    "                \n",
    "                last_screen = current_screen\n",
    "                current_screen = get_screen(self.env)\n",
    "                if not done:\n",
    "                    next_state = current_screen - last_screen\n",
    "                \n",
    "                state = state.squeeze(0)\n",
    "                next_state = next_state.squeeze(0)                                                \n",
    "                state = next_state         \n",
    "                           \n",
    "                ep_reward += reward                                  \n",
    "                \n",
    "                if save_gif:                                       \n",
    "                    img = self.env.render(mode = 'rgb_array')\n",
    "                    img = Image.fromarray(img)\n",
    "                    img.save('{}/{}.jpg'.format(epdir, t))\n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "            print('Test episode: {}\\tReward: {:4.2f}'.format(episode, ep_reward))           \n",
    "            self.env.close()        \n",
    "                   \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda:0\n",
      "NETWORK=Sequential(\n",
      "  (0): Conv2d(3, 16, kernel_size=(5, 5), stride=(2, 2))\n",
      "  (1): ReLU()\n",
      "  (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (3): Conv2d(16, 32, kernel_size=(5, 5), stride=(2, 2))\n",
      "  (4): ReLU()\n",
      "  (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (6): Conv2d(32, 32, kernel_size=(5, 5), stride=(2, 2))\n",
      "  (7): ReLU()\n",
      "  (8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ") Sequential(\n",
      "  (0): Linear(in_features=448, out_features=2, bias=True)\n",
      ")\n",
      "DEVICE: cuda:0\n",
      "NETWORK=Sequential(\n",
      "  (0): Conv2d(3, 16, kernel_size=(5, 5), stride=(2, 2))\n",
      "  (1): ReLU()\n",
      "  (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (3): Conv2d(16, 32, kernel_size=(5, 5), stride=(2, 2))\n",
      "  (4): ReLU()\n",
      "  (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (6): Conv2d(32, 32, kernel_size=(5, 5), stride=(2, 2))\n",
      "  (7): ReLU()\n",
      "  (8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ") Sequential(\n",
      "  (0): Linear(in_features=448, out_features=2, bias=True)\n",
      ")\n",
      "Random Seed: 42\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action_space=Discrete(2)\n",
      "Obs_space=Box(4,)\n",
      "Threshold=475.0 \n",
      "\n",
      "DIR=./preTrained/dqn NAME=dqn_CartPole-v1_42\n",
      "No models to load\n",
      "\n",
      "Training started ... \n",
      "Ep:    0  Rew:   17.00  Avg Rew:   17.00  LR:0.00100000  Bf: 0  EPS:0.5000  Loss: 0.468\n",
      "Ep:   10  Rew:   18.00  Avg Rew:   18.27  LR:0.00099900  Bf: 0  EPS:0.4902  Loss: 1.144\n",
      "Ep:   20  Rew:   12.00  Avg Rew:   18.71  LR:0.00099800  Bf: 0  EPS:0.4808  Loss: 4.147\n",
      "Ep:   30  Rew:   12.00  Avg Rew:   22.77  LR:0.00099701  Bf: 0  EPS:0.4717  Loss: 12.086\n",
      "Ep:   40  Rew:  295.00  Avg Rew:   44.59  LR:0.00099602  Bf: 0  EPS:0.4630  Loss: 13.948\n",
      "Ep:   50  Rew:  209.00  Avg Rew:   54.63  LR:0.00099502  Bf: 0  EPS:0.4545  Loss: 17.724\n",
      "Ep:   60  Rew:   65.00  Avg Rew:   56.10  LR:0.00099404  Bf: 0  EPS:0.4464  Loss: 30.198\n",
      "Ep:   70  Rew:   23.00  Avg Rew:   58.77  LR:0.00099305  Bf: 0  EPS:0.4386  Loss: 31.822\n",
      "Ep:   80  Rew:   75.00  Avg Rew:   65.31  LR:0.00099206  Bf: 0  EPS:0.4310  Loss: 40.430\n",
      "Ep:   90  Rew:   83.00  Avg Rew:   63.79  LR:0.00099108  Bf: 0  EPS:0.4237  Loss: 53.144\n",
      "Ep:  100  Rew:   76.00  Avg Rew:   63.17  LR:0.00099010  Bf: 0  EPS:0.4167  Loss: 62.550\n",
      "Ep:  110  Rew:  146.00  Avg Rew:   68.86  LR:0.00098912  Bf: 0  EPS:0.4098  Loss: 66.432\n",
      "Ep:  120  Rew:   53.00  Avg Rew:   72.03  LR:0.00098814  Bf: 0  EPS:0.4032  Loss: 59.186\n",
      "Ep:  130  Rew:   52.00  Avg Rew:   74.75  LR:0.00098717  Bf: 0  EPS:0.3968  Loss: 71.713\n",
      "Ep:  140  Rew:  117.00  Avg Rew:   67.58  LR:0.00098619  Bf: 0  EPS:0.3906  Loss: 85.323\n",
      "Ep:  150  Rew:   27.00  Avg Rew:   62.90  LR:0.00098522  Bf: 0  EPS:0.3846  Loss: 86.375\n",
      "Ep:  160  Rew:   50.00  Avg Rew:   63.67  LR:0.00098425  Bf: 0  EPS:0.3788  Loss: 117.728\n",
      "Ep:  170  Rew:   68.00  Avg Rew:   62.36  LR:0.00098328  Bf: 0  EPS:0.3731  Loss: 124.334\n",
      "Ep:  180  Rew:  149.00  Avg Rew:   59.92  LR:0.00098232  Bf: 0  EPS:0.3676  Loss: 95.361\n",
      "Ep:  190  Rew:  113.00  Avg Rew:   60.83  LR:0.00098135  Bf: 0  EPS:0.3623  Loss: 120.951\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-af7edbc390bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                    \u001b[0mmax_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                    max_buffer_length=max_buffer_length, log_interval=log_interval)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-7f78811268c8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_record\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m             \u001b[0mep_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mlast_screen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_screen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36m_after_reset\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_video_recorder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;31m# Bump *after* all reset activity has finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mreset_video_recorder\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_video_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         )\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_close_video_recorder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36mcapture_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_ansi_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_image_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36m_encode_image_frame\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_encode_image_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframes_per_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoder_version'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, output_path, frame_shape, frames_per_sec)\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDependencyNotInstalled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\"Found neither the ffmpeg nor avconv executables. On OS X, you can install ffmpeg via `brew install ffmpeg`. On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`.\"\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Starting ffmpeg with \"%s\"'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmdline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'setsid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#setsid not present on Windows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmdline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreexec_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetsid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmdline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gym/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors)\u001b[0m\n\u001b[1;32m    727\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    730\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0;31m# Cleanup if the child failed starting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/gym/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1293\u001b[0m                             \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m                             \u001b[0merrpipe_read\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrpipe_write\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m                             restore_signals, start_new_session, preexec_fn)\n\u001b[0m\u001b[1;32m   1296\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_child_created\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "agent = DQN_Conv_Trainer(env_name, config, random_seed=random_seed, lr_base=lr_base, lr_decay=lr_decay, \n",
    "                   epsilon_base=epsilon_base, epsilon_decay=epsilon_decay, gamma=gamma, batch_size=batch_size,\n",
    "                   max_episodes=max_episodes, max_timesteps=max_timesteps, \n",
    "                   max_buffer_length=max_buffer_length, log_interval=log_interval)\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "agent.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
