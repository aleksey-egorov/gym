{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'baselines'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c4cdf0115f13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mDQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdqn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDQN_Conv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mDQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReplayBuffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mDQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_screen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmkdir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/gym/cartpole (raw pixels)/DQN/buffer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbaselines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegment_tree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSumSegmentTree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMinSegmentTree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'baselines'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import gym\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gym import wrappers\n",
    "from PIL import Image\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "\n",
    "from DQN.dqn import DQN_Conv\n",
    "from DQN.buffer import ReplayBuffer\n",
    "from DQN.utils import get_screen, mkdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v1'\n",
    "lr_base = 0.001\n",
    "lr_decay = 0.0001\n",
    "epsilon_base = 0.5 \n",
    "epsilon_decay = 0.002\n",
    "\n",
    "random_seed = 42\n",
    "gamma = 0.99                # discount for future rewards\n",
    "batch_size = 32        # num of transitions sampled from replay buffer\n",
    "polyak = 0.999               # target policy update parameter (1-tau)\n",
    "max_episodes = 100000         # max num of episodes\n",
    "max_timesteps = 3000        # max timesteps in one episode\n",
    "max_buffer_length = 5000000\n",
    "log_interval = 10           # print avg reward after interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_config = [\n",
    "    {'dim': [3,16], 'kernel': 5, 'stride': 2, 'batch_norm': True, 'activation': 'relu'},\n",
    "    {'dim': [16,32], 'kernel': 5, 'stride': 2, 'batch_norm': True, 'activation': 'relu'},\n",
    "    {'dim': [32,32], 'kernel': 5, 'stride': 2, 'batch_norm': True, 'activation': 'relu'},\n",
    "]\n",
    "\n",
    "fc_config = [\n",
    "        {'dim': [448, None], 'dropout': False, 'activation': False}\n",
    "    ] \n",
    "config = [conv_config, fc_config]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_Conv_Trainer():\n",
    "    \n",
    "    def __init__(self, env_name, fc_config, random_seed=42, lr_base=0.001, lr_decay=0.00005, \n",
    "                 epsilon_base=0.3, epsilon_decay=0.0001, gamma=0.99, batch_size=1024, \n",
    "                 max_episodes=100000, max_timesteps=3000, max_buffer_length=5000000, \n",
    "                 log_interval=5, threshold=None, lr_minimum=1e-10, epsilon_minimum=1e-10,\n",
    "                 record_videos=True, record_interval=100):        \n",
    "        \n",
    "        self.algorithm_name = 'dqn'\n",
    "        self.env_name = env_name\n",
    "        self.env = gym.make(env_name)\n",
    "        self.record_videos = record_videos\n",
    "        self.record_interval = record_interval        \n",
    "        if self.record_videos == True:\n",
    "            videos_dir = mkdir('.', 'videos')\n",
    "            monitor_dir = mkdir(videos_dir, self.algorithm_name)\n",
    "            should_record = lambda i: self.should_record\n",
    "            self.env = wrappers.Monitor(self.env, monitor_dir, video_callable=should_record, force=True)            \n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.n    \n",
    "        self.should_record = False\n",
    "        if not threshold == None:\n",
    "            self.threshold = threshold\n",
    "        else:    \n",
    "            self.threshold = self.env.spec.reward_threshold\n",
    "              \n",
    "        self.config = config\n",
    "        #self.fc_config[0]['dim'][0] = self.state_dim\n",
    "        self.config[1][-1]['dim'][1] = self.action_dim    \n",
    "        self.conv_config = self.config[0]\n",
    "        self.fc_config = self.config[1]\n",
    "        \n",
    "        self.random_seed = random_seed\n",
    "        self.lr_base = lr_base\n",
    "        self.lr_decay = lr_decay\n",
    "        self.lr_minimum = lr_minimum  \n",
    "        self.epsilon_base = epsilon_base\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_minimum = epsilon_minimum\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size              \n",
    "        self.max_episodes = max_episodes\n",
    "        self.max_timesteps = max_timesteps\n",
    "        self.max_buffer_length = max_buffer_length\n",
    "        self.log_interval = log_interval\n",
    "        \n",
    "        prdir = mkdir('.', 'preTrained')\n",
    "        self.directory = mkdir(prdir, self.algorithm_name)\n",
    "        self.filename = \"{}_{}_{}\".format(self.algorithm_name, self.env_name, self.random_seed)\n",
    "        \n",
    "        self.policy = DQN_Conv(self.env, self.conv_config, self.fc_config)   \n",
    "        self.replay_buffer = ReplayBuffer(size=self.max_buffer_length)\n",
    "        \n",
    "        self.reward_history = []\n",
    "        self.make_plots = False       \n",
    "        \n",
    "        if self.random_seed:\n",
    "            print(\"Random Seed: {}\".format(self.random_seed))\n",
    "            self.env.seed(self.random_seed)\n",
    "            torch.manual_seed(self.random_seed)\n",
    "            np.random.seed(self.random_seed)\n",
    "            \n",
    "    def train(self):\n",
    "        \n",
    "        start_time = time.time()   \n",
    "        self.env.reset()\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.imshow(get_screen(self.env).cpu().squeeze(0).permute(1, 2, 0).numpy(),interpolation='none')\n",
    "        plt.title('Example extracted screen')\n",
    "        plt.show()\n",
    "                \n",
    "        print(\"Action_space={}\".format(self.env.action_space))\n",
    "        print(\"Obs_space={}\".format(self.env.observation_space))\n",
    "        print(\"Threshold={} \\n\".format(self.threshold))        \n",
    "\n",
    "        # loading models\n",
    "        self.policy.load(self.directory, self.filename)\n",
    "                \n",
    "        # logging variables:        \n",
    "        log_f = open(\"train_{}.txt\".format(self.algorithm_name), \"w+\")\n",
    "        \n",
    "        print(\"\\nTraining started ... \")\n",
    "                \n",
    "        # training procedure:        \n",
    "        for episode in range(self.max_episodes):\n",
    "            \n",
    "            # Only record video during evaluation, every n steps\n",
    "            if episode % self.record_interval == 0:\n",
    "                self.should_record = True\n",
    "            \n",
    "            self.env.reset()\n",
    "            ep_reward = 0.0                    \n",
    "            last_screen = get_screen(self.env)\n",
    "            current_screen = get_screen(self.env)\n",
    "            state = current_screen - last_screen\n",
    "                       \n",
    "            # calculate params\n",
    "            epsilon = max(self.epsilon_base / (1.0 + episode * self.epsilon_decay), self.epsilon_minimum)\n",
    "            learning_rate = max(self.lr_base / (1.0 + episode * self.lr_decay), self.lr_minimum)      \n",
    "            self.policy.set_optimizers(lr=learning_rate)\n",
    "           \n",
    "            for t in range(self.max_timesteps):\n",
    "                \n",
    "                action = self.policy.select_action(state, epsilon)                \n",
    "                _, reward, done, _ = self.env.step(action)          \n",
    "                \n",
    "                last_screen = current_screen\n",
    "                current_screen = get_screen(self.env)\n",
    "                if not done:\n",
    "                    next_state = current_screen - last_screen\n",
    "                \n",
    "                state = state.squeeze(0)\n",
    "                next_state = next_state.squeeze(0)\n",
    "                #print (\"STATE: {} NEXT: {}\".format(state.shape, next_state.shape))\n",
    "                self.replay_buffer.add(state, action, reward, next_state, float(done))\n",
    "                \n",
    "                # Updating policy\n",
    "                self.policy.update(self.replay_buffer, t, self.batch_size, self.gamma)\n",
    "                                \n",
    "                state = next_state               \n",
    "                ep_reward += reward            \n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "           \n",
    "            self.reward_history.append(ep_reward)\n",
    "            avg_reward = np.mean(self.reward_history[-100:]) \n",
    "                       \n",
    "            # logging updates:        \n",
    "            log_f.write('{},{}\\n'.format(episode, ep_reward))\n",
    "            log_f.flush()\n",
    "           \n",
    "            if len(self.policy.Q_loss_list) > 0:               \n",
    "                avg_Q_loss = np.mean(self.policy.Q_loss_list[-100:])     \n",
    "            \n",
    "            # Truncate training history if we don't plan to plot it later\n",
    "            if not self.make_plots:\n",
    "                self.policy.truncate_loss_lists() \n",
    "                if len(self.reward_history) > 100:\n",
    "                    self.reward_history.pop(0)    \n",
    "         \n",
    "            # Print avg reward every log interval:\n",
    "            if episode % self.log_interval == 0:            \n",
    "                self.policy.save(self.directory, self.filename)\n",
    "                print(\"Ep:{:5d}  Rew:{:8.2f}  Avg Rew:{:8.2f}  LR:{:8.8f}  Bf:{:2.0f}  EPS:{:0.4f}  Loss: {:5.3f}\".format(\n",
    "                    episode, ep_reward, avg_reward, learning_rate, self.replay_buffer.get_fill(), \n",
    "                    epsilon, avg_Q_loss))\n",
    "                        \n",
    "            self.should_record = False\n",
    "            \n",
    "            # if avg reward > threshold then save and stop traning:\n",
    "            if avg_reward >= self.threshold and episode > 100: \n",
    "                print(\"Ep:{:5d}  Rew:{:8.2f}  Avg Rew:{:8.2f}  LR:{:8.8f}  Bf:{:2.0f}  EPS:{:0.4f}  Loss: {:5.3f}\".format(\n",
    "                    episode, ep_reward, avg_reward, learning_rate, self.replay_buffer.get_fill(), \n",
    "                    epsilon, avg_Q_loss))\n",
    "                print(\"########## Solved! ###########\")\n",
    "                name = self.filename + '_solved'\n",
    "                self.policy.save(self.directory, name)\n",
    "                log_f.close()\n",
    "                self.env.close()  \n",
    "                training_time = time.time() - start_time\n",
    "                print(\"Training time: {:6.2f} sec\".format(training_time))\n",
    "                break    \n",
    "                                \n",
    "    def test(self, episodes=3, render=True, save_gif=True):              \n",
    "\n",
    "        gifdir = mkdir('.','gif')\n",
    "        algdir = mkdir(gifdir, self.algorithm_name)\n",
    "        \n",
    "        for episode in range(1, episodes+1):\n",
    "           \n",
    "            self.env.reset()\n",
    "            ep_reward = 0.0                    \n",
    "            last_screen = get_screen(self.env)\n",
    "            current_screen = get_screen(self.env)\n",
    "            state = current_screen - last_screen\n",
    "            epdir = mkdir(algdir, str(episode))\n",
    "                       \n",
    "            for t in range(self.max_timesteps):\n",
    "                action = self.policy.select_action(state, 0)\n",
    "                _, reward, done, _ = self.env.step(action) \n",
    "                \n",
    "                last_screen = current_screen\n",
    "                current_screen = get_screen(self.env)\n",
    "                if not done:\n",
    "                    next_state = current_screen - last_screen\n",
    "                \n",
    "                state = state.squeeze(0)\n",
    "                next_state = next_state.squeeze(0)                                                \n",
    "                state = next_state         \n",
    "                           \n",
    "                ep_reward += reward                                  \n",
    "                \n",
    "                if save_gif:                                       \n",
    "                    img = self.env.render(mode = 'rgb_array')\n",
    "                    img = Image.fromarray(img)\n",
    "                    img.save('{}/{}.jpg'.format(epdir, t))\n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "            print('Test episode: {}\\tReward: {:4.2f}'.format(episode, ep_reward))           \n",
    "            self.env.close()        \n",
    "                   \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "agent = DQN_Conv_Trainer(env_name, config, random_seed=random_seed, lr_base=lr_base, lr_decay=lr_decay, \n",
    "                   epsilon_base=epsilon_base, epsilon_decay=epsilon_decay, gamma=gamma, batch_size=batch_size,\n",
    "                   max_episodes=max_episodes, max_timesteps=max_timesteps, \n",
    "                   max_buffer_length=max_buffer_length, log_interval=log_interval)\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "agent.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
